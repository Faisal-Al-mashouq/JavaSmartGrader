#!/usr/bin/env node
import "./chunk-DHDDz29n.js";
import { C as getEnvString, o as logger_default } from "./logger-Bzi5o47S.js";
import "./fetch-TcCC0nEJ.js";
import "./cloud-UwfmA0Ne.js";
import "./types-n3ttAnOa.js";
import { a as fetchWithCache } from "./cache-xb4oZe27.js";
import "./util-CyzW5ayt.js";
import "./esm-Bwuh5HXW.js";
import "./pythonUtils-mprm8p4h.js";
import "./genaiTracer-CqNnnXrE.js";
import { t as OpenAiChatCompletionProvider } from "./chat-BbgGuvwu.js";
import "./transform-B-AYIQmM.js";
import "./openai-D_5h8sBa.js";
import "./util-CwsTKPXi.js";
import { n as OpenAiEmbeddingProvider, t as OpenAiCompletionProvider } from "./completion-BTZ42lqL.js";

//#region src/providers/docker.ts
async function fetchLocalModels(apiBaseUrl) {
	try {
		const { data } = await fetchWithCache(`${apiBaseUrl}/models`, void 0, void 0, "json", true, 0);
		return data?.data ?? [];
	} catch (e) {
		throw new Error(`Failed to connect to Docker Model Runner. Is it enabled? Are the API endpoints enabled? For details, see https://docs.docker.com/ai/model-runner. \n${e.message}`);
	}
}
async function hasLocalModel(modelId, apiBaseUrl) {
	return (await fetchLocalModels(apiBaseUrl)).some((model) => model && model.id?.toLocaleLowerCase() === modelId?.toLocaleLowerCase());
}
function parseProviderPath(providerPath) {
	const splits = providerPath.split(":");
	const type = splits[1];
	switch (type) {
		case "chat":
		case "completion":
		case "embeddings": return {
			type,
			model: splits.slice(2).join(":")
		};
		case "embedding": return {
			type: "embeddings",
			model: splits.slice(2).join(":")
		};
		default: return {
			type: "chat",
			model: splits.slice(1).join(":")
		};
	}
}
/**
* Factory for creating Docker Model Runner providers using OpenAI-compatible endpoints.
*/
function createDockerProvider(providerPath, options = {}) {
	const apiBaseUrl = (options?.env?.DOCKER_MODEL_RUNNER_BASE_URL ?? getEnvString("DOCKER_MODEL_RUNNER_BASE_URL") ?? "http://localhost:12434") + "/engines/v1";
	const apiKey = options?.env?.DOCKER_MODEL_RUNNER_API_KEY ?? getEnvString("DOCKER_MODEL_RUNNER_API_KEY") ?? "dmr";
	const openaiOptions = {
		...options,
		config: {
			...options.config || {},
			apiBaseUrl,
			apiKey
		}
	};
	const { type, model } = parseProviderPath(providerPath);
	switch (type) {
		case "chat":
		default: return new DMRChatCompletionProvider(model, openaiOptions);
		case "completion": return new DMRCompletionProvider(model, openaiOptions);
		case "embeddings": return new DMREmbeddingProvider(model, openaiOptions);
	}
}
var DMRChatCompletionProvider = class extends OpenAiChatCompletionProvider {
	async callApi(prompt, context, callApiOptions) {
		if (!await hasLocalModel(this.modelName, this.getApiUrl())) logger_default.warn(`Model '${this.modelName}' not found. Run 'docker model pull ${this.modelName}'.`);
		return super.callApi(prompt, context, callApiOptions);
	}
};
var DMRCompletionProvider = class extends OpenAiCompletionProvider {
	async callApi(prompt, context, callApiOptions) {
		if (!await hasLocalModel(this.modelName, this.getApiUrl())) logger_default.warn(`Model '${this.modelName}' not found. Run 'docker model pull ${this.modelName}'.`);
		return super.callApi(prompt, context, callApiOptions);
	}
};
var DMREmbeddingProvider = class extends OpenAiEmbeddingProvider {
	async callEmbeddingApi(text) {
		if (!await hasLocalModel(this.modelName, this.getApiUrl())) logger_default.warn(`Model '${this.modelName}' not found. Run 'docker model pull ${this.modelName}'.`);
		return super.callEmbeddingApi(text);
	}
};

//#endregion
export { createDockerProvider };
//# sourceMappingURL=docker-kvFgJqkg.js.map