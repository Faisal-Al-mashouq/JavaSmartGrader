require('./logger-CMp-NS-e.cjs');
require('./esm-gWVPXn3r.cjs');
require('./pythonUtils-rOCm9w_5.cjs');
require('./types-DY--pQrH.cjs');
require('./util-BNInkV3p.cjs');
require('./fetch-ZkfbmPN0.cjs');
require('./cache-DH-n6i0J.cjs');
require('./genaiTracer-zK0Mtd_D.cjs');
const require_chat = require('./chat-CAz560qk.cjs');
require('./transform-CGHhgwFQ.cjs');
require('./openai-eZz8QsxY.cjs');
require('./util-CHAqsUj1.cjs');
const require_completion = require('./completion-CBZRdGsT.cjs');

//#region src/providers/litellm.ts
/**
* Base class for LiteLLM providers that maintains LiteLLM identity
*/
var LiteLLMProviderWrapper = class {
	provider;
	providerType;
	constructor(provider, providerType) {
		this.provider = provider;
		this.providerType = providerType;
	}
	get modelName() {
		return this.provider.modelName;
	}
	get config() {
		return this.provider.config;
	}
	id() {
		return `litellm${this.providerType === "chat" ? "" : `:${this.providerType}`}:${this.modelName}`;
	}
	toString() {
		return `[LiteLLM Provider${this.providerType === "chat" ? "" : ` ${this.providerType}`} ${this.modelName}]`;
	}
	toJSON() {
		return {
			provider: "litellm",
			model: this.modelName,
			type: this.providerType,
			config: {
				...this.config,
				...this.getApiKey && this.getApiKey() && { apiKey: void 0 }
			}
		};
	}
	async callApi(prompt, context, options) {
		return this.provider.callApi(prompt, context, options);
	}
	getApiKey;
};
/**
* LiteLLM Chat Provider
*/
var LiteLLMChatProvider = class extends LiteLLMProviderWrapper {
	constructor(modelName, options) {
		const provider = new require_chat.OpenAiChatCompletionProvider(modelName, options);
		super(provider, "chat");
		if (provider.getApiKey) this.getApiKey = provider.getApiKey.bind(provider);
	}
};
/**
* LiteLLM Completion Provider
*/
var LiteLLMCompletionProvider = class extends LiteLLMProviderWrapper {
	constructor(modelName, options) {
		const provider = new require_completion.OpenAiCompletionProvider(modelName, options);
		super(provider, "completion");
		if (provider.getApiKey) this.getApiKey = provider.getApiKey.bind(provider);
	}
};
/**
* LiteLLM Embedding Provider
*/
var LiteLLMEmbeddingProvider = class extends LiteLLMProviderWrapper {
	embeddingProvider;
	constructor(modelName, options) {
		const provider = new require_completion.OpenAiEmbeddingProvider(modelName, options);
		super(provider, "embedding");
		this.embeddingProvider = provider;
		if (provider.getApiKey) this.getApiKey = provider.getApiKey.bind(provider);
	}
	async callEmbeddingApi(text) {
		return this.embeddingProvider.callEmbeddingApi(text);
	}
};
var LiteLLMProvider = class extends LiteLLMChatProvider {};
/**
* Creates a LiteLLM provider using OpenAI-compatible endpoints
*
* LiteLLM supports chat, completion, and embedding models through its proxy server.
* All parameters are automatically passed through to the LiteLLM API.
*
* @example
* // Chat model (default)
* createLiteLLMProvider('litellm:gpt-4')
* createLiteLLMProvider('litellm:chat:gpt-4')
*
* // Completion model
* createLiteLLMProvider('litellm:completion:gpt-3.5-turbo-instruct')
*
* // Embedding model
* createLiteLLMProvider('litellm:embedding:text-embedding-3-large')
*/
function createLiteLLMProvider(providerPath, options = {}) {
	const splits = providerPath.split(":");
	const providerType = splits[1];
	const modelName = [
		"chat",
		"completion",
		"embedding",
		"embeddings"
	].includes(providerType) ? splits.slice(2).join(":") : splits.slice(1).join(":");
	const config = options.config?.config || {};
	const mergedConfig = {
		apiKeyEnvar: "LITELLM_API_KEY",
		apiKeyRequired: false,
		apiBaseUrl: "http://0.0.0.0:4000"
	};
	Object.keys(config).forEach((key) => {
		if (config[key] !== void 0 && config[key] !== null) mergedConfig[key] = config[key];
	});
	const litellmConfig = {
		id: options.config?.id,
		label: options.config?.label,
		prompts: options.config?.prompts,
		transform: options.config?.transform,
		delay: options.config?.delay,
		env: options.config?.env,
		config: mergedConfig
	};
	switch (providerType) {
		case "completion": return new LiteLLMCompletionProvider(modelName, litellmConfig);
		case "embedding":
		case "embeddings": return new LiteLLMEmbeddingProvider(modelName, litellmConfig);
		case "chat": return new LiteLLMProvider(modelName, litellmConfig);
		default: return new LiteLLMProvider(modelName, litellmConfig);
	}
}

//#endregion
exports.createLiteLLMProvider = createLiteLLMProvider;
//# sourceMappingURL=litellm-CrWeKkyH.cjs.map