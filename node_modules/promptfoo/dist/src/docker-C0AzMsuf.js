import { A as logger_default, P as getEnvString, c as OpenAiChatCompletionProvider, g as fetchWithCache, n as OpenAiCompletionProvider, s as OpenAiEmbeddingProvider } from "./server/index.js";

//#region src/providers/docker.ts
async function fetchLocalModels(apiBaseUrl) {
	try {
		const { data } = await fetchWithCache(`${apiBaseUrl}/models`, void 0, void 0, "json", true, 0);
		return data?.data ?? [];
	} catch (e) {
		throw new Error(`Failed to connect to Docker Model Runner. Is it enabled? Are the API endpoints enabled? For details, see https://docs.docker.com/ai/model-runner. \n${e.message}`);
	}
}
async function hasLocalModel(modelId, apiBaseUrl) {
	return (await fetchLocalModels(apiBaseUrl)).some((model) => model && model.id?.toLocaleLowerCase() === modelId?.toLocaleLowerCase());
}
function parseProviderPath(providerPath) {
	const splits = providerPath.split(":");
	const type = splits[1];
	switch (type) {
		case "chat":
		case "completion":
		case "embeddings": return {
			type,
			model: splits.slice(2).join(":")
		};
		case "embedding": return {
			type: "embeddings",
			model: splits.slice(2).join(":")
		};
		default: return {
			type: "chat",
			model: splits.slice(1).join(":")
		};
	}
}
/**
* Factory for creating Docker Model Runner providers using OpenAI-compatible endpoints.
*/
function createDockerProvider(providerPath, options = {}) {
	const apiBaseUrl = (options?.env?.DOCKER_MODEL_RUNNER_BASE_URL ?? getEnvString("DOCKER_MODEL_RUNNER_BASE_URL") ?? "http://localhost:12434") + "/engines/v1";
	const apiKey = options?.env?.DOCKER_MODEL_RUNNER_API_KEY ?? getEnvString("DOCKER_MODEL_RUNNER_API_KEY") ?? "dmr";
	const openaiOptions = {
		...options,
		config: {
			...options.config || {},
			apiBaseUrl,
			apiKey
		}
	};
	const { type, model } = parseProviderPath(providerPath);
	switch (type) {
		case "chat":
		default: return new DMRChatCompletionProvider(model, openaiOptions);
		case "completion": return new DMRCompletionProvider(model, openaiOptions);
		case "embeddings": return new DMREmbeddingProvider(model, openaiOptions);
	}
}
var DMRChatCompletionProvider = class extends OpenAiChatCompletionProvider {
	async callApi(prompt, context, callApiOptions) {
		if (!await hasLocalModel(this.modelName, this.getApiUrl())) logger_default.warn(`Model '${this.modelName}' not found. Run 'docker model pull ${this.modelName}'.`);
		return super.callApi(prompt, context, callApiOptions);
	}
};
var DMRCompletionProvider = class extends OpenAiCompletionProvider {
	async callApi(prompt, context, callApiOptions) {
		if (!await hasLocalModel(this.modelName, this.getApiUrl())) logger_default.warn(`Model '${this.modelName}' not found. Run 'docker model pull ${this.modelName}'.`);
		return super.callApi(prompt, context, callApiOptions);
	}
};
var DMREmbeddingProvider = class extends OpenAiEmbeddingProvider {
	async callEmbeddingApi(text) {
		if (!await hasLocalModel(this.modelName, this.getApiUrl())) logger_default.warn(`Model '${this.modelName}' not found. Run 'docker model pull ${this.modelName}'.`);
		return super.callEmbeddingApi(text);
	}
};

//#endregion
export { createDockerProvider };
//# sourceMappingURL=docker-C0AzMsuf.js.map