#!/usr/bin/env node
import "./chunk-DHDDz29n.js";
import "./logger-Bzi5o47S.js";
import "./fetch-TcCC0nEJ.js";
import "./cloud-UwfmA0Ne.js";
import "./types-n3ttAnOa.js";
import "./cache-xb4oZe27.js";
import "./util-CyzW5ayt.js";
import "./esm-Bwuh5HXW.js";
import "./pythonUtils-mprm8p4h.js";
import "./genaiTracer-CqNnnXrE.js";
import { t as OpenAiChatCompletionProvider } from "./chat-BbgGuvwu.js";
import "./transform-B-AYIQmM.js";
import "./openai-D_5h8sBa.js";
import "./util-CwsTKPXi.js";
import { n as OpenAiEmbeddingProvider, t as OpenAiCompletionProvider } from "./completion-BTZ42lqL.js";

//#region src/providers/litellm.ts
/**
* Base class for LiteLLM providers that maintains LiteLLM identity
*/
var LiteLLMProviderWrapper = class {
	provider;
	providerType;
	constructor(provider, providerType) {
		this.provider = provider;
		this.providerType = providerType;
	}
	get modelName() {
		return this.provider.modelName;
	}
	get config() {
		return this.provider.config;
	}
	id() {
		return `litellm${this.providerType === "chat" ? "" : `:${this.providerType}`}:${this.modelName}`;
	}
	toString() {
		return `[LiteLLM Provider${this.providerType === "chat" ? "" : ` ${this.providerType}`} ${this.modelName}]`;
	}
	toJSON() {
		return {
			provider: "litellm",
			model: this.modelName,
			type: this.providerType,
			config: {
				...this.config,
				...this.getApiKey && this.getApiKey() && { apiKey: void 0 }
			}
		};
	}
	async callApi(prompt, context, options) {
		return this.provider.callApi(prompt, context, options);
	}
	getApiKey;
};
/**
* LiteLLM Chat Provider
*/
var LiteLLMChatProvider = class extends LiteLLMProviderWrapper {
	constructor(modelName, options) {
		const provider = new OpenAiChatCompletionProvider(modelName, options);
		super(provider, "chat");
		if (provider.getApiKey) this.getApiKey = provider.getApiKey.bind(provider);
	}
};
/**
* LiteLLM Completion Provider
*/
var LiteLLMCompletionProvider = class extends LiteLLMProviderWrapper {
	constructor(modelName, options) {
		const provider = new OpenAiCompletionProvider(modelName, options);
		super(provider, "completion");
		if (provider.getApiKey) this.getApiKey = provider.getApiKey.bind(provider);
	}
};
/**
* LiteLLM Embedding Provider
*/
var LiteLLMEmbeddingProvider = class extends LiteLLMProviderWrapper {
	embeddingProvider;
	constructor(modelName, options) {
		const provider = new OpenAiEmbeddingProvider(modelName, options);
		super(provider, "embedding");
		this.embeddingProvider = provider;
		if (provider.getApiKey) this.getApiKey = provider.getApiKey.bind(provider);
	}
	async callEmbeddingApi(text) {
		return this.embeddingProvider.callEmbeddingApi(text);
	}
};
var LiteLLMProvider = class extends LiteLLMChatProvider {};
/**
* Creates a LiteLLM provider using OpenAI-compatible endpoints
*
* LiteLLM supports chat, completion, and embedding models through its proxy server.
* All parameters are automatically passed through to the LiteLLM API.
*
* @example
* // Chat model (default)
* createLiteLLMProvider('litellm:gpt-4')
* createLiteLLMProvider('litellm:chat:gpt-4')
*
* // Completion model
* createLiteLLMProvider('litellm:completion:gpt-3.5-turbo-instruct')
*
* // Embedding model
* createLiteLLMProvider('litellm:embedding:text-embedding-3-large')
*/
function createLiteLLMProvider(providerPath, options = {}) {
	const splits = providerPath.split(":");
	const providerType = splits[1];
	const modelName = [
		"chat",
		"completion",
		"embedding",
		"embeddings"
	].includes(providerType) ? splits.slice(2).join(":") : splits.slice(1).join(":");
	const config = options.config?.config || {};
	const mergedConfig = {
		apiKeyEnvar: "LITELLM_API_KEY",
		apiKeyRequired: false,
		apiBaseUrl: "http://0.0.0.0:4000"
	};
	Object.keys(config).forEach((key) => {
		if (config[key] !== void 0 && config[key] !== null) mergedConfig[key] = config[key];
	});
	const litellmConfig = {
		id: options.config?.id,
		label: options.config?.label,
		prompts: options.config?.prompts,
		transform: options.config?.transform,
		delay: options.config?.delay,
		env: options.config?.env,
		config: mergedConfig
	};
	switch (providerType) {
		case "completion": return new LiteLLMCompletionProvider(modelName, litellmConfig);
		case "embedding":
		case "embeddings": return new LiteLLMEmbeddingProvider(modelName, litellmConfig);
		case "chat": return new LiteLLMProvider(modelName, litellmConfig);
		default: return new LiteLLMProvider(modelName, litellmConfig);
	}
}

//#endregion
export { createLiteLLMProvider };
//# sourceMappingURL=litellm-CR8i8YRn.js.map