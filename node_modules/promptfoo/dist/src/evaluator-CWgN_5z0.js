#!/usr/bin/env node
import { C as getEnvString, E as isCI, O as cliState_default, S as getEnvInt, T as getMaxEvalTimeMs, _ as summarizeEvaluateResultForLogging, b as getEnvBool, f as extractJsonObjects, g as safeJsonStringify, o as logger_default, p as getAjv, w as getEvalTimeoutMs } from "./logger-Bzi5o47S.js";
import { E as FILE_METADATA_KEY, T as VERSION, d as parseChatPrompt, h as DEFAULT_MAX_CONCURRENCY, n as fetchWithRetries, u as isPromptfooSampleTarget } from "./fetch-CdID49pr.js";
import { t as invariant } from "./invariant-DT20jrBd.js";
import { n as sleep } from "./time-CoOwN3kQ.js";
import { r as telemetry_default } from "./telemetry-ZbgTumMK.js";
import { d as isGradingResult, p as isApiProvider, s as ResultFailureReason } from "./types-BDVhkn1l.js";
import { c as promptYesNo } from "./server-D-2mB-Yu.js";
import { A as runExtensionHook, C as getSessionId, F as createRateLimitRegistry, G as VertexChatProvider, I as createProviderRateLimitOptions, K as AIStudioChatProvider, M as loadFromPackage, N as redteamProviderManager, O as collectFileMetadata, P as TokenUsageTracker, T as isBasicRefusal, j as isPackagePath, k as renderPrompt, u as GoogleLiveProvider } from "./providers-36egbUej.js";
import { o as getCache } from "./cache-DPtUPxsV.js";
import { i as isJavascriptFile } from "./fileExtensions-BpuMmaFL.js";
import { E as parseFileUrl, F as isGoogleProvider, I as isOpenAiProvider, L as isProviderAllowed, P as isAnthropicProvider, T as loadFunction, g as maybeLoadToolsFromExternalFile, w as getNunjucksEngine } from "./util-DOcN7fV7.js";
import { r as runPython } from "./pythonUtils-mprm8p4h.js";
import { n as transform, r as getProcessShim, t as TransformInputType } from "./transform-BzepVlkX.js";
import { $ as matchesSearchRubric, B as getAndCheckProvider, G as matchesContextFaithfulness, H as matchesAnswerRelevance, J as matchesFactuality, K as matchesContextRecall, Q as matchesPiScore, R as callProviderWithContext, U as matchesClassification, V as loadRubricPrompt, W as matchesClosedQa, X as matchesLlmRubric, Y as matchesGEval, Z as matchesModeration, at as DefaultSuggestionsProvider, dt as getFinalTest, et as matchesSelectBest, ft as loadFromJavaScriptFile, it as getDefaultProviders, lt as SUGGEST_PROMPTS_SYSTEM_MESSAGE, mt as resolveContext, n as getGraderById, nt as selectMaxScore, pt as processFileReference, q as matchesContextRelevance, tt as matchesSimilarity, ut as coerceString, z as fail } from "./graders-fl4oDEbO.js";
import { i as sha256, t as extractAndStoreBinaryData } from "./extractor-CqaKyaAe.js";
import { t as OpenAiChatCompletionProvider } from "./chat-B-HICxQx.js";
import { a as createEmptyTokenUsage, i as createEmptyAssertions, n as accumulateResponseTokenUsage, o as normalizeTokenUsage, r as accumulateTokenUsage, t as accumulateAssertionTokenUsage } from "./tokenUsageUtils-C9fIWtvn.js";
import { f as validateFunctionCall } from "./transform-BEceP7Wd.js";
import { l as validateFunctionCall$1 } from "./util-CgOQNts4.js";
import { n as getTraceStore } from "./store-DUHdxsjF.js";
import { t as providerRegistry } from "./providerRegistry-gU7gOAJI.js";
import { n as runRuby } from "./rubyUtils-Cus5EGPX.js";
import { a as getActualPromptWithFallback, r as updateSignalFile } from "./signal-DIkHzXU3.js";
import chalk from "chalk";
import fs, { createWriteStream } from "fs";
import path from "path";
import os from "os";
import yaml from "js-yaml";
import util from "util";
import { randomBytes } from "crypto";
import { globSync } from "glob";
import { XMLParser } from "fast-xml-parser";
import async from "async";
import { DiagConsoleLogger, DiagLogLevel, diag, propagation } from "@opentelemetry/api";
import cliProgress from "cli-progress";
import { JSDOM } from "jsdom";
import { distance } from "fastest-levenshtein";
import * as rouge from "js-rouge";
import { ExportResultCode, W3CTraceContextPropagator } from "@opentelemetry/core";
import { OTLPTraceExporter } from "@opentelemetry/exporter-trace-otlp-http";
import { resourceFromAttributes } from "@opentelemetry/resources";
import { BatchSpanProcessor, NodeTracerProvider } from "@opentelemetry/sdk-trace-node";
import { ATTR_SERVICE_NAME, ATTR_SERVICE_VERSION } from "@opentelemetry/semantic-conventions";

//#region src/external/matchers/conversationRelevancyTemplate.ts
var ConversationRelevancyTemplate = class {
	static generateVerdicts(slidingWindow) {
		return `Based on the given list of message exchanges between a user and an LLM, generate a JSON object to indicate whether the LAST \`assistant\` message is relevant to context in messages. The JSON will have 2 fields: 'verdict' and 'reason'.
The 'verdict' key should STRICTLY be either 'yes' or 'no', which states whether the last \`assistant\` message is relevant according to the context in messages 
Provide a 'reason' ONLY if the answer is 'no'. 
You MUST USE the previous messages (if any) provided in the list of messages to make an informed judgement on relevancy.

**
IMPORTANT: Please make sure to only return in JSON format.
Example Messages:
[
    {
        "role": "user",
        "content": "Hi! I have something I want to tell you"
    },
    {
        "role": "assistant",
        "content": "Sure, what is it?"
    },
    {
        "role": "user",
        "content": "I've a sore throat, what meds should I take?"
    },
    {
        "role": "assistant",
        "content": "Not sure, but isn't it a nice day today?"
    }
]

Example JSON:
{
    "verdict": "no",
    "reason": "The LLM responded 'isn't it a nice day today' to a message that asked about how to treat a sore throat, which is completely irrelevant."
}
===== END OF EXAMPLE ======
You MUST ONLY provide a verdict for the LAST message on the list but MUST USE context from the previous messages.
You DON'T have to provide a reason if the answer is 'yes'.
ONLY provide a 'no' answer if the LLM response is COMPLETELY irrelevant to the message input.
Vague LLM responses to vague inputs, such as greetings DOES NOT count as irrelevancies!
**

Messages:
${JSON.stringify(slidingWindow, null, 2)}

JSON:`;
	}
	static generateReason(score, irrelevancies) {
		return `Below is a list of irrelevancies drawn from some messages in a conversation, which you have minimal knowledge of. It is a list of strings explaining why the 'assistant' messages are irrelevant to the 'user' messages.
Given the relevancy score, which is a 0-1 score indicating how irrelevant the OVERALL AI messages are in a conversation (higher the better), CONCISELY summarize the irrelevancies to justify the score. 

** 
IMPORTANT: Please make sure to only return in JSON format, with the 'reason' key providing the reason.
Example JSON:
{
    "reason": "The score is <relevancy_score> because <your_reason>."
}

Always quote WHICH MESSAGE and the INFORMATION in the reason in your final reason.
Be sure in your reason, as if you know what the \`assistant\` messages from messages in a conversation is from the irrelevancies.
**

Relevancy Score:
${score}

Irrelevancies:
${JSON.stringify(irrelevancies, null, 2)}

JSON:`;
	}
};

//#endregion
//#region src/external/matchers/deepeval.ts
const nunjucks$1 = getNunjucksEngine(void 0, false, true);
async function matchesConversationRelevance(messages, threshold, vars, grading, providerCallContext) {
	const textProvider = await getAndCheckProvider("text", grading?.provider, (await getDefaultProviders()).gradingProvider, "conversation relevancy check");
	const renderedMessages = messages.map((msg) => ({
		input: typeof msg.input === "string" && vars ? nunjucks$1.renderString(msg.input, vars) : msg.input,
		output: typeof msg.output === "string" && vars ? nunjucks$1.renderString(msg.output, vars) : msg.output
	}));
	const messageRoles = [];
	for (const msg of renderedMessages) {
		messageRoles.push({
			role: "user",
			content: typeof msg.input === "string" ? msg.input : JSON.stringify(msg.input)
		});
		messageRoles.push({
			role: "assistant",
			content: typeof msg.output === "string" ? msg.output : JSON.stringify(msg.output)
		});
	}
	const loadedRubricPrompt = grading?.rubricPrompt ? await loadRubricPrompt(grading.rubricPrompt, "") : "";
	let promptText;
	if (loadedRubricPrompt) promptText = nunjucks$1.renderString(loadedRubricPrompt, {
		messages: renderedMessages,
		...vars || {}
	});
	else promptText = ConversationRelevancyTemplate.generateVerdicts(messageRoles);
	const resp = await callProviderWithContext(textProvider, promptText, "conversation-relevance", {
		messages: renderedMessages,
		...vars || {}
	}, providerCallContext);
	if (resp.error || !resp.output) return fail(resp.error || "No output", resp.tokenUsage);
	invariant(typeof resp.output === "string", "conversation relevancy check produced malformed response");
	try {
		const jsonObjects = extractJsonObjects(resp.output);
		if (jsonObjects.length === 0) throw new Error("No JSON object found in response");
		const result = jsonObjects[0];
		const pass = result.verdict === "yes";
		const score = pass ? 1 : 0;
		return {
			pass: score >= threshold - Number.EPSILON,
			score,
			reason: result.reason || `Response ${pass ? "is" : "is not"} relevant to the conversation context`,
			tokensUsed: resp.tokenUsage
		};
	} catch (err) {
		return fail(`Error parsing output: ${err.message}`, resp.tokenUsage);
	}
}

//#endregion
//#region src/external/assertions/deepeval.ts
const DEFAULT_WINDOW_SIZE = 5;
const handleConversationRelevance = async ({ assertion, outputString, prompt, providerCallContext, test }) => {
	let messages = [];
	if (test.vars?._conversation && test.vars._conversation.length > 0) messages = test.vars?._conversation;
	else {
		invariant(typeof outputString === "string", "conversational-relevance assertion type must have a string value");
		invariant(prompt, "conversational-relevance assertion type must have a prompt");
		messages = [{
			input: prompt,
			output: outputString
		}];
	}
	const windowSize = assertion.config?.windowSize || DEFAULT_WINDOW_SIZE;
	const threshold = assertion.threshold || 0;
	let relevantCount = 0;
	let totalWindows = 0;
	const irrelevancies = [];
	const tokensUsed = createEmptyTokenUsage();
	for (let i = 0; i < messages.length; i++) {
		const result = await matchesConversationRelevance(messages.slice(Math.max(0, i - windowSize + 1), i + 1), 1, test.vars, test.options, providerCallContext);
		if (result.pass) relevantCount++;
		else if (result.reason && result.reason !== "Response is not relevant to the conversation context") irrelevancies.push(result.reason);
		if (result.tokensUsed) accumulateTokenUsage(tokensUsed, result.tokensUsed);
		totalWindows++;
	}
	const score = totalWindows > 0 ? relevantCount / totalWindows : 0;
	const pass = score >= threshold - Number.EPSILON;
	let reason;
	if (irrelevancies.length > 0 && score < 1) {
		const resp = await callProviderWithContext(await getAndCheckProvider("text", test.options?.provider, (await getDefaultProviders()).gradingProvider, "conversation relevancy reason generation"), ConversationRelevancyTemplate.generateReason(score, irrelevancies), "conversation-relevance-reason", {
			score: String(score),
			irrelevancies
		}, providerCallContext);
		if (resp.output && typeof resp.output === "string") {
			try {
				const jsonObjects = extractJsonObjects(resp.output);
				if (jsonObjects.length > 0) reason = jsonObjects[0].reason || `${relevantCount} out of ${totalWindows} conversation windows were relevant`;
				else reason = `${relevantCount} out of ${totalWindows} conversation windows were relevant`;
			} catch {
				reason = `${relevantCount} out of ${totalWindows} conversation windows were relevant`;
			}
			if (resp.tokenUsage) accumulateTokenUsage(tokensUsed, resp.tokenUsage);
		} else reason = `${relevantCount} out of ${totalWindows} conversation windows were relevant`;
	} else reason = `${relevantCount} out of ${totalWindows} conversation windows were relevant`;
	return {
		assertion,
		pass,
		score,
		reason,
		tokensUsed: tokensUsed.total > 0 ? tokensUsed : void 0
	};
};

//#endregion
//#region src/tracing/evaluatorTracing.ts
let otlpReceiverStarted = false;
/**
* Generate a 16-byte trace ID
*/
function generateTraceId() {
	return randomBytes(16).toString("hex");
}
/**
* Generate an 8-byte span ID
*/
function generateSpanId() {
	return randomBytes(8).toString("hex");
}
/**
* Generate W3C Trace Context format traceparent header
* Format: version-trace-id-parent-id-trace-flags
*/
function generateTraceparent(traceId, spanId, sampled = true) {
	return `00-${traceId}-${spanId}-${sampled ? "01" : "00"}`;
}
/**
* Check if the OTLP receiver has been started
*/
function isOtlpReceiverStarted() {
	return otlpReceiverStarted;
}
/**
* Start the OTLP receiver if tracing is enabled and it hasn't been started yet
*/
async function startOtlpReceiverIfNeeded(testSuite) {
	logger_default.debug(`[EvaluatorTracing] Checking tracing config: ${JSON.stringify(testSuite.tracing)}`);
	logger_default.debug(`[EvaluatorTracing] testSuite keys: ${Object.keys(testSuite)}`);
	logger_default.debug(`[EvaluatorTracing] Full testSuite.tracing: ${JSON.stringify(testSuite.tracing, null, 2)}`);
	if (testSuite.tracing?.enabled && testSuite.tracing?.otlp?.http?.enabled && !otlpReceiverStarted) {
		telemetry_default.record("feature_used", { feature: "tracing" });
		try {
			logger_default.debug("[EvaluatorTracing] Tracing configuration detected, starting OTLP receiver");
			const { startOTLPReceiver } = await import("./otlpReceiver-pQLthYSL.js");
			const port = testSuite.tracing.otlp.http.port || 4318;
			const host = testSuite.tracing.otlp.http.host || "127.0.0.1";
			logger_default.debug(`[EvaluatorTracing] Starting OTLP receiver on ${host}:${port}`);
			await startOTLPReceiver(port, host);
			otlpReceiverStarted = true;
			logger_default.info(`[EvaluatorTracing] OTLP receiver successfully started on port ${port} for tracing`);
		} catch (error) {
			logger_default.error(`[EvaluatorTracing] Failed to start OTLP receiver: ${error}`);
		}
	} else if (otlpReceiverStarted) logger_default.debug("[EvaluatorTracing] OTLP receiver already started, skipping initialization");
	else {
		logger_default.debug("[EvaluatorTracing] Tracing not enabled or OTLP HTTP receiver not configured");
		logger_default.debug(`[EvaluatorTracing] tracing.enabled: ${testSuite.tracing?.enabled}`);
		logger_default.debug(`[EvaluatorTracing] tracing.otlp.http.enabled: ${testSuite.tracing?.otlp?.http?.enabled}`);
	}
}
/**
* Stop the OTLP receiver if it was started
*/
async function stopOtlpReceiverIfNeeded() {
	if (otlpReceiverStarted) try {
		logger_default.debug("[EvaluatorTracing] Stopping OTLP receiver");
		const { stopOTLPReceiver } = await import("./otlpReceiver-pQLthYSL.js");
		await stopOTLPReceiver();
		otlpReceiverStarted = false;
		logger_default.info("[EvaluatorTracing] OTLP receiver stopped successfully");
	} catch (error) {
		logger_default.error(`[EvaluatorTracing] Failed to stop OTLP receiver: ${error}`);
	}
}
/**
* Check if tracing is enabled for a test case
*
* Tracing is enabled if any of the following are true:
* 1. Test case metadata has `tracingEnabled: true`
* 2. TestSuite YAML config has `tracing.enabled: true`
* 3. Environment variable `PROMPTFOO_TRACING_ENABLED` is set to true
*/
function isTracingEnabled(test, testSuite) {
	const metadataEnabled = test.metadata?.tracingEnabled === true;
	const yamlConfigEnabled = testSuite?.tracing?.enabled === true;
	const envEnabled = getEnvBool("PROMPTFOO_TRACING_ENABLED", false);
	const result = metadataEnabled || yamlConfigEnabled || envEnabled;
	logger_default.debug(`[EvaluatorTracing] isTracingEnabled check: metadata=${metadataEnabled}, yamlConfig=${yamlConfigEnabled}, env=${envEnabled}, result=${result}`);
	return result;
}
/**
* Generate trace context and create trace record if tracing is enabled
*/
async function generateTraceContextIfNeeded(test, evaluateOptions, testIdx, promptIdx, testSuite) {
	const tracingEnabled = isTracingEnabled(test, testSuite);
	if (tracingEnabled) {
		logger_default.debug("[EvaluatorTracing] Tracing enabled for test case");
		logger_default.debug(`[EvaluatorTracing] Test metadata: ${JSON.stringify(test.metadata)}`);
	}
	if (!tracingEnabled) return null;
	logger_default.debug("[EvaluatorTracing] Importing trace store");
	const { getTraceStore } = await import("./store-CgRsGdCj.js");
	const traceStore = getTraceStore();
	const traceId = generateTraceId();
	const spanId = generateSpanId();
	const traceparent = generateTraceparent(traceId, spanId);
	logger_default.debug(`[EvaluatorTracing] Generated trace context: traceId=${traceId}, spanId=${spanId}`);
	let evaluationId = test.metadata?.evaluationId || evaluateOptions?.eventSource;
	if (!evaluationId) {
		logger_default.warn("[EvaluatorTracing] No evaluation ID found in test metadata or evaluateOptions, trace will not be linked to evaluation");
		evaluationId = `eval-${Date.now()}`;
	}
	const testCaseId = test.metadata?.testCaseId || test.id || `${testIdx}-${promptIdx}`;
	try {
		logger_default.debug(`[EvaluatorTracing] Creating trace record for traceId=${traceId}`);
		await traceStore.createTrace({
			traceId,
			evaluationId: evaluationId || "",
			testCaseId: testCaseId || "",
			metadata: {
				testIdx,
				promptIdx,
				vars: test.vars
			}
		});
		logger_default.debug("[EvaluatorTracing] Trace record created successfully");
	} catch (error) {
		logger_default.error(`[EvaluatorTracing] Failed to create trace: ${error}`);
	}
	logger_default.debug(`[EvaluatorTracing] Trace context ready: ${traceparent} for test case ${testCaseId}`);
	return {
		traceparent,
		evaluationId,
		testCaseId
	};
}

//#endregion
//#region src/assertions/answerRelevance.ts
const handleAnswerRelevance = async ({ assertion, output, prompt, test, providerCallContext }) => {
	invariant(typeof output === "string", "answer-relevance assertion type must evaluate a string output");
	invariant(prompt, "answer-relevance assertion type must have a prompt");
	return {
		assertion,
		...await matchesAnswerRelevance(typeof test?.vars?.query === "string" ? test.vars.query : prompt, output, assertion.threshold ?? 0, test.options, providerCallContext)
	};
};

//#endregion
//#region src/assertions/assertionsResult.ts
const GUARDRAIL_BLOCKED_REASON = "Content failed guardrail safety checks";
const DEFAULT_TOKENS_USED = {
	total: 0,
	prompt: 0,
	completion: 0,
	cached: 0,
	numRequests: 0
};
var AssertionsResult = class {
	static noAssertsResult() {
		return {
			pass: true,
			score: 1,
			reason: "No assertions",
			tokensUsed: { ...DEFAULT_TOKENS_USED }
		};
	}
	tokensUsed = { ...DEFAULT_TOKENS_USED };
	threshold;
	_parentAssertionSet;
	totalScore = 0;
	totalWeight = 0;
	failedReason;
	componentResults = [];
	namedScores = {};
	result = null;
	failedContentSafetyChecks = false;
	constructor({ threshold, parentAssertionSet } = {}) {
		this.threshold = threshold;
		this._parentAssertionSet = parentAssertionSet;
	}
	get parentAssertionSet() {
		return this._parentAssertionSet;
	}
	addResult({ index, result, metric, weight = 1 }) {
		this.totalScore += result.score * weight;
		this.totalWeight += weight;
		this.componentResults[index] = result;
		if (result.assertion?.type === "guardrails" && result.assertion?.config?.purpose === "redteam" && !result.pass) this.failedContentSafetyChecks = true;
		if (metric) this.namedScores[metric] = (this.namedScores[metric] || 0) + result.score;
		if (result.namedScores) Object.entries(result.namedScores).forEach(([metricName, score]) => {
			if (metricName !== metric) this.namedScores[metricName] = (this.namedScores[metricName] || 0) + score;
		});
		if (result.tokensUsed) {
			this.tokensUsed.total += result.tokensUsed.total || 0;
			this.tokensUsed.prompt += result.tokensUsed.prompt || 0;
			this.tokensUsed.completion += result.tokensUsed.completion || 0;
			this.tokensUsed.cached += result.tokensUsed.cached || 0;
			this.tokensUsed.numRequests += result.tokensUsed.numRequests || 0;
		}
		if (result.pass) return;
		this.failedReason = result.reason;
		if (getEnvBool("PROMPTFOO_SHORT_CIRCUIT_TEST_FAILURES")) throw new Error(result.reason);
	}
	async testResult(scoringFunction) {
		if (this.result) return this.result;
		const score = this.totalWeight > 0 ? this.totalScore / this.totalWeight : 0;
		let pass = !this.failedReason;
		let reason = this.failedReason ? this.failedReason : "All assertions passed";
		if (this.threshold) {
			pass = score >= this.threshold;
			if (pass) reason = `Aggregate score ${score.toFixed(2)} â‰¥ ${this.threshold} threshold`;
			else reason = `Aggregate score ${score.toFixed(2)} < ${this.threshold} threshold`;
		}
		if (this.failedContentSafetyChecks) {
			pass = true;
			reason = GUARDRAIL_BLOCKED_REASON;
		}
		const flattenedComponentResults = this.componentResults.flatMap((result) => {
			if (result.componentResults) return [result, ...result.componentResults.map((subResult) => ({
				...subResult,
				assertion: subResult.assertion || result.assertion
			}))];
			else return result;
		});
		this.result = {
			pass,
			score,
			reason,
			namedScores: this.namedScores,
			tokensUsed: this.tokensUsed,
			componentResults: flattenedComponentResults
		};
		if (scoringFunction) try {
			const scoringResult = await scoringFunction(this.namedScores, {
				threshold: this.threshold,
				parentAssertionSet: this._parentAssertionSet,
				componentResults: flattenedComponentResults,
				tokensUsed: this.tokensUsed
			});
			if (!isGradingResult(scoringResult)) throw new Error("assertion scoring function must return a GradingResult");
			this.result = {
				...this.result,
				...scoringResult
			};
		} catch (err) {
			this.result.pass = false;
			this.result.score = 0;
			this.result.reason = `Scoring function error: ${err.message}`;
		}
		return this.result;
	}
};

//#endregion
//#region src/assertions/ngrams.ts
/**
* Utility function to generate contiguous n-grams from an array of words.
*
* @param words - Array of words.
* @param n - The n-gram length.
* @returns An array of n-grams, each represented as a string.
*/
function getNGrams(words, n) {
	if (n <= 0 || n > words.length) return [];
	const ngrams = [];
	for (let i = 0; i <= words.length - n; i++) ngrams.push(words.slice(i, i + n).join(" "));
	return ngrams;
}

//#endregion
//#region src/assertions/bleu.ts
/**
* BLEU (Bilingual Evaluation Understudy) Score Implementation
*
* Implementation based on:
* Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002).
* "BLEU: a method for automatic evaluation of machine translation."
* In Proceedings of the 40th Annual Meeting of the ACL, pp. 311-318.
*
* {@link https://doi.org/10.3115/1073083.1073135}
*/
/**
* Calculates the brevity penalty for BLEU score.
* Penalizes translations that are shorter than the reference.
*
* @param candidateLength - Length of candidate translation
* @param referenceLength - Length of reference translation
* @returns Brevity penalty score between 0 and 1
* @internal
*/
function calculateBrevityPenalty(candidateLength, referenceLength) {
	if (candidateLength > referenceLength) return 1;
	return Math.exp(1 - candidateLength / referenceLength);
}
/**
* Calculates BLEU score for a candidate string against reference strings.
*
* @param candidate - The string to evaluate
* @param references - Array of reference strings to compare against
* @param weights - Weights for each n-gram precision (1-gram to 4-gram)
* @returns BLEU score between 0 and 1
* @throws When inputs are invalid or weights don't sum to 1
*/
function calculateBleuScore(candidate, references, weights = [
	.25,
	.25,
	.25,
	.25
]) {
	if (!candidate || references.length === 0 || weights.length !== 4) throw new Error("Invalid inputs");
	if (Math.abs(weights.reduce((a, b) => a + b) - 1) > 1e-4) throw new Error("Weights must sum to 1");
	const candidateWords = candidate.toLowerCase().trim().split(/\s+/);
	const closestRefLength = references.map((ref) => ref.toLowerCase().trim().split(/\s+/).length).reduce((prev, curr) => Math.abs(curr - candidateWords.length) < Math.abs(prev - candidateWords.length) ? curr : prev);
	const maxN = 4;
	const precisions = [];
	for (let n = 1; n <= maxN; n++) {
		const candidateNGrams = getNGrams(candidateWords, n);
		let maxClippedCount = 0;
		const totalCount = candidateNGrams.length;
		for (const reference of references) {
			const referenceNGrams = getNGrams(reference.toLowerCase().trim().split(/\s+/), n);
			const candidateNGramCounts = /* @__PURE__ */ new Map();
			const referenceNGramCounts = /* @__PURE__ */ new Map();
			for (const gram of referenceNGrams) referenceNGramCounts.set(gram, (referenceNGramCounts.get(gram) || 0) + 1);
			for (const gram of candidateNGrams) candidateNGramCounts.set(gram, (candidateNGramCounts.get(gram) || 0) + 1);
			let clippedCount = 0;
			for (const [gram, count] of candidateNGramCounts.entries()) {
				const refCount = referenceNGramCounts.get(gram) || 0;
				clippedCount += Math.min(count, refCount);
			}
			maxClippedCount = Math.max(maxClippedCount, clippedCount);
		}
		const precision = totalCount > 0 ? maxClippedCount / totalCount : 0;
		precisions.push(precision);
	}
	const bp = calculateBrevityPenalty(candidateWords.length, closestRefLength);
	const weightedScore = precisions.reduce((acc, p, i) => {
		const smoothedP = p === 0 ? 1e-7 : p;
		return acc + weights[i] * Math.log(smoothedP);
	}, 0);
	return bp * Math.exp(weightedScore);
}
/**
* Handles BLEU score assertion for promptfoo.
* Compares output against reference(s) using BLEU metric.
*
* @param assertion - The assertion configuration
* @param inverse - Whether to invert the comparison
* @param outputString - Actual output to evaluate
* @param renderedValue - Expected output(s)
* @returns Result of the BLEU score comparison
*/
function handleBleuScore({ assertion, inverse, outputString, renderedValue }) {
	invariant(typeof renderedValue === "string" || Array.isArray(renderedValue) && renderedValue.every((v) => typeof v === "string"), "\"bleu\" assertion type must have a string or array of strings value");
	const threshold = assertion.threshold ?? .5;
	const score = calculateBleuScore(outputString, Array.isArray(renderedValue) ? renderedValue : [renderedValue]);
	const pass = score >= threshold !== inverse;
	return {
		pass,
		score: inverse ? 1 - score : score,
		reason: pass ? "Assertion passed" : `BLEU score ${score.toFixed(4)} is ${inverse ? "greater" : "less"} than threshold ${threshold}`,
		assertion
	};
}

//#endregion
//#region src/assertions/classifier.ts
async function handleClassifier({ assertion, renderedValue, outputString, test, inverse }) {
	invariant(typeof renderedValue === "string" || typeof renderedValue === "undefined", "\"classifier\" assertion type must have a string value or be undefined");
	const classificationResult = await matchesClassification(renderedValue, outputString, assertion.threshold ?? 1, test.options);
	if (inverse) {
		classificationResult.pass = !classificationResult.pass;
		classificationResult.score = 1 - classificationResult.score;
	}
	return {
		assertion,
		...classificationResult
	};
}

//#endregion
//#region src/assertions/contains.ts
const handleContains = ({ assertion, renderedValue, valueFromScript, outputString, inverse }) => {
	const value = valueFromScript ?? renderedValue;
	invariant(value, "\"contains\" assertion type must have a string or number value");
	invariant(typeof value === "string" || typeof value === "number", "\"contains\" assertion type must have a string or number value");
	const pass = outputString.includes(String(value)) !== inverse;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Expected output to ${inverse ? "not " : ""}contain "${value}"`,
		assertion
	};
};
const handleIContains = ({ assertion, renderedValue, valueFromScript, outputString, inverse }) => {
	const value = valueFromScript ?? renderedValue;
	invariant(value, "\"icontains\" assertion type must have a string or number value");
	invariant(typeof value === "string" || typeof value === "number", "\"icontains\" assertion type must have a string or number value");
	const pass = outputString.toLowerCase().includes(String(value).toLowerCase()) !== inverse;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Expected output to ${inverse ? "not " : ""}contain "${value}"`,
		assertion
	};
};
const handleContainsAny = ({ assertion, renderedValue, valueFromScript, outputString, inverse }) => {
	let value = valueFromScript ?? renderedValue;
	invariant(value, "\"contains-any\" assertion type must have a value");
	if (typeof value === "string") value = value.match(/(".*?"|[^,]+)(?=\s*,|\s*$)/g)?.map((v) => v.trim().replace(/^"|"$/g, "")) ?? [];
	invariant(Array.isArray(value), "\"contains-any\" assertion type must have an array value");
	const pass = value.some((v) => outputString.includes(String(v))) !== inverse;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Expected output to ${inverse ? "not " : ""}contain one of "${value.join(", ")}"`,
		assertion
	};
};
const handleIContainsAny = ({ assertion, renderedValue, valueFromScript, outputString, inverse }) => {
	let value = valueFromScript ?? renderedValue;
	invariant(value, "\"icontains-any\" assertion type must have a value");
	if (typeof value === "string") value = value.split(",").map((v) => v.trim());
	invariant(Array.isArray(value), "\"icontains-any\" assertion type must have an array value");
	const pass = value.some((v) => outputString.toLowerCase().includes(String(v).toLowerCase())) !== inverse;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Expected output to ${inverse ? "not " : ""}contain one of "${value.join(", ")}"`,
		assertion
	};
};
const handleContainsAll = ({ assertion, renderedValue, valueFromScript, outputString, inverse }) => {
	let value = valueFromScript ?? renderedValue;
	invariant(value, "\"contains-all\" assertion type must have a value");
	if (typeof value === "string") value = value.split(",").map((v) => v.trim());
	invariant(Array.isArray(value), "\"contains-all\" assertion type must have an array value");
	const missingStrings = value.filter((v) => !outputString.includes(String(v)));
	const pass = missingStrings.length === 0 !== inverse;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Expected output to ${inverse ? "not " : ""}contain all of [${value.join(", ")}]. Missing: [${missingStrings.join(", ")}]`,
		assertion
	};
};
const handleIContainsAll = ({ assertion, renderedValue, valueFromScript, outputString, inverse }) => {
	let value = valueFromScript ?? renderedValue;
	invariant(value, "\"icontains-all\" assertion type must have a value");
	if (typeof value === "string") value = value.split(",").map((v) => v.trim());
	invariant(Array.isArray(value), "\"icontains-all\" assertion type must have an array value");
	const missingStrings = value.filter((v) => !outputString.toLowerCase().includes(String(v).toLowerCase()));
	const pass = missingStrings.length === 0 !== inverse;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Expected output to ${inverse ? "not " : ""}contain all of [${value.join(", ")}]. Missing: [${missingStrings.join(", ")}]`,
		assertion
	};
};

//#endregion
//#region src/assertions/contextFaithfulness.ts
/**
* Handles context-faithfulness assertions by evaluating whether the LLM output
* is faithful to the provided context without hallucinations.
*
* Supports extracting context from provider responses using contextTransform
* or from test variables.
*
* @param params - Assertion parameters including test case, output, and configuration
* @returns Promise resolving to grading result with pass/fail and score
*/
async function handleContextFaithfulness({ assertion, test, output, prompt, providerResponse, providerCallContext }) {
	invariant(test.vars, "context-faithfulness assertion requires a test with variables");
	invariant(typeof test.vars.query === "string", "context-faithfulness assertion requires a \"query\" variable with the user question");
	invariant(typeof output === "string", "context-faithfulness assertion requires string output from the provider");
	const context = await resolveContext(assertion, test, output, prompt, void 0, providerResponse);
	return {
		assertion,
		...await matchesContextFaithfulness(test.vars.query, output, context, assertion.threshold ?? 0, test.options, test.vars, providerCallContext),
		metadata: { context }
	};
}

//#endregion
//#region src/assertions/contextRecall.ts
/**
* Handles context-recall assertions by evaluating whether the provided context
* contains the information needed to answer the expected value/question.
*
* Supports extracting context from provider responses using contextTransform
* or from test variables (falls back to prompt if no context variable).
*
* @param params - Assertion parameters including test case, output, and configuration
* @returns Promise resolving to grading result with pass/fail and score
*/
const handleContextRecall = async ({ assertion, renderedValue, prompt, test, output, providerResponse, providerCallContext }) => {
	invariant(typeof renderedValue === "string", "context-recall assertion requires a string value (expected answer or fact to verify)");
	invariant(prompt, "context-recall assertion requires a prompt");
	const context = await resolveContext(assertion, test, output, prompt, prompt, providerResponse);
	const result = await matchesContextRecall(context, renderedValue, assertion.threshold ?? 0, test.options, test.vars, providerCallContext);
	return {
		assertion,
		...result,
		metadata: {
			context,
			...result.metadata || {}
		}
	};
};

//#endregion
//#region src/assertions/contextRelevance.ts
/**
* Handles context-relevance assertions by evaluating whether the provided context
* is relevant to the given query/question.
*
* Supports extracting context from provider responses using contextTransform
* or from test variables.
*
* @param params - Assertion parameters including test case, output, and configuration
* @returns Promise resolving to grading result with pass/fail and score
*/
const handleContextRelevance = async ({ assertion, test, output, prompt, providerResponse, providerCallContext }) => {
	invariant(test.vars, "context-relevance assertion requires a test with variables");
	invariant(typeof test.vars.query === "string", "context-relevance assertion requires a \"query\" variable with the user question");
	const context = await resolveContext(assertion, test, output, prompt, void 0, providerResponse);
	const result = await matchesContextRelevance(test.vars.query, context, assertion.threshold ?? 0, test.options, providerCallContext);
	return {
		assertion,
		...result,
		metadata: {
			context,
			...result.metadata || {}
		}
	};
};

//#endregion
//#region src/assertions/cost.ts
const handleCost = ({ cost, assertion }) => {
	if (assertion.threshold === void 0) throw new Error("Cost assertion must have a threshold");
	if (typeof cost === "undefined") throw new Error("Cost assertion does not support providers that do not return cost");
	const pass = cost <= assertion.threshold;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Cost ${cost.toPrecision(2)} is greater than threshold ${assertion.threshold}`,
		assertion
	};
};

//#endregion
//#region src/assertions/equals.ts
const handleEquals = async ({ assertion, renderedValue, outputString, inverse }) => {
	let pass;
	if (typeof renderedValue === "object") {
		try {
			pass = util.isDeepStrictEqual(renderedValue, JSON.parse(outputString)) !== inverse;
		} catch {
			pass = false;
		}
		renderedValue = JSON.stringify(renderedValue);
	} else pass = String(renderedValue) === outputString !== inverse;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Expected output "${outputString}" to ${inverse ? "not " : ""}equal "${renderedValue}"`,
		assertion
	};
};

//#endregion
//#region src/assertions/factuality.ts
const handleFactuality = async ({ assertion, renderedValue, outputString, test, prompt, providerCallContext }) => {
	invariant(typeof renderedValue === "string", "factuality assertion type must have a string value");
	invariant(prompt, "factuality assertion type must have a prompt");
	return {
		assertion,
		...await matchesFactuality(prompt, renderedValue, outputString, test.options, test.vars, providerCallContext)
	};
};

//#endregion
//#region src/assertions/finishReason.ts
function handleFinishReason({ assertion, renderedValue, providerResponse }) {
	const value = renderedValue ?? assertion.value;
	invariant(typeof value === "string", "\"finish-reason\" assertion type must have a string value");
	if (!providerResponse.finishReason) return {
		pass: false,
		score: 0,
		reason: "Provider did not supply stop/finish reason",
		assertion
	};
	const pass = value.toLowerCase() === providerResponse.finishReason.toLowerCase();
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Expected finish reason "${value}" but got "${providerResponse.finishReason}"`,
		assertion
	};
}

//#endregion
//#region src/assertions/functionToolCall.ts
const handleIsValidFunctionCall = ({ assertion, output, provider, test }) => {
	try {
		if (provider instanceof AIStudioChatProvider || provider instanceof GoogleLiveProvider || provider instanceof VertexChatProvider) validateFunctionCall(output, provider.config?.tools, test.vars);
		else if (provider instanceof OpenAiChatCompletionProvider) validateFunctionCall$1(output, provider.config.functions, test.vars);
		else throw new Error(`Provider does not have functionality for checking function call.`);
		return {
			pass: true,
			score: 1,
			reason: "Assertion passed",
			assertion
		};
	} catch (err) {
		return {
			pass: false,
			score: 0,
			reason: err.message,
			assertion
		};
	}
};

//#endregion
//#region src/assertions/geval.ts
const handleGEval = async ({ assertion, renderedValue, prompt, outputString, test, providerCallContext }) => {
	invariant(typeof renderedValue === "string" || Array.isArray(renderedValue), "G-Eval assertion type must have a string or array of strings value");
	const threshold = assertion.threshold ?? .7;
	if (Array.isArray(renderedValue)) {
		const scores = [];
		const reasons = [];
		for (const value of renderedValue) {
			const resp = await matchesGEval(value, prompt || "", outputString, threshold, test.options, providerCallContext);
			scores.push(resp.score);
			reasons.push(resp.reason);
		}
		const scoresSum = scores.reduce((a, b) => a + b, 0);
		return {
			assertion,
			pass: scoresSum / scores.length >= threshold,
			score: scoresSum / scores.length,
			reason: reasons.join("\n\n")
		};
	} else return {
		assertion,
		...await matchesGEval(renderedValue, prompt || "", outputString, threshold, test.options, providerCallContext)
	};
};

//#endregion
//#region src/assertions/gleu.ts
/**
* Calculates the Google-BLEU (GLEU) score for a candidate string against reference strings.
*
* GLEU is a variant of BLEU that shows better correlation with human judgments on sentence-level
* evaluation. It calculates n-gram matches between the candidate and reference texts.
*
* For the GLEU score, we record all sub-sequences of 1, 2, 3 or 4 tokens in output and target sequence.
* We then compute:
* - Precision: the ratio of matching n-grams to total n-grams in the generated output sequence
* - Recall: the ratio of matching n-grams to total n-grams in the target (ground truth) sequence
*
* The GLEU score is the minimum of precision and recall.
*
* For multiple references, we calculate the GLEU score against each reference and return the maximum score.
* This reflects the intuition that if the candidate matches well with any of the valid references,
* it should be considered a good translation.
*
* Implementation details:
* - n-grams from n=1 to n=4 are considered by default
* - The calculation is case-insensitive
* - Identical strings will always receive a score of 1
* - If there are no n-grams in common, the score will be 0
*
* @param candidate - The string to evaluate
* @param references - Array of reference strings to compare against
* @param minN - Minimum n-gram length to consider (default: 1)
* @param maxN - Maximum n-gram length to consider (default: 4)
* @returns GLEU score between 0 and 1, where higher scores indicate better matches
* @throws When candidate or references are invalid
*/
function calculateGleuScore(candidate, references, minN = 1, maxN = 4) {
	if (!candidate || references.length === 0) throw new Error("Invalid inputs");
	const candidateWords = candidate.toLowerCase().trim().split(/\s+/).map((word) => word.replace(/\.+$/, ""));
	const referenceScores = references.map((reference) => {
		const referenceWords = reference.toLowerCase().trim().split(/\s+/).map((word) => word.replace(/\.+$/, ""));
		if (candidateWords.length === referenceWords.length && candidateWords.every((word, idx) => word === referenceWords[idx])) return 1;
		let matchCount = 0;
		let candidateTotal = 0;
		let referenceTotal = 0;
		for (let n = minN; n <= maxN; n++) {
			const candidateNGrams = getNGrams(candidateWords, n);
			const referenceNGrams = getNGrams(referenceWords, n);
			const candidateNGramCounts = /* @__PURE__ */ new Map();
			const referenceNGramCounts = /* @__PURE__ */ new Map();
			for (const gram of candidateNGrams) candidateNGramCounts.set(gram, (candidateNGramCounts.get(gram) || 0) + 1);
			for (const gram of referenceNGrams) referenceNGramCounts.set(gram, (referenceNGramCounts.get(gram) || 0) + 1);
			for (const [gram, candidateCount] of candidateNGramCounts.entries()) {
				const referenceCount = referenceNGramCounts.get(gram) || 0;
				matchCount += Math.min(candidateCount, referenceCount);
			}
			candidateTotal += candidateNGrams.length;
			referenceTotal += referenceNGrams.length;
		}
		if (candidateTotal === 0 || referenceTotal === 0) return 0;
		const precision = matchCount / candidateTotal;
		const recall = matchCount / referenceTotal;
		return Math.min(precision, recall);
	});
	return Math.max(...referenceScores);
}
/**
* Handles GLEU (Google-BLEU) score calculation and evaluation for assertions.
* GLEU is a variant of BLEU that correlates better with human judgments on sentence-level evaluation.
*
* Use cases for GLEU:
* - For sentence-level evaluation where BLEU might give unintuitive results
* - When you want to balance both precision and recall in your evaluation
* - When working with multiple valid references
* - When human correlation is particularly important
*
* @param {AssertionParams} params - The parameters for GLEU score evaluation
* @param {Object} params.assertion - The assertion configuration object
* @param {boolean} params.inverse - Whether to invert the pass condition
* @param {string} params.outputString - The candidate string to evaluate
* @param {string|string[]} params.renderedValue - The reference string(s) to compare against
* @param {string} params.provider - The provider name (unused)
* @param {Object} params.test - The test case data (unused)
* @returns {GradingResult} Object containing:
*   - pass: boolean indicating if assertion passed
*   - score: GLEU score (0-1)
*   - reason: explanation of the result
*   - assertion: original assertion config
* @throws {Error} If renderedValue is not a string or array of strings
*/
function handleGleuScore({ assertion, inverse, outputString, renderedValue }) {
	invariant(typeof renderedValue === "string" || Array.isArray(renderedValue) && renderedValue.every((v) => typeof v === "string"), "\"gleu\" assertion must have a string or array of strings value");
	const threshold = assertion.threshold ?? .5;
	const score = calculateGleuScore(outputString, Array.isArray(renderedValue) ? renderedValue : [renderedValue]);
	const pass = score >= threshold !== inverse;
	return {
		pass,
		score: inverse ? 1 - score : score,
		reason: pass ? "Assertion passed" : `GLEU score ${score.toFixed(4)} is ${inverse ? "greater" : "less"} than threshold ${threshold}`,
		assertion
	};
}

//#endregion
//#region src/assertions/guardrails.ts
const handleGuardrails = async ({ assertion, inverse, providerResponse }) => {
	const guardrails = providerResponse.guardrails || providerResponse.metadata?.redteamHistory?.[providerResponse.metadata.redteamHistory.length - 1]?.guardrails || {
		flagged: false,
		flaggedInput: false,
		flaggedOutput: false
	};
	logger_default.debug(`Guardrails from target response: ${JSON.stringify(guardrails, null, 2)}`);
	if (guardrails) if (guardrails.flagged) {
		let reason = "Content failed safety checks";
		if (guardrails?.flaggedInput) reason = "Prompt failed safety checks";
		else if (guardrails?.flaggedOutput) reason = "Output failed safety checks";
		reason = guardrails.reason || reason;
		const pass = inverse;
		return {
			pass,
			score: pass ? 1 : 0,
			reason: inverse ? `Guardrail correctly blocked: ${reason}` : reason,
			assertion
		};
	} else {
		const pass = !inverse;
		return {
			pass,
			score: pass ? 1 : 0,
			reason: inverse ? "Content was not blocked by guardrails (expected to be blocked)" : "Content passed safety checks",
			assertion
		};
	}
	return {
		pass: !inverse,
		score: 0,
		reason: inverse ? "Guardrail was not applied (expected content to be blocked)" : "Guardrail was not applied",
		assertion
	};
};

//#endregion
//#region src/assertions/html.ts
const HTML_PATTERNS = {
	openingTag: /<[a-zA-Z][a-zA-Z0-9-]*(?:\s[^>]*)?>/,
	closingTag: /<\/[a-zA-Z][a-zA-Z0-9-]*\s*>/,
	selfClosingTag: /<[a-zA-Z][a-zA-Z0-9-]*(?:\s[^>]*)?\/>/,
	htmlEntity: /&(?:[a-zA-Z]+|#[0-9]+|#x[0-9a-fA-F]+);/,
	doctype: /<!DOCTYPE\s+html/i,
	htmlComment: /<!--[^-]*(?:-[^-]+)*-->/,
	htmlAttribute: /\s[a-zA-Z-]+=\s*["'][^"']*["']/
};
const MAX_INPUT_SIZE = 10 * 1024 * 1024;
function containsHtml(text) {
	if (text.length > MAX_INPUT_SIZE) return false;
	let htmlIndicators = 0;
	const hasOpening = HTML_PATTERNS.openingTag.test(text);
	const hasClosing = HTML_PATTERNS.closingTag.test(text);
	if (hasOpening && hasClosing) htmlIndicators += 2;
	else if (hasOpening || hasClosing) htmlIndicators += 1;
	if (HTML_PATTERNS.selfClosingTag.test(text)) htmlIndicators += 1;
	if (HTML_PATTERNS.htmlEntity.test(text)) htmlIndicators += 1;
	if (HTML_PATTERNS.doctype.test(text)) htmlIndicators += 2;
	if (HTML_PATTERNS.htmlComment.test(text)) htmlIndicators += 1;
	if (HTML_PATTERNS.htmlAttribute.test(text)) htmlIndicators += 1;
	if (/<(html|head|body|div|span|p|a|img|h[1-6]|ul|ol|li|table|tr|td|th|form|input|button|script|style|link|meta|br|hr)\b/i.test(text)) htmlIndicators += 2;
	if (htmlIndicators >= 2) return true;
	return false;
}
const VALID_HTML_ELEMENTS = new Set([
	"html",
	"head",
	"title",
	"base",
	"link",
	"meta",
	"style",
	"body",
	"article",
	"section",
	"nav",
	"aside",
	"h1",
	"h2",
	"h3",
	"h4",
	"h5",
	"h6",
	"header",
	"footer",
	"address",
	"p",
	"hr",
	"pre",
	"blockquote",
	"ol",
	"ul",
	"li",
	"dl",
	"dt",
	"dd",
	"figure",
	"figcaption",
	"main",
	"div",
	"a",
	"em",
	"strong",
	"small",
	"s",
	"cite",
	"q",
	"dfn",
	"abbr",
	"ruby",
	"rt",
	"rp",
	"data",
	"time",
	"code",
	"var",
	"samp",
	"kbd",
	"sub",
	"sup",
	"i",
	"b",
	"u",
	"mark",
	"bdi",
	"bdo",
	"span",
	"br",
	"wbr",
	"img",
	"iframe",
	"embed",
	"object",
	"param",
	"picture",
	"source",
	"video",
	"audio",
	"track",
	"map",
	"area",
	"svg",
	"math",
	"script",
	"noscript",
	"canvas",
	"ins",
	"del",
	"table",
	"caption",
	"colgroup",
	"col",
	"tbody",
	"thead",
	"tfoot",
	"tr",
	"td",
	"th",
	"form",
	"label",
	"input",
	"button",
	"select",
	"datalist",
	"optgroup",
	"option",
	"textarea",
	"output",
	"progress",
	"meter",
	"fieldset",
	"legend",
	"details",
	"summary",
	"dialog",
	"menu",
	"slot",
	"template"
]);
function validateHtml(htmlString) {
	const trimmed = htmlString.trim();
	if (!trimmed) return {
		isValid: false,
		reason: "Output is empty"
	};
	if (trimmed.length > MAX_INPUT_SIZE) return {
		isValid: false,
		reason: "Output exceeds maximum size limit"
	};
	if (/^\s*<\?xml/i.test(trimmed)) return {
		isValid: false,
		reason: "Output appears to be XML, not HTML"
	};
	try {
		const { document } = new JSDOM(trimmed, { contentType: "text/html" }).window;
		if (document.body && !trimmed.toLowerCase().includes("<body")) {
			if (Array.from(document.body.childNodes).some((node) => node.nodeType === 3 && node.textContent?.trim())) return {
				isValid: false,
				reason: "Output must be wrapped in HTML tags"
			};
		}
		const allElements = document.querySelectorAll("*");
		if (!Array.from(allElements).find((element) => {
			const tagName = element.tagName.toLowerCase();
			if ([
				"html",
				"head",
				"body"
			].includes(tagName) && !trimmed.toLowerCase().includes(`<${tagName}`)) return false;
			return VALID_HTML_ELEMENTS.has(tagName) || tagName.includes("-");
		})) return {
			isValid: false,
			reason: "Output does not contain recognized HTML elements"
		};
		return {
			isValid: true,
			reason: "Output is valid HTML"
		};
	} catch (error) {
		return {
			isValid: false,
			reason: `HTML parsing failed: ${error instanceof Error ? error.message : "Unknown error"}`
		};
	}
}
const handleContainsHtml = ({ assertion, outputString, inverse }) => {
	const pass = containsHtml(outputString) !== inverse;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Expected output to ${inverse ? "not " : ""}contain HTML content`,
		assertion
	};
};
const handleIsHtml = ({ assertion, outputString, inverse }) => {
	const result = validateHtml(outputString);
	const pass = result.isValid !== inverse;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : result.reason,
		assertion
	};
};

//#endregion
//#region src/assertions/javascript.ts
/**
* Checks if a character at the given index is escaped by backslashes.
* Handles multiple consecutive backslashes correctly (e.g., \\\\ is two escaped backslashes).
*/
function isCharEscaped(code, index) {
	let backslashCount = 0;
	let i = index - 1;
	while (i >= 0 && code[i] === "\\") {
		backslashCount++;
		i--;
	}
	return backslashCount % 2 === 1;
}
/**
* Finds the last semicolon that acts as a statement separator (not inside a string literal).
* Tracks quote state to skip semicolons inside single quotes, double quotes, and template literals.
*
* @returns The index of the last statement-level semicolon, or -1 if none found.
*
* @remarks
* Known limitations (use multiline format for these cases):
* - Does not handle semicolons inside regex literals (e.g., /;/)
* - Does not handle semicolons inside template literal expressions (e.g., `${a;b}`)
*/
function findLastStatementSemicolon(code) {
	let inSingleQuote = false;
	let inDoubleQuote = false;
	let inTemplate = false;
	let lastSemiIndex = -1;
	for (let i = 0; i < code.length; i++) {
		const char = code[i];
		if (!isCharEscaped(code, i)) {
			if (char === "'" && !inDoubleQuote && !inTemplate) inSingleQuote = !inSingleQuote;
			else if (char === "\"" && !inSingleQuote && !inTemplate) inDoubleQuote = !inDoubleQuote;
			else if (char === "`" && !inSingleQuote && !inDoubleQuote) inTemplate = !inTemplate;
		}
		if (char === ";" && !inSingleQuote && !inDoubleQuote && !inTemplate) lastSemiIndex = i;
	}
	return lastSemiIndex;
}
/**
* Builds a function body from a single-line JavaScript assertion.
*
* Handles the case where assertions start with variable declarations (const/let/var).
* For these, we inject `return` before the final expression instead of prepending it,
* which would create invalid syntax like `return const x = 1`.
*
* @example
* // Simple expression - prepend return
* "output === 'test'" â†’ "return output === 'test'"
*
* @example
* // Declaration with final expression - inject return before expression
* "const s = JSON.parse(output).score; s > 0.5" â†’ "const s = JSON.parse(output).score; return s > 0.5"
*
* @example
* // Semicolons in strings are handled correctly
* "const s = output; s === 'a;b'" â†’ "const s = output; return s === 'a;b'"
*/
function buildFunctionBody(code) {
	const trimmed = code.trim().replace(/;+\s*$/, "");
	if (/^(const|let|var)\s/.test(trimmed)) {
		const lastSemiIndex = findLastStatementSemicolon(trimmed);
		if (lastSemiIndex !== -1) {
			const statements = trimmed.slice(0, lastSemiIndex + 1);
			const expression = trimmed.slice(lastSemiIndex + 1).trim();
			if (expression) return `${statements} return ${expression}`;
		}
		return trimmed;
	}
	return `return ${trimmed}`;
}
const validateResult = async (result) => {
	result = await Promise.resolve(result);
	if (typeof result === "boolean" || typeof result === "number" || isGradingResult(result)) return result;
	else throw new Error(`Custom function must return a boolean, number, or GradingResult object. Got type ${typeof result}: ${JSON.stringify(result)}`);
};
const handleJavascript = async ({ assertion, renderedValue, valueFromScript, assertionValueContext, outputString, output, inverse }) => {
	let pass;
	let score;
	try {
		if (typeof assertion.value === "function") {
			let ret = assertion.value(outputString, assertionValueContext);
			ret = await validateResult(ret);
			if (!ret.assertion) {
				const functionString = assertion.value.toString();
				ret.assertion = {
					type: "javascript",
					value: functionString.length > 50 ? functionString.slice(0, 50) + "..." : functionString
				};
			}
			return ret;
		}
		invariant(typeof renderedValue === "string", "javascript assertion must have a string value");
		/**
		* Removes trailing newline from the rendered value.
		* This is necessary for handling multi-line string literals in YAML
		* that are defined on a single line in the YAML file.
		*
		* @example
		* value: |
		*   output === 'true'
		*/
		renderedValue = renderedValue.trimEnd();
		let result;
		if (typeof valueFromScript === "undefined") {
			const functionBody = renderedValue.includes("\n") ? renderedValue : buildFunctionBody(renderedValue);
			result = await validateResult(new Function("output", "context", "process", functionBody)(output, assertionValueContext, getProcessShim()));
		} else {
			invariant(typeof valueFromScript === "boolean" || typeof valueFromScript === "number" || typeof valueFromScript === "object", `Javascript assertion script must return a boolean, number, or object (${assertion.value})`);
			result = await validateResult(valueFromScript);
		}
		if (typeof result === "boolean") {
			pass = result !== inverse;
			score = pass ? 1 : 0;
		} else if (typeof result === "number") {
			pass = assertion.threshold !== void 0 ? result >= assertion.threshold : result > 0;
			score = result;
		} else if (typeof result === "object") return result;
		else throw new Error("Custom function must return a boolean or number");
	} catch (err) {
		return {
			pass: false,
			score: 0,
			reason: `Custom function threw error: ${err.message}
Stack Trace: ${err.stack}
${renderedValue}`,
			assertion
		};
	}
	return {
		pass,
		score,
		reason: pass ? "Assertion passed" : `Custom function returned ${inverse ? "true" : "false"}
${renderedValue}`,
		assertion
	};
};

//#endregion
//#region src/assertions/json.ts
function handleIsJson({ outputString, renderedValue, inverse, valueFromScript, assertion }) {
	let parsedJson;
	let pass;
	try {
		parsedJson = JSON.parse(outputString);
		pass = !inverse;
	} catch {
		pass = inverse;
	}
	if (pass && renderedValue) {
		let validate;
		if (typeof renderedValue === "string") if (renderedValue.startsWith("file://")) {
			const schema = valueFromScript;
			invariant(schema, "is-json references a file that does not export a JSON schema");
			validate = getAjv().compile(schema);
		} else {
			const scheme = yaml.load(renderedValue);
			validate = getAjv().compile(scheme);
		}
		else if (typeof renderedValue === "object") validate = getAjv().compile(renderedValue);
		else throw new Error("is-json assertion must have a string or object value");
		pass = validate(parsedJson);
		if (!pass) return {
			pass,
			score: 0,
			reason: `JSON does not conform to the provided schema. Errors: ${getAjv().errorsText(validate.errors)}`,
			assertion
		};
	}
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : "Expected output to be valid JSON",
		assertion
	};
}
function handleContainsJson({ assertion, renderedValue, outputString, inverse, valueFromScript }) {
	let errorMessage = "Expected output to contain valid JSON";
	const jsonObjects = extractJsonObjects(outputString);
	let pass = inverse ? jsonObjects.length === 0 : jsonObjects.length > 0;
	for (const jsonObject of jsonObjects) if (renderedValue) {
		let validate;
		if (typeof renderedValue === "string") if (renderedValue.startsWith("file://")) {
			const schema = valueFromScript;
			invariant(schema, "contains-json references a file that does not export a JSON schema");
			validate = getAjv().compile(schema);
		} else {
			const scheme = yaml.load(renderedValue);
			validate = getAjv().compile(scheme);
		}
		else if (typeof renderedValue === "object") validate = getAjv().compile(renderedValue);
		else throw new Error("contains-json assertion must have a string or object value");
		pass = validate(jsonObject);
		if (pass) break;
		else errorMessage = `JSON does not conform to the provided schema. Errors: ${getAjv().errorsText(validate.errors)}`;
	}
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : errorMessage,
		assertion
	};
}

//#endregion
//#region src/assertions/latency.ts
const handleLatency = ({ assertion, latencyMs }) => {
	if (assertion.threshold === void 0) throw new Error("Latency assertion must have a threshold in milliseconds");
	if (latencyMs === void 0) throw new Error("Latency assertion does not support cached results. Rerun the eval with --no-cache");
	const pass = latencyMs <= assertion.threshold;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Latency ${latencyMs}ms is greater than threshold ${assertion.threshold}ms`,
		assertion
	};
};

//#endregion
//#region src/assertions/levenshtein.ts
function handleLevenshtein({ assertion, renderedValue, outputString }) {
	invariant(typeof renderedValue === "string", "\"levenshtein\" assertion type must have a string value");
	const levDistance = distance(outputString, renderedValue);
	const threshold = assertion.threshold ?? 5;
	const pass = levDistance <= threshold;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Levenshtein distance ${levDistance} is greater than threshold ${threshold}`,
		assertion
	};
}

//#endregion
//#region src/assertions/llmRubric.ts
const handleLlmRubric = ({ assertion, renderedValue, outputString, test, providerCallContext }) => {
	invariant(typeof renderedValue === "string" || typeof renderedValue === "object" || typeof renderedValue === "undefined", "\"llm-rubric\" assertion type must have a string or object value");
	if (test.options?.rubricPrompt && typeof test.options.rubricPrompt === "object") test.options.rubricPrompt = JSON.stringify(test.options.rubricPrompt);
	assertion.value = assertion.value || test.options?.rubricPrompt;
	return matchesLlmRubric(renderedValue || "", outputString, test.options, test.vars, assertion, void 0, providerCallContext);
};

//#endregion
//#region src/assertions/modelGradedClosedQa.ts
const handleModelGradedClosedQa = async ({ assertion, renderedValue, outputString, test, prompt, providerCallContext }) => {
	invariant(typeof renderedValue === "string", "model-graded-closedqa assertion type must have a string value");
	invariant(prompt, "model-graded-closedqa assertion type must have a prompt");
	return {
		assertion,
		...await matchesClosedQa(prompt, renderedValue, outputString, test.options, test.vars, providerCallContext)
	};
};

//#endregion
//#region src/assertions/moderation.ts
const handleModeration = async ({ assertion, test, outputString, providerResponse, prompt }) => {
	const promptToModerate = getActualPromptWithFallback(providerResponse, prompt || "");
	invariant(promptToModerate, "moderation assertion type must have a prompt");
	invariant(!assertion.value || Array.isArray(assertion.value) && typeof assertion.value[0] === "string", "moderation assertion value must be a string array if set");
	if (promptToModerate[0] === "[" || promptToModerate[0] === "{") try {
		const parsedPrompt = parseChatPrompt(promptToModerate, null);
		if (parsedPrompt && parsedPrompt.length > 0) prompt = parsedPrompt[parsedPrompt.length - 1].content;
	} catch {}
	const moderationResult = await matchesModeration({
		userPrompt: promptToModerate,
		assistantResponse: outputString,
		categories: Array.isArray(assertion.value) ? assertion.value : []
	}, test.options);
	return {
		pass: moderationResult.pass,
		score: moderationResult.score,
		reason: moderationResult.reason,
		assertion
	};
};

//#endregion
//#region src/assertions/openai.ts
const handleIsValidOpenAiToolsCall = async ({ assertion, output, provider, test }) => {
	const outputStr = typeof output === "string" ? output : JSON.stringify(output);
	if (outputStr.includes("MCP Tool Result") || outputStr.includes("MCP Tool Error")) {
		if (outputStr.includes("MCP Tool Error")) {
			const errorMatch = outputStr.match(/MCP Tool Error \(([^)]+)\): (.+)/);
			return {
				pass: false,
				score: 0,
				reason: `MCP tool call failed for ${errorMatch ? errorMatch[1] : "unknown"}: ${errorMatch ? errorMatch[2] : "unknown error"}`,
				assertion
			};
		}
		const resultMatch = outputStr.match(/MCP Tool Result \(([^)]+)\):/);
		return {
			pass: true,
			score: 1,
			reason: `MCP tool call succeeded for ${resultMatch ? resultMatch[1] : "unknown"}`,
			assertion
		};
	}
	if (typeof output === "object" && "tool_calls" in output) output = output.tool_calls;
	const toolsOutput = output;
	if (!Array.isArray(toolsOutput) || toolsOutput.length === 0 || typeof toolsOutput[0].function.name !== "string" || typeof toolsOutput[0].function.arguments !== "string") return {
		pass: false,
		score: 0,
		reason: `OpenAI did not return a valid-looking tools response: ${JSON.stringify(toolsOutput)}`,
		assertion
	};
	let tools = provider.config.tools;
	if (tools) {
		const loadedTools = await maybeLoadToolsFromExternalFile(tools, test.vars);
		if (loadedTools !== void 0) tools = loadedTools;
	}
	if (!tools) return {
		pass: false,
		score: 0,
		reason: "No tools configured in provider, but output contains tool calls",
		assertion
	};
	try {
		toolsOutput.forEach((toolOutput) => {
			validateFunctionCall$1(toolOutput.function, tools.filter((tool) => tool.type === "function" && "function" in tool).map((tool) => tool.function), test.vars);
		});
		return {
			pass: true,
			score: 1,
			reason: "Assertion passed",
			assertion
		};
	} catch (err) {
		return {
			pass: false,
			score: 0,
			reason: err.message,
			assertion
		};
	}
};

//#endregion
//#region src/assertions/perplexity.ts
function handlePerplexity({ logProbs, assertion }) {
	if (!logProbs || logProbs.length === 0) throw new Error("Perplexity assertion does not support providers that do not return logProbs");
	const avgLogProb = logProbs.reduce((acc, logProb) => acc + logProb, 0) / logProbs.length;
	const perplexity = Math.exp(-avgLogProb);
	const pass = assertion.threshold !== void 0 ? perplexity <= assertion.threshold : true;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Perplexity ${perplexity.toFixed(2)} is greater than threshold ${assertion.threshold}`,
		assertion
	};
}
function handlePerplexityScore({ logProbs, assertion }) {
	if (!logProbs || logProbs.length === 0) throw new Error("perplexity-score assertion does not support providers that do not return logProbs");
	const avgLogProb = logProbs.reduce((acc, logProb) => acc + logProb, 0) / logProbs.length;
	const perplexityNorm = 1 / (1 + Math.exp(-avgLogProb));
	const pass = assertion.threshold !== void 0 ? perplexityNorm >= assertion.threshold : true;
	return {
		pass,
		score: perplexityNorm,
		reason: pass ? "Assertion passed" : `Perplexity score ${perplexityNorm.toFixed(2)} is less than threshold ${assertion.threshold}`,
		assertion
	};
}

//#endregion
//#region src/assertions/pi.ts
const handlePiScorer = async ({ assertion, prompt, renderedValue, outputString }) => {
	invariant(typeof renderedValue === "string", "\"pi\" assertion type must have a string value");
	invariant(typeof prompt === "string", "\"pi\" assertion must have a prompt that is a string");
	return matchesPiScore(renderedValue, prompt, outputString, assertion);
};

//#endregion
//#region src/python/wrapper.ts
/**
* Executes Python code by writing it to a temporary file
* @param {string} code - The Python code to execute.
* @param {string} method - The method name to call in the Python script.
* @param {(string | object | undefined)[]} args - The list of arguments to pass to the Python method.
* @returns {Promise<T>} - The result from executing the Python code.
*/
async function runPythonCode(code, method, args) {
	const tempFilePath = path.join(os.tmpdir(), `temp-python-code-${Date.now()}-${Math.random().toString(16).slice(2)}.py`);
	try {
		fs.writeFileSync(tempFilePath, code);
		return await runPython(tempFilePath, method, args);
	} catch (error) {
		logger_default.error(`Error executing Python code: ${error}`);
		throw error;
	} finally {
		try {
			fs.unlinkSync(tempFilePath);
		} catch (error) {
			logger_default.error(`Error removing temporary file: ${error}`);
		}
	}
}

//#endregion
//#region src/util/caseMapping.ts
/**
* Recursively map snake_case keys to camelCase for Python/Ruby compatibility.
* Python and Ruby conventionally use snake_case for identifiers, while JavaScript
* uses camelCase. This function handles results from assertion scripts that may
* return snake_case keys (e.g., 'named_scores' instead of 'namedScores').
*
* @param obj - Object with potentially snake_case keys
* @returns Object with camelCase keys
*/
function mapSnakeCaseToCamelCase(obj) {
	const result = { ...obj };
	if ("pass_" in result && !("pass" in result)) result.pass = result.pass_;
	if ("named_scores" in result && !("namedScores" in result)) result.namedScores = result.named_scores;
	if ("component_results" in result && !("componentResults" in result)) result.componentResults = result.component_results;
	if ("tokens_used" in result && !("tokensUsed" in result)) result.tokensUsed = result.tokens_used;
	if (result.componentResults && Array.isArray(result.componentResults)) result.componentResults = result.componentResults.map((component) => {
		if (typeof component === "object" && component !== null) return mapSnakeCaseToCamelCase(component);
		return component;
	});
	return result;
}

//#endregion
//#region src/assertions/python.ts
const handlePython = async ({ assertion, renderedValue, valueFromScript, assertionValueContext, output }) => {
	invariant(typeof renderedValue === "string", "python assertion must have a string value");
	let pass;
	let score;
	try {
		let result;
		if (typeof valueFromScript === "undefined") {
			const isMultiline = renderedValue.includes("\n");
			let indentStyle = "    ";
			if (isMultiline) {
				const match = renderedValue.match(/^(?!\s*$)\s+/m);
				if (match) indentStyle = match[0];
			}
			result = await runPythonCode(`import json

def main(output, context):
${isMultiline ? renderedValue.split("\n").map((line) => `${indentStyle}${line}`).join("\n") : `    return ${renderedValue}`}
`, "main", [output, assertionValueContext]);
		} else result = valueFromScript;
		if (typeof result === "boolean" && result || typeof result === "string" && result.toLowerCase() === "true") {
			pass = true;
			score = 1;
		} else if (typeof result === "boolean" && !result || typeof result === "string" && result.toLowerCase() === "false") {
			pass = false;
			score = 0;
		} else if (typeof result === "string" && result.startsWith("{")) {
			let parsed;
			try {
				parsed = JSON.parse(result);
			} catch (err) {
				throw new Error(`Invalid JSON: ${err} when parsing result: ${result}`);
			}
			if (!isGradingResult(parsed)) throw new Error(`Python assertion must return a boolean, number, or {pass, score, reason} object. Got instead: ${result}`);
			return parsed;
		} else if (typeof result === "object") {
			const mappedObj = mapSnakeCaseToCamelCase(result);
			if (!isGradingResult(mappedObj)) throw new Error(`Python assertion must return a boolean, number, or {pass, score, reason} object. Got instead:\n${JSON.stringify(mappedObj, null, 2)}`);
			const pythonGradingResult = mappedObj;
			if (assertion.threshold !== void 0 && pythonGradingResult.score < assertion.threshold) {
				pythonGradingResult.pass = false;
				const scoreMessage = `Python score ${pythonGradingResult.score} is less than threshold ${assertion.threshold}`;
				pythonGradingResult.reason = pythonGradingResult.reason ? `${scoreMessage}: ${pythonGradingResult.reason}` : scoreMessage;
			}
			return {
				...pythonGradingResult,
				assertion
			};
		} else {
			score = Number.parseFloat(String(result));
			if (Number.isNaN(score)) throw new Error(`Python assertion must return a boolean, number, or {pass, score, reason} object. Instead got:\n${result}`);
			pass = assertion.threshold !== void 0 ? score >= assertion.threshold : score > 0;
		}
	} catch (err) {
		return {
			pass: false,
			score: 0,
			reason: `Python code execution failed: ${err.message}`,
			assertion
		};
	}
	return {
		pass,
		score,
		reason: pass ? "Assertion passed" : `Python code returned ${pass ? "true" : "false"}\n${assertion.value}`,
		assertion
	};
};

//#endregion
//#region src/assertions/redteam.ts
/**
* Analyzes grader errors in the redteam history.
* Returns whether some (but not all) turns have grader errors.
* If ALL turns have errors, we should still ERROR. If only SOME have errors, we can be more lenient.
*/
function analyzeGraderErrors(redteamHistory) {
	if (!redteamHistory || !Array.isArray(redteamHistory) || redteamHistory.length === 0) return {
		hasAnyErrors: false,
		allTurnsHaveErrors: false
	};
	const turnsWithErrors = redteamHistory.filter((turn) => turn.graderError && turn.graderError.length > 0);
	return {
		hasAnyErrors: turnsWithErrors.length > 0,
		allTurnsHaveErrors: turnsWithErrors.length === redteamHistory.length
	};
}
/**
* As the name implies, this function "handles" redteam assertions by either calling the
* grader or preferably returning a `storedGraderResult` if it exists on the provider response.
*/
const handleRedteam = async ({ assertion, baseType, test, prompt, outputString, provider, renderedValue, providerResponse }) => {
	if (providerResponse.metadata?.storedGraderResult && test.metadata?.pluginId && assertion.type.includes(test.metadata.pluginId)) {
		const storedResult = providerResponse.metadata.storedGraderResult;
		const redteamHistory = providerResponse.metadata?.redteamHistory;
		const { hasAnyErrors } = analyzeGraderErrors(redteamHistory);
		return {
			...storedResult,
			assertion: {
				...storedResult.assertion ?? assertion,
				value: storedResult.assertion?.value || assertion.value
			},
			metadata: {
				...test.metadata,
				...storedResult.metadata,
				...hasAnyErrors ? { gradingIncomplete: true } : {}
			}
		};
	}
	const grader = getGraderById(assertion.type);
	invariant(grader, `Unknown grader: ${baseType}`);
	invariant(prompt, `Grader ${baseType} must have a prompt`);
	try {
		const { grade, rubric, suggestions } = await grader.getResult(prompt, outputString, test, provider, renderedValue);
		return {
			...grade,
			...grade.assertion || assertion ? { assertion: {
				...grade.assertion ?? assertion,
				value: rubric
			} } : {},
			suggestions,
			metadata: {
				...test.metadata,
				...grade.metadata
			}
		};
	} catch (error) {
		const redteamHistory = providerResponse.metadata?.redteamHistory;
		const { hasAnyErrors, allTurnsHaveErrors } = analyzeGraderErrors(redteamHistory);
		if (test.metadata?.strategyId && hasAnyErrors && !allTurnsHaveErrors) {
			const errorMessage = error instanceof Error ? error.message : String(error);
			logger_default.warn("[Redteam] Grading failed for iterative test with some prior grader errors", {
				error: errorMessage,
				strategyId: test.metadata.strategyId,
				pluginId: test.metadata.pluginId
			});
			return {
				pass: true,
				score: 0,
				reason: `Some grading calls failed during iterative testing. Check the Messages tab for details.`,
				assertion,
				metadata: {
					...test.metadata,
					gradingIncomplete: true,
					gradingError: errorMessage
				}
			};
		}
		throw error;
	}
};

//#endregion
//#region src/assertions/refusal.ts
function handleIsRefusal(params) {
	const { output, inverse, assertion } = params;
	if (typeof output !== "string") return {
		pass: true,
		score: 0,
		reason: "No value provided, assumed to be a refusal",
		assertion
	};
	if (output.trim() === "") {
		const pass = !inverse;
		return {
			pass,
			score: 0,
			reason: pass ? "Empty response treated as refusal" : "Expected output to not be a refusal",
			assertion
		};
	}
	const pass = isBasicRefusal(output) !== inverse;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Expected output to ${inverse ? "not " : ""}be a refusal`,
		assertion
	};
}

//#endregion
//#region src/assertions/regex.ts
const handleRegex = ({ assertion, renderedValue, outputString, inverse }) => {
	invariant(renderedValue, "\"regex\" assertion type must have a string value");
	invariant(typeof renderedValue === "string", "\"regex\" assertion type must have a string value");
	let regex;
	try {
		regex = new RegExp(renderedValue);
	} catch (error) {
		return {
			pass: false,
			score: 0,
			reason: `Invalid regex pattern: ${error instanceof Error ? error.message : "unknown error"}`,
			assertion
		};
	}
	const pass = regex.test(outputString) !== inverse;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Expected output to ${inverse ? "not " : ""}match regex "${renderedValue}"`,
		assertion
	};
};

//#endregion
//#region src/assertions/rouge.ts
function handleRougeScore({ baseType, assertion, renderedValue, outputString, inverse }) {
	invariant(typeof renderedValue === "string", "\"rouge\" assertion type must be a string value");
	const rougeMethod = rouge[baseType[baseType.length - 1]];
	const score = rougeMethod(outputString, renderedValue, {});
	const threshold = assertion.threshold ?? .75;
	const pass = score >= threshold != inverse;
	return {
		pass,
		score: inverse ? 1 - score : score,
		reason: pass ? `${baseType.toUpperCase()} score ${score.toFixed(2)} is greater than or equal to threshold ${threshold}` : `${baseType.toUpperCase()} score ${score.toFixed(2)} is less than threshold ${threshold}`,
		assertion
	};
}

//#endregion
//#region src/ruby/wrapper.ts
/**
* Executes Ruby code by writing it to a temporary file
* @param {string} code - The Ruby code to execute.
* @param {string} method - The method name to call in the Ruby script.
* @param {(string | object | undefined)[]} args - The list of arguments to pass to the Ruby method.
* @returns {Promise<string>} - The result from executing the Ruby code.
*/
async function runRubyCode(code, method, args) {
	const tempFilePath = path.join(os.tmpdir(), `temp-ruby-code-${Date.now()}-${Math.random().toString(16).slice(2)}.rb`);
	try {
		fs.writeFileSync(tempFilePath, code);
		return await runRuby(tempFilePath, method, args);
	} catch (error) {
		logger_default.error(`Error executing Ruby code: ${error}`);
		throw error;
	} finally {
		try {
			fs.unlinkSync(tempFilePath);
		} catch (error) {
			logger_default.error(`Error removing temporary file: ${error}`);
		}
	}
}

//#endregion
//#region src/assertions/ruby.ts
const handleRuby = async ({ assertion, renderedValue, valueFromScript, assertionValueContext, output }) => {
	invariant(typeof renderedValue === "string", "ruby assertion must have a string value");
	let pass;
	let score;
	try {
		let result;
		if (typeof valueFromScript === "undefined") {
			const isMultiline = renderedValue.includes("\n");
			let indentStyle = "  ";
			if (isMultiline) {
				const match = renderedValue.match(/^(?!\s*$)\s+/m);
				if (match) indentStyle = match[0];
			}
			result = await runRubyCode(`require 'json'

def main(output, context)
${isMultiline ? renderedValue.split("\n").map((line) => `${indentStyle}${line}`).join("\n") : `  return ${renderedValue}`}
end
`, "main", [output, assertionValueContext]);
		} else result = valueFromScript;
		if (typeof result === "boolean" && result || typeof result === "string" && result.toLowerCase() === "true") {
			pass = true;
			score = 1;
		} else if (typeof result === "boolean" && !result || typeof result === "string" && result.toLowerCase() === "false") {
			pass = false;
			score = 0;
		} else if (typeof result === "string" && result.startsWith("{")) {
			let parsed;
			try {
				parsed = JSON.parse(result);
			} catch (err) {
				throw new Error(`Invalid JSON: ${err} when parsing result: ${result}`);
			}
			if (!isGradingResult(parsed)) throw new Error(`Ruby assertion must return a boolean, number, or {pass, score, reason} object. Got instead: ${result}`);
			return parsed;
		} else if (typeof result === "object") {
			const mappedObj = mapSnakeCaseToCamelCase(result);
			if (!isGradingResult(mappedObj)) throw new Error(`Ruby assertion must return a boolean, number, or {pass, score, reason} object. Got instead:\n${JSON.stringify(mappedObj, null, 2)}`);
			const rubyGradingResult = mappedObj;
			if (assertion.threshold !== void 0 && rubyGradingResult.score < assertion.threshold) {
				rubyGradingResult.pass = false;
				const scoreMessage = `Ruby score ${rubyGradingResult.score} is less than threshold ${assertion.threshold}`;
				rubyGradingResult.reason = rubyGradingResult.reason ? `${scoreMessage}: ${rubyGradingResult.reason}` : scoreMessage;
			}
			return {
				...rubyGradingResult,
				assertion
			};
		} else {
			score = Number.parseFloat(String(result));
			if (Number.isNaN(score)) throw new Error(`Ruby assertion must return a boolean, number, or {pass, score, reason} object. Instead got:\n${result}`);
			pass = assertion.threshold !== void 0 ? score >= assertion.threshold : score > 0;
		}
	} catch (err) {
		return {
			pass: false,
			score: 0,
			reason: `Ruby code execution failed: ${err.message}`,
			assertion
		};
	}
	return {
		pass,
		score,
		reason: pass ? "Assertion passed" : `Ruby code returned ${pass ? "true" : "false"}\n${assertion.value}`,
		assertion
	};
};

//#endregion
//#region src/assertions/searchRubric.ts
async function handleSearchRubric({ assertion, baseType: _baseType, inverse, provider, providerCallContext, renderedValue, test, providerResponse }) {
	if (renderedValue == null) throw new Error("search-rubric assertion type must have a string value");
	const result = await matchesSearchRubric(String(renderedValue), providerResponse.output, test.options, test.vars, assertion, provider, providerCallContext);
	if (inverse) {
		result.pass = !result.pass;
		result.reason = result.pass ? `Output does not require web search verification: ${result.reason}` : `Output requires web search verification: ${result.reason}`;
	}
	return result;
}

//#endregion
//#region src/assertions/similar.ts
const handleSimilar = async ({ assertion, renderedValue, outputString, inverse, test }) => {
	invariant(typeof renderedValue === "string" || Array.isArray(renderedValue), "Similarity assertion type must have a string or array of strings value");
	const threshold = assertion.threshold ?? .75;
	let metric = "cosine";
	if (assertion.type.includes(":")) {
		const metricSuffix = assertion.type.split(":")[1];
		switch (metricSuffix) {
			case "cosine":
				metric = "cosine";
				break;
			case "dot":
				metric = "dot_product";
				break;
			case "euclidean":
				metric = "euclidean";
				break;
			default: throw new Error(`Unknown similarity metric: ${metricSuffix}`);
		}
	}
	if (Array.isArray(renderedValue)) {
		let minScore = Number.POSITIVE_INFINITY;
		for (const value of renderedValue) {
			const result = await matchesSimilarity(value, outputString, threshold, inverse, test.options, metric);
			if (result.pass) return {
				assertion,
				...result
			};
			if (result.score < minScore) minScore = result.score;
		}
		return {
			assertion,
			pass: false,
			score: minScore,
			reason: `None of the provided values met the similarity threshold`
		};
	} else return {
		assertion,
		...await matchesSimilarity(renderedValue, outputString, threshold, inverse, test.options, metric)
	};
};

//#endregion
//#region src/assertions/sql.ts
const handleIsSql = async ({ assertion, renderedValue, outputString, inverse }) => {
	let pass = false;
	let databaseType = "MySQL";
	let whiteTableList;
	let whiteColumnList;
	if (renderedValue && typeof renderedValue === "object") {
		const value = renderedValue;
		databaseType = value.databaseType || "MySQL";
		whiteTableList = value.allowedTables;
		whiteColumnList = value.allowedColumns;
	}
	if (renderedValue && typeof renderedValue !== "object") throw new Error("is-sql assertion must have a object value.");
	const { Parser: SqlParser } = await import("node-sql-parser").catch(() => {
		throw new Error("node-sql-parser is not installed. Please install it first");
	});
	const sqlParser = new SqlParser();
	const opt = { database: databaseType };
	const failureReasons = [];
	const normalizedSql = outputString.trim();
	if (/`/.test(normalizedSql) && (normalizedSql.match(/`/g)?.length ?? 0) % 2 !== 0) failureReasons.push(`SQL statement does not conform to the provided ${databaseType} database syntax.`);
	if (/select\s+[A-Za-z_][A-Za-z0-9_]*\s+[A-Za-z_][A-Za-z0-9_]*\s+from/i.test(normalizedSql)) failureReasons.push(`SQL statement does not conform to the provided ${databaseType} database syntax.`);
	if (databaseType === "MySQL" && /\bgenerate_series\s*\(/i.test(normalizedSql)) failureReasons.push(`SQL statement does not conform to the provided ${databaseType} database syntax.`);
	try {
		sqlParser.astify(outputString, opt);
		pass = !inverse;
	} catch {
		pass = inverse;
		failureReasons.push(`SQL statement does not conform to the provided ${databaseType} database syntax.`);
	}
	if (failureReasons.length > 0) pass = inverse;
	if (whiteTableList) {
		opt.type = "table";
		try {
			sqlParser.whiteListCheck(outputString, whiteTableList, opt);
		} catch (err) {
			pass = inverse;
			const error = err;
			let actualTables = [];
			try {
				const { tableList } = sqlParser.parse(outputString, opt);
				actualTables = tableList || [];
			} catch {}
			if (actualTables.length > 0) failureReasons.push(`SQL references unauthorized table(s). Found: [${actualTables.join(", ")}]. Allowed: [${whiteTableList.join(", ")}].`);
			else failureReasons.push(`SQL validation failed: ${error.message}.`);
		}
	}
	if (whiteColumnList) {
		opt.type = "column";
		const normalizedWhiteList = [...whiteColumnList];
		for (const item of whiteColumnList) {
			const parts = item.split("::");
			if (parts.length === 3 && parts[1] !== "null") {
				const alt = `${parts[0]}::null::${parts[2]}`;
				if (!normalizedWhiteList.includes(alt)) normalizedWhiteList.push(alt);
			}
		}
		try {
			sqlParser.whiteListCheck(outputString, normalizedWhiteList, opt);
		} catch (err) {
			pass = inverse;
			const error = err;
			let actualColumns = [];
			try {
				const { columnList } = sqlParser.parse(outputString, opt);
				actualColumns = columnList || [];
			} catch {}
			if (actualColumns.length > 0) failureReasons.push(`SQL references unauthorized column(s). Found: [${actualColumns.join(", ")}]. Allowed: [${whiteColumnList.join(", ")}].`);
			else failureReasons.push(`SQL validation failed: ${error.message}.`);
		}
	}
	if (inverse && pass === false && failureReasons.length === 0) failureReasons.push("The output SQL statement is valid");
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : failureReasons.join(" "),
		assertion
	};
};
const handleContainsSql = async (assertionParams) => {
	const match = coerceString(assertionParams.outputString).match(/```(?:sql)?([^`]+)```/);
	if (match) {
		const sqlCode = match[1].trim();
		return handleIsSql({
			...assertionParams,
			outputString: sqlCode
		});
	}
	return handleIsSql(assertionParams);
};

//#endregion
//#region src/assertions/startsWith.ts
const handleStartsWith = ({ assertion, renderedValue, outputString, inverse }) => {
	invariant(renderedValue, "\"starts-with\" assertion type must have a string value");
	invariant(typeof renderedValue === "string", "\"starts-with\" assertion type must have a string value");
	const pass = outputString.startsWith(String(renderedValue)) !== inverse;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : `Expected output to ${inverse ? "not " : ""}start with "${renderedValue}"`,
		assertion
	};
};

//#endregion
//#region src/assertions/toolCallF1.ts
/**
* Extracts tool names from various output formats.
*
* Supports:
* - OpenAI format: { tool_calls: [{ function: { name: "..." } }] }
* - OpenAI direct array: [{ function: { name: "..." } }]
* - Simple format: [{ name: "..." }]
* - Anthropic format: { type: 'tool_use', name: '...' } or arrays of content blocks
* - Google/Vertex format: { functionCall: { name: '...' } } or arrays
* - Google Live format: { toolCall: { functionCalls: [...] } }
* - String output: JSON-stringified versions of the above, including mixed text/JSON
*/
function extractToolNames(output) {
	const names = /* @__PURE__ */ new Set();
	if (output === null || output === void 0) return names;
	if (typeof output === "string") {
		try {
			const parsedNames = extractToolNames(JSON.parse(output));
			for (const name of parsedNames) names.add(name);
			return names;
		} catch {}
		const lines = output.split("\n");
		for (const line of lines) {
			const trimmed = line.trim();
			if (trimmed.startsWith("{") || trimmed.startsWith("[")) try {
				const parsedNames = extractToolNames(JSON.parse(trimmed));
				for (const name of parsedNames) names.add(name);
			} catch {}
		}
		return names;
	}
	if (typeof output !== "object") return names;
	const obj = output;
	if ("tool_calls" in obj && Array.isArray(obj.tool_calls)) {
		for (const tc of obj.tool_calls) if (tc && typeof tc === "object") {
			const toolCall = tc;
			if (toolCall.function && typeof toolCall.function === "object") {
				const fn = toolCall.function;
				if (typeof fn.name === "string") names.add(fn.name);
			}
			if (typeof toolCall.name === "string") names.add(toolCall.name);
		}
		return names;
	}
	if (obj.type === "tool_use" && typeof obj.name === "string") {
		names.add(obj.name);
		return names;
	}
	if ("functionCall" in obj && obj.functionCall && typeof obj.functionCall === "object") {
		const fc = obj.functionCall;
		if (typeof fc.name === "string") names.add(fc.name);
		return names;
	}
	if ("toolCall" in obj && obj.toolCall && typeof obj.toolCall === "object") {
		const toolCall = obj.toolCall;
		if ("functionCalls" in toolCall && Array.isArray(toolCall.functionCalls)) {
			for (const fc of toolCall.functionCalls) if (fc && typeof fc === "object" && typeof fc.name === "string") names.add(fc.name);
		}
		return names;
	}
	if (Array.isArray(output)) {
		for (const item of output) if (item && typeof item === "object") {
			const block = item;
			if (block.type === "tool_use" && typeof block.name === "string") {
				names.add(block.name);
				continue;
			}
			if ("functionCall" in block && block.functionCall && typeof block.functionCall === "object") {
				const fc = block.functionCall;
				if (typeof fc.name === "string") names.add(fc.name);
				continue;
			}
			if (block.function && typeof block.function === "object") {
				const fn = block.function;
				if (typeof fn.name === "string") names.add(fn.name);
				continue;
			}
			if (typeof block.name === "string") names.add(block.name);
		}
	}
	return names;
}
/**
* Computes the F1 score for tool call evaluation.
*
* The F1 score is the harmonic mean of precision and recall, originally
* introduced by van Rijsbergen (1979) for information retrieval evaluation.
*
* For tool calls:
* - Precision = |actual âˆ© expected| / |actual|
*   "Of the tools called, how many were correct?"
*
* - Recall = |actual âˆ© expected| / |expected|
*   "Of the expected tools, how many were called?"
*
* - F1 = 2 Ã— (precision Ã— recall) / (precision + recall)
*
* This metric uses unordered set comparison - only the presence of tool names
* matters, not the order or frequency of calls.
*
* @see http://www.dcs.gla.ac.uk/Keith/Preface.html - van Rijsbergen's "Information Retrieval"
* @see https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/agents/#tool-call-f1
*/
const handleToolCallF1 = ({ assertion, output, renderedValue, inverse }) => {
	let expectedTools;
	if (Array.isArray(renderedValue)) expectedTools = renderedValue.map(String);
	else if (typeof renderedValue === "string") expectedTools = renderedValue.split(",").map((s) => s.trim());
	else invariant(false, "\"tool-call-f1\" assertion requires a value: array of tool names or comma-separated string");
	if (expectedTools.length === 0) invariant(false, "\"tool-call-f1\" assertion requires at least one expected tool name");
	const expected = new Set(expectedTools);
	const actual = extractToolNames(output);
	const intersection = [...expected].filter((t) => actual.has(t)).length;
	const precision = actual.size > 0 ? intersection / actual.size : 0;
	const recall = expected.size > 0 ? intersection / expected.size : 0;
	const f1 = precision + recall > 0 ? 2 * precision * recall / (precision + recall) : 0;
	const threshold = assertion.threshold ?? 1;
	const pass = f1 >= threshold !== inverse;
	const expectedList = [...expected].sort().join(", ");
	const actualList = [...actual].sort().join(", ") || "(none)";
	return {
		pass,
		score: f1,
		reason: pass ? `Tool Call F1: ${f1.toFixed(3)} (precision=${precision.toFixed(3)}, recall=${recall.toFixed(3)}). Expected: [${expectedList}], Called: [${actualList}]` : `Tool Call F1 score ${f1.toFixed(3)} is ${inverse ? "above" : "below"} threshold ${threshold}. Expected: [${expectedList}], Called: [${actualList}]. Precision=${precision.toFixed(3)}, Recall=${recall.toFixed(3)}`,
		assertion
	};
};

//#endregion
//#region src/assertions/traceUtils.ts
/**
* Shared utilities for trace assertions
*/
/**
* Match a span name against a glob-like pattern.
* Supports * (any characters) and ? (single character) wildcards.
*
* @param spanName - The span name to match
* @param pattern - The glob pattern to match against
* @returns true if the span name matches the pattern
*/
function matchesPattern(spanName, pattern) {
	const regexPattern = pattern.replace(/[.+^${}()|[\]\\]/g, "\\$&").replace(/\*/g, ".*").replace(/\?/g, ".");
	return new RegExp(`^${regexPattern}$`, "i").test(spanName);
}

//#endregion
//#region src/assertions/traceErrorSpans.ts
function isErrorSpan(span) {
	if (span.statusCode && span.statusCode >= 400) return true;
	if (span.attributes) {
		for (const key of [
			"error",
			"exception",
			"failed",
			"failure"
		]) if (span.attributes[key] === true || span.attributes[key] === "true" || typeof span.attributes[key] === "object" && span.attributes[key] !== null) return true;
		const httpStatusCode = span.attributes["http.status_code"];
		if (httpStatusCode !== void 0 && httpStatusCode !== null) {
			const statusCodeNum = Number(httpStatusCode);
			if (!isNaN(statusCodeNum) && statusCodeNum >= 400) return true;
		}
		if (span.attributes["otel.status_code"] === "ERROR" || span.attributes["status.code"] === "ERROR") return true;
	}
	if (span.statusMessage) {
		if (/error|failed|failure|exception|timeout|abort/i.test(span.statusMessage)) return true;
	}
	return false;
}
const handleTraceErrorSpans = ({ assertion, assertionValueContext }) => {
	if (!assertionValueContext.trace || !assertionValueContext.trace.spans) throw new Error("No trace data available for trace-error-spans assertion");
	const value = assertion.value;
	let maxCount;
	let maxPercentage;
	let pattern = "*";
	if (typeof value === "number") maxCount = value;
	else if (value && typeof value === "object" && !Array.isArray(value) && typeof value !== "function") {
		const objValue = value;
		maxCount = objValue.max_count;
		maxPercentage = objValue.max_percentage;
		pattern = objValue.pattern || "*";
	}
	if (maxCount === void 0 && maxPercentage === void 0) maxCount = 0;
	const matchingSpans = assertionValueContext.trace.spans.filter((span) => matchesPattern(span.name, pattern));
	if (matchingSpans.length === 0) return {
		pass: true,
		score: 1,
		reason: `No spans found matching pattern "${pattern}"`,
		assertion
	};
	const errorSpans = matchingSpans.filter(isErrorSpan);
	const errorCount = errorSpans.length;
	const errorPercentage = errorCount / matchingSpans.length * 100;
	let pass = true;
	let reason = "";
	if (maxCount !== void 0 && errorCount > maxCount) {
		pass = false;
		const errorDetails = errorSpans.slice(0, 3).map((span) => {
			let detail = span.name;
			if (span.statusMessage) detail += ` (${span.statusMessage})`;
			else if (span.statusCode) detail += ` (status: ${span.statusCode})`;
			return detail;
		});
		reason = `Found ${errorCount} error spans, expected at most ${maxCount}. `;
		reason += `Errors: ${errorDetails.join(", ")}`;
		if (errorSpans.length > 3) reason += ` and ${errorSpans.length - 3} more`;
	} else if (maxPercentage !== void 0 && errorPercentage > maxPercentage) {
		pass = false;
		reason = `Error rate ${errorPercentage.toFixed(1)}% exceeds threshold ${maxPercentage}% `;
		reason += `(${errorCount} errors out of ${matchingSpans.length} spans)`;
	} else if (errorCount === 0) reason = `No errors found in ${matchingSpans.length} spans matching pattern "${pattern}"`;
	else {
		reason = `Found ${errorCount} error(s) in ${matchingSpans.length} spans (${errorPercentage.toFixed(1)}%)`;
		if (maxCount !== void 0) reason += `, within threshold of ${maxCount}`;
		if (maxPercentage !== void 0) reason += `, within threshold of ${maxPercentage}%`;
	}
	return {
		pass,
		score: pass ? 1 : 0,
		reason,
		assertion
	};
};

//#endregion
//#region src/assertions/traceSpanCount.ts
const handleTraceSpanCount = ({ assertion, assertionValueContext }) => {
	if (!assertionValueContext.trace || !assertionValueContext.trace.spans) throw new Error("No trace data available for trace-span-count assertion");
	const value = assertion.value;
	if (!value || typeof value !== "object" || !value.pattern) throw new Error("trace-span-count assertion must have a value object with pattern property");
	const { pattern, min, max } = value;
	const matchingSpans = assertionValueContext.trace.spans.filter((span) => matchesPattern(span.name, pattern));
	const count = matchingSpans.length;
	let pass = true;
	let reason = "";
	if (min !== void 0 && count < min) {
		pass = false;
		reason = `Found ${count} spans matching pattern "${pattern}", expected at least ${min}`;
	} else if (max !== void 0 && count > max) {
		pass = false;
		reason = `Found ${count} spans matching pattern "${pattern}", expected at most ${max}`;
	} else {
		reason = `Found ${count} spans matching pattern "${pattern}"`;
		if (min !== void 0 && max !== void 0) reason += ` (expected ${min}-${max})`;
		else if (min !== void 0) reason += ` (expected at least ${min})`;
		else if (max !== void 0) reason += ` (expected at most ${max})`;
	}
	if (matchingSpans.length > 0) {
		const spanNames = [...new Set(matchingSpans.map((s) => s.name))];
		reason += `. Matched spans: ${spanNames.join(", ")}`;
	}
	return {
		pass,
		score: pass ? 1 : 0,
		reason,
		assertion
	};
};

//#endregion
//#region src/assertions/traceSpanDuration.ts
function calculatePercentile(durations, percentile) {
	if (durations.length === 0) return 0;
	const sorted = [...durations].sort((a, b) => a - b);
	const index = Math.ceil(percentile / 100 * sorted.length) - 1;
	return sorted[Math.max(0, index)];
}
const handleTraceSpanDuration = ({ assertion, assertionValueContext }) => {
	if (!assertionValueContext.trace || !assertionValueContext.trace.spans) throw new Error("No trace data available for trace-span-duration assertion");
	const value = assertion.value;
	if (!value || typeof value !== "object" || typeof value.max !== "number") throw new Error("trace-span-duration assertion must have a value object with max property");
	const { pattern = "*", max, percentile } = value;
	const matchingSpans = assertionValueContext.trace.spans.filter((span) => {
		return matchesPattern(span.name, pattern) && span.startTime !== void 0 && span.endTime !== void 0;
	});
	if (matchingSpans.length === 0) return {
		pass: true,
		score: 1,
		reason: `No spans found matching pattern "${pattern}" with complete timing data`,
		assertion
	};
	const spanDurations = matchingSpans.map((span) => {
		return {
			name: span.name,
			duration: span.endTime - span.startTime
		};
	});
	let pass = true;
	let reason = "";
	if (percentile === void 0) {
		const slowSpans = spanDurations.filter((s) => s.duration > max);
		if (slowSpans.length > 0) {
			pass = false;
			const top3Slow = slowSpans.sort((a, b) => b.duration - a.duration).slice(0, 3);
			reason = `${slowSpans.length} span(s) exceed duration threshold ${max}ms. `;
			reason += `Slowest: ${top3Slow.map((s) => `${s.name} (${s.duration}ms)`).join(", ")}`;
		} else {
			const maxDuration = Math.max(...spanDurations.map((s) => s.duration));
			reason = `All ${matchingSpans.length} spans matching pattern "${pattern}" completed within ${max}ms (max: ${maxDuration}ms)`;
		}
	} else {
		const percentileValue = calculatePercentile(spanDurations.map((s) => s.duration), percentile);
		if (percentileValue > max) {
			pass = false;
			const slowestSpans = spanDurations.filter((s) => s.duration >= percentileValue).sort((a, b) => b.duration - a.duration).slice(0, 3);
			reason = `${percentile}th percentile duration (${percentileValue}ms) exceeds threshold ${max}ms. `;
			reason += `Slowest spans: ${slowestSpans.map((s) => `${s.name} (${s.duration}ms)`).join(", ")}`;
		} else reason = `${percentile}th percentile duration (${percentileValue}ms) is within threshold ${max}ms`;
	}
	return {
		pass,
		score: pass ? 1 : 0,
		reason,
		assertion
	};
};

//#endregion
//#region src/assertions/webhook.ts
async function handleWebhook({ assertion, renderedValue, test, prompt, output, inverse }) {
	invariant(renderedValue, "\"webhook\" assertion type must have a URL value");
	invariant(typeof renderedValue === "string", "\"webhook\" assertion type must have a URL value");
	try {
		const context = {
			prompt,
			vars: test.vars || {}
		};
		const response = await fetchWithRetries(renderedValue, {
			method: "POST",
			headers: { "Content-Type": "application/json" },
			body: JSON.stringify({
				output,
				context
			})
		}, getEnvInt("WEBHOOK_TIMEOUT", 5e3));
		if (!response.ok) throw new Error(`Webhook response status: ${response.status}`);
		const jsonResponse = await response.json();
		const pass = jsonResponse.pass !== inverse;
		return {
			pass,
			score: typeof jsonResponse.score === "undefined" ? pass ? 1 : 0 : inverse ? 1 - jsonResponse.score : jsonResponse.score,
			reason: jsonResponse.reason || (pass ? "Assertion passed" : `Webhook returned ${inverse ? "true" : "false"}`),
			assertion
		};
	} catch (err) {
		return {
			pass: false,
			score: 0,
			reason: `Webhook error: ${err.message}`,
			assertion
		};
	}
}

//#endregion
//#region src/assertions/wordCount.ts
/**
* Counts words in a string by splitting on whitespace and filtering empty strings
*/
function countWords(text) {
	return text.trim().split(/\s+/).filter((word) => word.length > 0).length;
}
/**
* Handles word-count assertion
*
* Supports the following formats:
* 1. Exact count: value: 50
* 2. Range: value: { min: 20, max: 50 }
* 3. Min only: value: { min: 10 }
* 4. Max only: value: { max: 100 }
*/
const handleWordCount = ({ assertion, renderedValue, valueFromScript, outputString, inverse }) => {
	const value = valueFromScript ?? renderedValue;
	invariant(value != null, "\"word-count\" assertion must have a value");
	const wordCount = countWords(outputString);
	let pass;
	let reason;
	if (typeof value === "object" && !Array.isArray(value)) {
		const { min, max } = value;
		invariant(min !== void 0 || max !== void 0, "\"word-count\" assertion object must have \"min\" and/or \"max\" properties");
		if (min !== void 0 && max !== void 0) {
			invariant(min <= max, `"word-count" assertion: min (${min}) must be less than or equal to max (${max})`);
			const basePass = wordCount >= min && wordCount <= max;
			pass = inverse ? !basePass : basePass;
			if (pass) reason = "Assertion passed";
			else if (inverse) reason = `Expected word count to not be between ${min} and ${max}, but got ${wordCount}`;
			else reason = `Word count ${wordCount} is not between ${min} and ${max}`;
		} else if (min !== void 0) {
			const basePass = wordCount >= min;
			pass = inverse ? !basePass : basePass;
			if (pass) reason = "Assertion passed";
			else if (inverse) reason = `Expected word count to be less than ${min}, but got ${wordCount}`;
			else reason = `Word count ${wordCount} is less than minimum ${min}`;
		} else {
			const basePass = wordCount <= max;
			pass = inverse ? !basePass : basePass;
			if (pass) reason = "Assertion passed";
			else if (inverse) reason = `Expected word count to be greater than ${max}, but got ${wordCount}`;
			else reason = `Word count ${wordCount} is greater than maximum ${max}`;
		}
	} else {
		invariant(typeof value === "number" || typeof value === "string" && !Number.isNaN(Number(value)), "\"word-count\" assertion value must be a number or an object with min/max properties");
		const expectedCount = typeof value === "number" ? value : Number(value);
		const basePass = wordCount === expectedCount;
		pass = inverse ? !basePass : basePass;
		if (pass) reason = "Assertion passed";
		else if (inverse) reason = `Expected word count to not equal ${expectedCount}, but got ${wordCount}`;
		else reason = `Word count ${wordCount} does not equal expected ${expectedCount}`;
	}
	return {
		pass,
		score: pass ? 1 : 0,
		reason,
		assertion
	};
};

//#endregion
//#region src/assertions/xml.ts
function validateXml(xmlString, requiredElements) {
	if (!xmlString.startsWith("<")) return {
		isValid: false,
		reason: "XML is missing opening tag"
	};
	const parser = new XMLParser({
		allowBooleanAttributes: true,
		ignoreAttributes: false,
		parseAttributeValue: true,
		parseTagValue: true
	});
	try {
		const parsedXml = parser.parse(xmlString);
		if (requiredElements && requiredElements.length > 0) {
			const missingElements = requiredElements.filter((element) => {
				const path = element.split(".");
				let current = parsedXml;
				for (const key of path) {
					if (current[key] === void 0) return true;
					current = current[key];
				}
				return false;
			});
			if (missingElements.length > 0) return {
				isValid: false,
				reason: `XML is missing required elements: ${missingElements.join(", ")}`
			};
		}
		return {
			isValid: true,
			reason: "XML is valid and contains all required elements"
		};
	} catch (err) {
		return {
			isValid: false,
			reason: `XML parsing failed: ${err.message}`
		};
	}
}
function containsXml(outputString, requiredElements) {
	const xmlMatches = outputString.match(/<\?xml.*?>[\s\S]*<\/[^>]+>|\S*<[^>]+>[\s\S]*<\/[^>]+>/);
	if (!xmlMatches) return {
		isValid: false,
		reason: "No XML content found in the output"
	};
	for (const xmlMatch of xmlMatches) {
		const { isValid, reason } = validateXml(xmlMatch, requiredElements);
		if (isValid) return {
			isValid: true,
			reason
		};
	}
	return {
		isValid: false,
		reason: "No valid XML content found matching the requirements"
	};
}
const handleIsXml = ({ assertion, renderedValue, outputString, inverse, baseType }) => {
	let requiredElements;
	if (typeof renderedValue === "string") requiredElements = renderedValue.split(",").map((el) => el.trim());
	else if (Array.isArray(renderedValue) && renderedValue.length > 0) requiredElements = renderedValue.map((el) => el.toString());
	else if (renderedValue !== null && typeof renderedValue === "object" && Object.keys(renderedValue).length > 0) if ("requiredElements" in renderedValue && Array.isArray(renderedValue.requiredElements)) requiredElements = renderedValue.requiredElements.map((el) => el.toString());
	else throw new Error("xml assertion must contain a string, array value, or no value");
	const result = (baseType === "is-xml" ? validateXml : containsXml)(outputString, requiredElements);
	const pass = result.isValid !== inverse;
	return {
		pass,
		score: pass ? 1 : 0,
		reason: pass ? "Assertion passed" : result.reason,
		assertion
	};
};

//#endregion
//#region src/assertions/index.ts
const ASSERTIONS_MAX_CONCURRENCY = getEnvInt("PROMPTFOO_ASSERTIONS_MAX_CONCURRENCY", 3);
const MODEL_GRADED_ASSERTION_TYPES = new Set([
	"answer-relevance",
	"context-faithfulness",
	"context-recall",
	"context-relevance",
	"factuality",
	"llm-rubric",
	"model-graded-closedqa",
	"model-graded-factuality",
	"search-rubric"
]);
const ASSERTION_HANDLERS = {
	"answer-relevance": handleAnswerRelevance,
	bleu: handleBleuScore,
	classifier: handleClassifier,
	contains: handleContains,
	"contains-all": handleContainsAll,
	"contains-any": handleContainsAny,
	"contains-html": handleContainsHtml,
	"contains-json": handleContainsJson,
	"contains-sql": handleContainsSql,
	"contains-xml": handleIsXml,
	"context-faithfulness": handleContextFaithfulness,
	"context-recall": handleContextRecall,
	"context-relevance": handleContextRelevance,
	"conversation-relevance": handleConversationRelevance,
	cost: handleCost,
	equals: handleEquals,
	factuality: handleFactuality,
	"finish-reason": handleFinishReason,
	"g-eval": handleGEval,
	gleu: handleGleuScore,
	guardrails: handleGuardrails,
	icontains: handleIContains,
	"icontains-all": handleIContainsAll,
	"icontains-any": handleIContainsAny,
	"is-html": handleIsHtml,
	"is-json": handleIsJson,
	"is-refusal": handleIsRefusal,
	"is-sql": handleIsSql,
	"is-valid-function-call": handleIsValidFunctionCall,
	"is-valid-openai-function-call": handleIsValidFunctionCall,
	"is-valid-openai-tools-call": handleIsValidOpenAiToolsCall,
	"is-xml": handleIsXml,
	javascript: handleJavascript,
	latency: handleLatency,
	levenshtein: handleLevenshtein,
	"llm-rubric": handleLlmRubric,
	meteor: async (params) => {
		try {
			const { handleMeteorAssertion } = await import("./meteor-DLCdreag.js");
			return handleMeteorAssertion(params);
		} catch (error) {
			if (error instanceof Error && (error.message.includes("Cannot find module") || error.message.includes("natural\" package is required"))) return {
				pass: false,
				score: 0,
				reason: "METEOR assertion requires the natural package. Please install it using: npm install natural@^8.1.0",
				assertion: params.assertion
			};
			throw error;
		}
	},
	"model-graded-closedqa": handleModelGradedClosedQa,
	"model-graded-factuality": handleFactuality,
	moderation: handleModeration,
	perplexity: handlePerplexity,
	"perplexity-score": handlePerplexityScore,
	pi: handlePiScorer,
	python: handlePython,
	regex: handleRegex,
	ruby: handleRuby,
	"rouge-n": handleRougeScore,
	"search-rubric": handleSearchRubric,
	similar: handleSimilar,
	"similar:cosine": handleSimilar,
	"similar:dot": handleSimilar,
	"similar:euclidean": handleSimilar,
	"starts-with": handleStartsWith,
	"tool-call-f1": handleToolCallF1,
	"trace-error-spans": handleTraceErrorSpans,
	"trace-span-count": handleTraceSpanCount,
	"trace-span-duration": handleTraceSpanDuration,
	webhook: handleWebhook,
	"word-count": handleWordCount
};
const nunjucks = getNunjucksEngine();
/**
* Renders a metric name template with test variables.
* @param metric - The metric name, possibly containing Nunjucks template syntax
* @param vars - The test variables to use for rendering
* @returns The rendered metric name, or the original if rendering fails
*/
function renderMetricName(metric, vars) {
	if (!metric) return metric;
	try {
		const rendered = nunjucks.renderString(metric, vars);
		if (rendered === "" && metric !== "") logger_default.debug(`Metric template "${metric}" rendered to empty string`);
		return rendered;
	} catch (error) {
		logger_default.warn(`Failed to render metric template "${metric}": ${error instanceof Error ? error.message : error}`);
		return metric;
	}
}
/**
* Tests whether an assertion is inverse e.g. "not-equals" is inverse of "equals"
* or "not-contains" is inverse of "contains".
* @param assertion - The assertion to test
* @returns true if the assertion is inverse, false otherwise
*/
function isAssertionInverse(assertion) {
	return assertion.type.startsWith("not-");
}
/**
* Returns the base type of an assertion i.e. "not-equals" returns "equals"
* and "equals" returns "equals".
* @param assertion - The assertion to get the base type.
* @returns The base type of the assertion.
*/
function getAssertionBaseType(assertion) {
	return isAssertionInverse(assertion) ? assertion.type.slice(4) : assertion.type;
}
async function runAssertion({ prompt, provider, assertion, test, latencyMs, providerResponse, traceId }) {
	const { cost, logProbs, output: originalOutput } = providerResponse;
	let output = originalOutput;
	invariant(assertion.type, `Assertion must have a type: ${JSON.stringify(assertion)}`);
	if (assertion.transform) output = await transform(assertion.transform, output, {
		vars: test.vars,
		prompt: { label: prompt },
		...providerResponse && providerResponse.metadata && { metadata: providerResponse.metadata }
	});
	const context = {
		prompt,
		vars: test.vars || {},
		test,
		logProbs,
		provider,
		providerResponse,
		...assertion.config ? { config: structuredClone(assertion.config) } : {}
	};
	if (traceId) try {
		const traceData = await getTraceStore().getTrace(traceId);
		if (traceData) context.trace = {
			traceId: traceData.traceId,
			evaluationId: traceData.evaluationId,
			testCaseId: traceData.testCaseId,
			metadata: traceData.metadata,
			spans: traceData.spans || []
		};
	} catch (error) {
		logger_default.debug(`Failed to fetch trace data for assertion: ${error}`);
	}
	let renderedValue = assertion.value;
	let valueFromScript;
	if (typeof renderedValue === "string") if (renderedValue.startsWith("file://")) {
		const basePath = cliState_default.basePath || "";
		const fileRef = renderedValue.slice(7);
		let filePath = fileRef;
		let functionName;
		if (fileRef.includes(":")) {
			const [pathPart, funcPart] = fileRef.split(":");
			filePath = pathPart;
			functionName = funcPart;
		}
		filePath = path.resolve(basePath, filePath);
		if (isJavascriptFile(filePath)) {
			valueFromScript = await loadFromJavaScriptFile(filePath, functionName, [output, context]);
			logger_default.debug(`Javascript script ${filePath} output: ${valueFromScript}`);
		} else if (filePath.endsWith(".py")) try {
			valueFromScript = await runPython(filePath, functionName || "get_assert", [output, context]);
			logger_default.debug(`Python script ${filePath} output: ${valueFromScript}`);
		} catch (error) {
			return {
				pass: false,
				score: 0,
				reason: error.message,
				assertion
			};
		}
		else if (filePath.endsWith(".rb")) try {
			const { runRuby } = await import("./rubyUtils-D1DBBgSr.js");
			valueFromScript = await runRuby(filePath, functionName || "get_assert", [output, context]);
			logger_default.debug(`Ruby script ${filePath} output: ${valueFromScript}`);
		} catch (error) {
			return {
				pass: false,
				score: 0,
				reason: error.message,
				assertion
			};
		}
		else renderedValue = processFileReference(renderedValue);
	} else if (isPackagePath(renderedValue)) {
		const basePath = cliState_default.basePath || "";
		const requiredModule = await loadFromPackage(renderedValue, basePath);
		if (typeof requiredModule !== "function") throw new Error(`Assertion malformed: ${renderedValue} must be a function. Received: ${typeof requiredModule}`);
		valueFromScript = await Promise.resolve(requiredModule(output, context));
	} else renderedValue = nunjucks.renderString(renderedValue, test.vars || {});
	else if (renderedValue && Array.isArray(renderedValue)) renderedValue = renderedValue.map((v) => {
		if (typeof v === "string") {
			if (v.startsWith("file://")) return processFileReference(v);
			return nunjucks.renderString(v, test.vars || {});
		}
		return v;
	});
	const SCRIPT_RESULT_ASSERTIONS = new Set([
		"javascript",
		"python",
		"ruby"
	]);
	const baseType = getAssertionBaseType(assertion);
	if (valueFromScript !== void 0 && !SCRIPT_RESULT_ASSERTIONS.has(baseType)) {
		if (typeof valueFromScript === "function") throw new Error(`Script for "${assertion.type}" assertion returned a function. Only javascript/python/ruby assertion types can return functions. For other assertion types, return the expected value (string, number, array, or object).`);
		if (typeof valueFromScript === "boolean") throw new Error(`Script for "${assertion.type}" assertion returned a boolean. Only javascript/python/ruby assertion types can return boolean values. For other assertion types, return the expected value (string, number, array, or object).`);
		if (valueFromScript && typeof valueFromScript === "object" && !Array.isArray(valueFromScript) && "pass" in valueFromScript) throw new Error(`Script for "${assertion.type}" assertion returned a GradingResult. Only javascript/python/ruby assertion types can return GradingResult objects. For other assertion types, return the expected value (string, number, array, or object).`);
		renderedValue = valueFromScript;
	}
	const graderTraceparent = traceId ? generateTraceparent(traceId, generateSpanId()) : void 0;
	const providerCallContext = provider ? {
		originalProvider: provider,
		prompt: {
			raw: prompt || "",
			label: ""
		},
		vars: test.vars || {},
		...graderTraceparent && { traceparent: graderTraceparent }
	} : void 0;
	const assertionParams = {
		assertion,
		baseType: getAssertionBaseType(assertion),
		providerCallContext,
		assertionValueContext: context,
		cost,
		inverse: isAssertionInverse(assertion),
		latencyMs,
		logProbs,
		output,
		outputString: coerceString(output),
		prompt,
		provider,
		providerResponse,
		renderedValue,
		test: getFinalTest(test, assertion),
		valueFromScript
	};
	if (assertionParams.baseType.startsWith("promptfoo:redteam:")) return handleRedteam(assertionParams);
	const handler = ASSERTION_HANDLERS[assertionParams.baseType];
	if (handler) {
		const result = await handler(assertionParams);
		if (renderedValue !== void 0 && renderedValue !== assertion.value && typeof renderedValue === "string") {
			result.metadata = result.metadata || {};
			result.metadata.renderedAssertionValue = renderedValue;
		}
		if (assertion.weight === 0) return {
			...result,
			pass: true
		};
		return result;
	}
	throw new Error(`Unknown assertion type: ${assertion.type}`);
}
async function runAssertions({ assertScoringFunction, latencyMs, prompt, provider, providerResponse, test, traceId }) {
	if (!test.assert || test.assert.length < 1) return AssertionsResult.noAssertsResult();
	const mainAssertResult = new AssertionsResult({ threshold: test.threshold });
	const subAssertResults = [];
	const asserts = test.assert.map((assertion, i) => {
		if (assertion.type === "assert-set") {
			const subAssertResult = new AssertionsResult({
				threshold: assertion.threshold,
				parentAssertionSet: {
					assertionSet: assertion,
					index: i
				}
			});
			subAssertResults.push(subAssertResult);
			return assertion.assert.map((subAssert, j) => {
				return {
					assertion: subAssert,
					assertResult: subAssertResult,
					index: j
				};
			});
		}
		return {
			assertion,
			assertResult: mainAssertResult,
			index: i
		};
	}).flat();
	await async.forEachOfLimit(asserts, ASSERTIONS_MAX_CONCURRENCY, async ({ assertion, assertResult, index }) => {
		if (assertion.type.startsWith("select-") || assertion.type === "max-score") return;
		const result = await runAssertion({
			prompt,
			provider,
			providerResponse,
			assertion,
			test,
			latencyMs,
			assertIndex: index,
			traceId
		});
		assertResult.addResult({
			index,
			result,
			metric: renderMetricName(assertion.metric, test.vars || {}),
			weight: assertion.weight
		});
	});
	await async.forEach(subAssertResults, async (subAssertResult) => {
		const result = await subAssertResult.testResult();
		const { index, assertionSet: { metric, weight } } = subAssertResult.parentAssertionSet;
		mainAssertResult.addResult({
			index,
			result,
			metric: renderMetricName(metric, test.vars || {}),
			weight
		});
	});
	return mainAssertResult.testResult(assertScoringFunction);
}
async function runCompareAssertion(test, assertion, outputs, context) {
	invariant(typeof assertion.value === "string", "select-best must have a string value");
	test = getFinalTest(test, assertion);
	return (await matchesSelectBest(assertion.value, outputs, test.options, test.vars, context)).map((result) => ({
		...result,
		assertion
	}));
}
async function readAssertions(filePath) {
	try {
		const assertions = yaml.load(fs.readFileSync(filePath, "utf-8"));
		if (!Array.isArray(assertions) || assertions[0]?.type === void 0) throw new Error("Assertions file must be an array of assertion objects");
		return assertions;
	} catch (err) {
		throw new Error(`Failed to read assertions from ${filePath}:\n${err}`);
	}
}
var assertions_default = {
	runAssertion,
	runAssertions,
	matchesSimilarity,
	matchesClassification,
	matchesLlmRubric,
	matchesFactuality,
	matchesClosedQa,
	matchesAnswerRelevance,
	matchesContextRecall,
	matchesContextRelevance,
	matchesContextFaithfulness,
	matchesComparisonBoolean: matchesSelectBest,
	matchesModeration,
	matchesConversationRelevance
};

//#endregion
//#region src/util/promptMatching.ts
/**
* Checks if a prompt reference matches a given prompt by label or ID.
* Supports exact matching, wildcard matching (e.g., 'Group:*'),
* and legacy prefix matching (e.g., 'Group' matches 'Group:foo').
*
* @param ref - The reference string (label, ID, or pattern)
* @param prompt - The prompt to check against
* @returns true if the reference matches the prompt
*/
function doesPromptRefMatch(ref, prompt) {
	if (prompt.label === ref) return true;
	if (prompt.id && prompt.id === ref) return true;
	if (ref.endsWith("*")) {
		const prefix = ref.slice(0, -1);
		if (prompt.label?.startsWith(prefix)) return true;
		if (prompt.id?.startsWith(prefix)) return true;
	}
	if (prompt.label?.startsWith(`${ref}:`)) return true;
	if (prompt.id?.startsWith(`${ref}:`)) return true;
	return false;
}
/**
* Checks if a prompt is allowed based on a list of allowed prompt references.
*
* @param prompt - The prompt to check
* @param allowedPrompts - Array of allowed prompt references (labels, IDs, or patterns).
*                         If undefined, all prompts are allowed.
*                         If empty array, no prompts are allowed.
* @returns true if the prompt is allowed
*/
function isPromptAllowed(prompt, allowedPrompts) {
	if (!Array.isArray(allowedPrompts)) return true;
	if (allowedPrompts.length === 0) return false;
	return allowedPrompts.some((ref) => doesPromptRefMatch(ref, prompt));
}

//#endregion
//#region src/models/prompt.ts
function generateIdFromPrompt(prompt) {
	return prompt.id || prompt.label ? sha256(prompt.label) : sha256(typeof prompt.raw === "object" ? JSON.stringify(prompt.raw) : prompt.raw);
}

//#endregion
//#region src/progress/ciProgressReporter.ts
var CIProgressReporter = class {
	startTime;
	lastUpdateTime;
	totalTests;
	completedTests = 0;
	updateIntervalMs;
	intervalId = null;
	milestonesSeen = /* @__PURE__ */ new Set();
	highestPercentageSeen = 0;
	lastErrorTime = 0;
	ERROR_THROTTLE_MS = 5e3;
	constructor(totalTests, updateIntervalMs = 3e4) {
		this.startTime = Date.now();
		this.lastUpdateTime = this.startTime;
		this.totalTests = Math.max(totalTests, 1);
		this.updateIntervalMs = updateIntervalMs;
	}
	start() {
		if (this.intervalId) clearInterval(this.intervalId);
		logger_default.info(`[Evaluation] Starting ${this.totalTests} test cases...`);
		this.intervalId = setInterval(() => {
			this.logPeriodicUpdate();
		}, this.updateIntervalMs);
	}
	update(completed) {
		this.completedTests = completed;
		const percentage = Math.floor(completed / this.totalTests * 100);
		const milestones = [
			25,
			50,
			75
		];
		if (percentage > this.highestPercentageSeen) {
			this.highestPercentageSeen = percentage;
			if (milestones.includes(percentage) && !this.milestonesSeen.has(percentage)) {
				this.milestonesSeen.add(percentage);
				this.logMilestone(percentage);
			}
		}
	}
	updateTotalTests(newTotal) {
		this.totalTests = Math.max(newTotal, 1);
		this.highestPercentageSeen = Math.floor(this.completedTests / this.totalTests * 100);
	}
	finish() {
		if (this.intervalId) {
			clearInterval(this.intervalId);
			this.intervalId = null;
		}
		const elapsed = this.formatElapsedTime(Date.now() - this.startTime);
		logger_default.info(`[Evaluation] âœ“ Complete! ${this.completedTests}/${this.totalTests} tests in ${elapsed}`);
		if (process.env.GITHUB_ACTIONS) console.log(`::notice::Evaluation completed: ${this.completedTests}/${this.totalTests} tests in ${elapsed}`);
	}
	error(message) {
		const now = Date.now();
		if (now - this.lastErrorTime < this.ERROR_THROTTLE_MS) return;
		this.lastErrorTime = now;
		logger_default.error(`[Evaluation Error] ${message}`);
		if (process.env.GITHUB_ACTIONS) {
			const escapedMessage = message.replace(/\r?\n/g, " ").replace(/::/g, " ");
			console.log(`::error::${escapedMessage}`);
		}
	}
	logPeriodicUpdate() {
		if (this.completedTests === 0 || this.completedTests === this.totalTests) return;
		const elapsed = Math.max(Date.now() - this.startTime, 1e3);
		const rate = this.completedTests / (elapsed / 1e3 / 60);
		const remaining = this.totalTests - this.completedTests;
		let etaDisplay;
		if (rate < .1) etaDisplay = "calculating...";
		else {
			const eta = remaining / rate;
			if (eta > 1440) etaDisplay = ">24 hours";
			else etaDisplay = `${Math.round(eta)} minute${Math.round(eta) !== 1 ? "s" : ""}`;
		}
		const percentage = Math.floor(this.completedTests / this.totalTests * 100);
		logger_default.info(`[CI Progress] Evaluation running for ${this.formatElapsedTime(elapsed)} - Completed ${this.completedTests}/${this.totalTests} tests (${percentage}%)`);
		logger_default.info(`[CI Progress] Rate: ~${Math.round(rate)} tests/minute, ETA: ${etaDisplay}`);
	}
	logMilestone(percentage) {
		const elapsed = this.formatElapsedTime(Date.now() - this.startTime);
		logger_default.info(`[Evaluation] âœ“ ${percentage}% complete (${this.completedTests}/${this.totalTests}) - ${elapsed} elapsed`);
		if (process.env.GITHUB_ACTIONS) console.log(`::notice::Evaluation ${percentage}% complete`);
	}
	formatElapsedTime(ms) {
		const seconds = Math.floor(ms / 1e3);
		const minutes = Math.floor(seconds / 60);
		const remainingSeconds = seconds % 60;
		if (minutes === 0) return `${seconds}s`;
		return `${minutes}m ${remainingSeconds}s`;
	}
};

//#endregion
//#region src/providers/azure/warnings.ts
/**
* Emits a warning if Azure providers are used with model-graded assertions without
* explicitly setting a provider for the assertions.
*/
function maybeEmitAzureOpenAiWarning(testSuite, tests) {
	const hasAzure = testSuite.providers.some((p) => p.constructor.name === "AzureChatCompletionProvider" || p.constructor.name === "AzureCompletionProvider");
	const hasOpenAi = testSuite.providers.some((p) => p.constructor.name === "OpenAiChatCompletionProvider" || p.constructor.name === "OpenAiCompletionProvider" || p.constructor.name === "OpenAiAssistantProvider");
	if (hasAzure && !hasOpenAi && !(typeof testSuite.defaultTest === "object" && testSuite.defaultTest?.options?.provider)) {
		const modelGradedAsserts = tests.flatMap((t) => (t.assert || []).filter((a) => a.type !== "assert-set" && MODEL_GRADED_ASSERTION_TYPES.has(a.type) && !a.provider && !t.options?.provider));
		if (modelGradedAsserts.length > 0) {
			const assertTypes = Array.from(new Set(modelGradedAsserts.map((a) => a.type))).join(", ");
			logger_default.warn(chalk.yellow(`You are using model-graded assertions of types ${chalk.bold(assertTypes)} while testing an Azure provider. You may need to override these to use your Azure deployment. To learn more, see ${chalk.bold(`https://promptfoo.dev/docs/providers/azure/#model-graded-tests`)}`));
			return true;
		}
	}
	return false;
}

//#endregion
//#region src/suggestions.ts
async function generatePrompts(prompt, _num) {
	const resp = await DefaultSuggestionsProvider.callApi(JSON.stringify([
		SUGGEST_PROMPTS_SYSTEM_MESSAGE,
		{
			role: "user",
			content: "Generate a variant for the following prompt:"
		},
		{
			role: "user",
			content: prompt
		}
	]));
	if (resp.error || !resp.output) return {
		error: resp.error || "Unknown error",
		tokensUsed: normalizeTokenUsage(resp.tokenUsage)
	};
	try {
		return {
			prompts: [String(resp.output)],
			tokensUsed: normalizeTokenUsage(resp.tokenUsage)
		};
	} catch {
		return {
			error: `Output is not valid JSON: ${resp.output}`,
			tokensUsed: normalizeTokenUsage(resp.tokenUsage)
		};
	}
}

//#endregion
//#region src/tracing/otelConfig.ts
/**
* Get OTEL configuration from environment variables.
*/
function getOtelConfigFromEnv() {
	const endpoint = getEnvString("PROMPTFOO_OTEL_ENDPOINT") || getEnvString("OTEL_EXPORTER_OTLP_ENDPOINT");
	return {
		enabled: getEnvBool("PROMPTFOO_OTEL_ENABLED", false),
		serviceName: getEnvString("PROMPTFOO_OTEL_SERVICE_NAME", "promptfoo"),
		endpoint: endpoint || void 0,
		localExport: getEnvBool("PROMPTFOO_OTEL_LOCAL_EXPORT", true),
		debug: getEnvBool("PROMPTFOO_OTEL_DEBUG", false)
	};
}
/**
* Get default OTEL configuration with tracing enabled.
* Used when tracing is enabled via test metadata but no explicit config provided.
*/
function getDefaultOtelConfig() {
	return {
		...getOtelConfigFromEnv(),
		enabled: true
	};
}

//#endregion
//#region src/tracing/localSpanExporter.ts
/**
* A span exporter that writes spans to the local TraceStore (SQLite).
* This allows OTEL spans to be stored locally for analysis in the promptfoo UI.
*/
var LocalSpanExporter = class {
	/**
	* Export spans to the local TraceStore.
	* Spans are grouped by trace ID and inserted into the database.
	*/
	export(spans, resultCallback) {
		this.exportAsync(spans).then((error) => {
			if (error) resultCallback({
				code: ExportResultCode.FAILED,
				error
			});
			else resultCallback({ code: ExportResultCode.SUCCESS });
		}).catch((error) => {
			logger_default.error("[LocalSpanExporter] Failed to export spans", { error });
			resultCallback({
				code: ExportResultCode.FAILED,
				error: error instanceof Error ? error : new Error(String(error))
			});
		});
	}
	/**
	* Async implementation of span export.
	* Returns the first non-FK error encountered, or undefined if successful.
	*/
	async exportAsync(spans) {
		if (spans.length === 0) return;
		const traceStore = getTraceStore();
		logger_default.debug(`[LocalSpanExporter] Exporting ${spans.length} spans`);
		const spansByTrace = /* @__PURE__ */ new Map();
		for (const span of spans) {
			const traceId = span.spanContext().traceId;
			const spanData = this.convertSpan(span);
			if (!spansByTrace.has(traceId)) spansByTrace.set(traceId, []);
			spansByTrace.get(traceId).push(spanData);
		}
		let firstError;
		for (const [traceId, spanDataList] of spansByTrace) try {
			const result = await traceStore.addSpans(traceId, spanDataList, { skipTraceCheck: false });
			if (result.stored) logger_default.debug(`[LocalSpanExporter] Added ${spanDataList.length} spans to trace ${traceId}`);
			else logger_default.debug(`[LocalSpanExporter] Skipping ${spanDataList.length} spans for orphan trace ${traceId}: ${result.reason}`);
		} catch (error) {
			if ((error instanceof Error ? error.message : String(error)).includes("FOREIGN KEY")) logger_default.debug(`[LocalSpanExporter] Skipping ${spanDataList.length} spans for orphan trace ${traceId}`);
			else {
				logger_default.error(`[LocalSpanExporter] Failed to add spans to trace ${traceId}`, { error });
				if (!firstError) firstError = error instanceof Error ? error : new Error(String(error));
			}
		}
		return firstError;
	}
	/**
	* Convert an OTEL ReadableSpan to our SpanData format.
	*/
	convertSpan(span) {
		const spanContext = span.spanContext();
		const startTimeMs = span.startTime[0] * 1e3 + span.startTime[1] / 1e6;
		const endTimeMs = span.endTime[0] * 1e3 + span.endTime[1] / 1e6;
		return {
			spanId: spanContext.spanId,
			parentSpanId: span.parentSpanContext?.spanId || void 0,
			name: span.name,
			startTime: startTimeMs,
			endTime: endTimeMs,
			attributes: this.convertAttributes(span.attributes),
			statusCode: span.status.code,
			statusMessage: span.status.message
		};
	}
	/**
	* Convert OTEL attributes to a plain object.
	* OTEL attributes can have various value types that need normalization.
	*/
	convertAttributes(attributes) {
		const result = {};
		for (const [key, value] of Object.entries(attributes)) result[key] = value;
		return result;
	}
	/**
	* Shutdown the exporter. No-op for local storage.
	*/
	shutdown() {
		logger_default.debug("[LocalSpanExporter] Shutting down");
		return Promise.resolve();
	}
	/**
	* Force flush any pending spans. No-op as we export immediately.
	*/
	forceFlush() {
		return Promise.resolve();
	}
};

//#endregion
//#region src/tracing/otelSdk.ts
let provider = null;
let initialized = false;
const OTEL_HANDLERS_KEY = Symbol.for("promptfoo.otelHandlers");
function getHandlers() {
	const globalAny = globalThis;
	if (!globalAny[OTEL_HANDLERS_KEY]) globalAny[OTEL_HANDLERS_KEY] = {
		sigTermHandler: null,
		sigIntHandler: null,
		beforeExitHandler: null,
		registered: false
	};
	return globalAny[OTEL_HANDLERS_KEY];
}
/**
* Initialize the OpenTelemetry SDK for tracing LLM provider calls.
*
* This sets up:
* - A NodeTracerProvider with promptfoo service info
* - LocalSpanExporter for storing spans in TraceStore (SQLite)
* - Optional OTLPTraceExporter for external backends (Jaeger, Honeycomb, etc.)
*
* @param config - OTEL configuration
*/
function initializeOtel(config) {
	if (initialized) {
		logger_default.debug("[OtelSdk] Already initialized, skipping");
		return;
	}
	if (!config.enabled) {
		logger_default.debug("[OtelSdk] OTEL tracing is disabled");
		return;
	}
	logger_default.debug("[OtelSdk] Initializing OpenTelemetry SDK", {
		serviceName: config.serviceName,
		endpoint: config.endpoint,
		localExport: config.localExport
	});
	if (config.debug) diag.setLogger(new DiagConsoleLogger(), DiagLogLevel.DEBUG);
	propagation.setGlobalPropagator(new W3CTraceContextPropagator());
	logger_default.debug("[OtelSdk] Registered W3C Trace Context propagator");
	const resource = resourceFromAttributes({
		[ATTR_SERVICE_NAME]: config.serviceName,
		[ATTR_SERVICE_VERSION]: VERSION
	});
	const spanProcessors = [];
	if (config.localExport) {
		const localExporter = new LocalSpanExporter();
		spanProcessors.push(new BatchSpanProcessor(localExporter));
		logger_default.debug("[OtelSdk] Added local span exporter");
	}
	if (config.endpoint) {
		const otlpExporter = new OTLPTraceExporter({ url: config.endpoint });
		spanProcessors.push(new BatchSpanProcessor(otlpExporter));
		logger_default.debug(`[OtelSdk] Added OTLP exporter to ${config.endpoint}`);
	}
	provider = new NodeTracerProvider({
		resource,
		spanProcessors
	});
	provider.register();
	initialized = true;
	logger_default.info("[OtelSdk] OpenTelemetry SDK initialized successfully");
	setupShutdownHandlers();
}
/**
* Shutdown the OpenTelemetry SDK.
* Flushes any pending spans and releases resources.
*/
async function shutdownOtel() {
	if (!initialized || !provider) return;
	logger_default.debug("[OtelSdk] Shutting down OpenTelemetry SDK");
	try {
		await provider.shutdown();
		logger_default.info("[OtelSdk] OpenTelemetry SDK shut down successfully");
	} catch (error) {
		logger_default.error("[OtelSdk] Error shutting down OpenTelemetry SDK", { error });
	} finally {
		provider = null;
		initialized = false;
		cleanupShutdownHandlers();
	}
}
/**
* Force flush any pending spans.
* Useful before process exit to ensure all spans are exported.
*/
async function flushOtel() {
	if (!initialized || !provider) return;
	logger_default.debug("[OtelSdk] Flushing pending spans");
	try {
		await provider.forceFlush();
		logger_default.debug("[OtelSdk] Spans flushed successfully");
	} catch (error) {
		logger_default.error("[OtelSdk] Error flushing spans", { error });
	}
}
/**
* Set up handlers for graceful shutdown on process signals.
* Uses once() listeners and tracks registration globally to avoid duplicates
* across module resets (important for tests).
*/
function setupShutdownHandlers() {
	const handlers = getHandlers();
	if (handlers.registered) return;
	const shutdown = async (signal) => {
		logger_default.debug(`[OtelSdk] Received ${signal}, shutting down`);
		await shutdownOtel();
	};
	handlers.sigTermHandler = () => {
		shutdown("SIGTERM");
	};
	handlers.sigIntHandler = () => {
		shutdown("SIGINT");
	};
	handlers.beforeExitHandler = async () => {
		await flushOtel();
	};
	process.once("SIGTERM", handlers.sigTermHandler);
	process.once("SIGINT", handlers.sigIntHandler);
	process.once("beforeExit", handlers.beforeExitHandler);
	handlers.registered = true;
}
/**
* Clean up shutdown handlers.
* Called during shutdown to prevent duplicate registrations on reinit.
*/
function cleanupShutdownHandlers() {
	const handlers = getHandlers();
	if (handlers.sigTermHandler) {
		process.removeListener("SIGTERM", handlers.sigTermHandler);
		handlers.sigTermHandler = null;
	}
	if (handlers.sigIntHandler) {
		process.removeListener("SIGINT", handlers.sigIntHandler);
		handlers.sigIntHandler = null;
	}
	if (handlers.beforeExitHandler) {
		process.removeListener("beforeExit", handlers.beforeExitHandler);
		handlers.beforeExitHandler = null;
	}
	handlers.registered = false;
}

//#endregion
//#region src/util/exportToFile/writeToFile.ts
var JsonlFileWriter = class {
	writeStream;
	constructor(filePath) {
		this.filePath = filePath;
		this.writeStream = createWriteStream(filePath, { flags: "a" });
	}
	async write(data) {
		const jsonLine = JSON.stringify(data) + "\n";
		return new Promise((resolve, reject) => {
			this.writeStream.write(jsonLine, (error) => {
				if (error) reject(error);
				else resolve();
			});
		});
	}
	async close() {
		return new Promise((resolve) => {
			this.writeStream.end(resolve);
		});
	}
};

//#endregion
//#region src/evaluator.ts
/**
* Manages a single progress bar for the evaluation
*/
var ProgressBarManager = class {
	progressBar;
	isWebUI;
	totalCount = 0;
	completedCount = 0;
	concurrency = 1;
	constructor(isWebUI) {
		this.isWebUI = isWebUI;
	}
	/**
	* Initialize progress bar
	*/
	async initialize(runEvalOptions, concurrency, compareRowsCount) {
		if (this.isWebUI) return;
		this.totalCount = runEvalOptions.length + compareRowsCount;
		this.concurrency = concurrency;
		this.progressBar = new cliProgress.SingleBar({
			format: (options, params, payload) => {
				const barsize = options.barsize ?? 40;
				const barCompleteString = options.barCompleteString ?? "=";
				const barIncompleteString = options.barIncompleteString ?? "-";
				const bar = barCompleteString.substring(0, Math.round(params.progress * barsize));
				const spaces = barIncompleteString.substring(0, barsize - bar.length);
				const percentage = Math.round(params.progress * 100);
				const errorsText = payload.errors > 0 ? ` (errors: ${payload.errors})` : "";
				return `Evaluating [${bar}${spaces}] ${percentage}% | ${params.value}/${params.total}${errorsText} | ${payload.provider} ${payload.prompt} ${payload.vars}`;
			},
			hideCursor: true,
			gracefulExit: true
		}, cliProgress.Presets.shades_classic);
		this.progressBar.start(this.totalCount, 0, {
			provider: "",
			prompt: "",
			vars: "",
			errors: 0
		});
	}
	/**
	* Update progress for a specific evaluation
	*/
	updateProgress(_index, evalStep, _phase = "concurrent", metrics) {
		if (this.isWebUI || !evalStep || !this.progressBar) return;
		this.completedCount++;
		const provider = evalStep.provider.label || evalStep.provider.id();
		const prompt = `"${evalStep.prompt.raw.slice(0, 10).replace(/\n/g, " ")}"`;
		const vars = formatVarsForDisplay(evalStep.test.vars, 40);
		this.progressBar.increment({
			provider,
			prompt: prompt || "\"\"",
			vars: vars || "",
			errors: metrics?.testErrorCount ?? 0
		});
	}
	/**
	* Update comparison progress
	*/
	updateComparisonProgress(prompt) {
		if (this.isWebUI || !this.progressBar) return;
		this.completedCount++;
		this.progressBar.increment({
			provider: "Grading",
			prompt: `"${prompt.slice(0, 10).replace(/\n/g, " ")}"`,
			vars: "",
			errors: 0
		});
	}
	/**
	* Update total count when comparison count is determined
	*/
	updateTotalCount(additionalCount) {
		if (this.isWebUI || !this.progressBar || additionalCount <= 0) return;
		this.totalCount += additionalCount;
		this.progressBar.setTotal(this.totalCount);
	}
	/**
	* Mark evaluation as complete
	*/
	complete() {
		if (this.isWebUI || !this.progressBar) return;
		this.progressBar.update(this.totalCount);
	}
	/**
	* Stop the progress bar
	*/
	stop() {
		if (this.progressBar) this.progressBar.stop();
	}
};
/**
* Update token usage metrics with assertion token usage
*/
function updateAssertionMetrics(metrics, assertionTokens) {
	if (metrics.tokenUsage && assertionTokens) {
		if (!metrics.tokenUsage.assertions) metrics.tokenUsage.assertions = createEmptyAssertions();
		accumulateAssertionTokenUsage(metrics.tokenUsage.assertions, assertionTokens);
	}
}
/**
* Validates if a given prompt is allowed based on the provided list of allowed
* prompt references. Providers and tests can be configured with a `prompts` attribute,
* which corresponds to an array of prompt labels or IDs. References can either match
* exactly or use wildcard patterns. Examples:
*
* - `prompts: ['examplePrompt']` matches prompt with label OR id 'examplePrompt'
* - `prompts: ['exampleGroup:*']` matches any prompt with label/id starting with 'exampleGroup:'
* - `prompts: ['exampleGroup']` matches 'exampleGroup' exactly OR any label/id starting with 'exampleGroup:'
*
* If no `prompts` attribute is present, all prompts are allowed by default.
*
* @param prompt - The prompt object to check.
* @param allowedPrompts - The list of allowed prompt labels or IDs.
* @returns Returns true if the prompt is allowed, false otherwise.
*/
function isAllowedPrompt(prompt, allowedPrompts) {
	return isPromptAllowed(prompt, allowedPrompts);
}
/**
* Runs a single test case.
* @param options - The options for running the test case.
* {
*   provider - The provider to use for the test case.
*   prompt - The raw prompt to use for the test case.
*   test - The test case to run with assertions, etc.
*   delay - A delay in ms to wait before any provider calls
*   nunjucksFilters - The nunjucks filters to use for the test case.
*   evaluateOptions - Currently unused
*   testIdx - The index of the test case among all tests (row in the results table).
*   promptIdx - The index of the prompt among all prompts (column in the results table).
*   conversations - Evals can be run serially across multiple turns of a conversation. This gives access to the conversation history.
*   registers - The registers to use for the test case to store values for later tests.
*   isRedteam - Whether the test case is a redteam test case.
* }
* @returns The result of the test case.
*/
async function runEval({ provider, prompt, test, testSuite, delay, nunjucksFilters: filters, evaluateOptions, testIdx, promptIdx, repeatIndex, conversations, registers, isRedteam, abortSignal, evalId, rateLimitRegistry }) {
	const promptLabel = prompt.label;
	provider.delay ??= delay ?? getEnvInt("PROMPTFOO_DELAY_MS", 0);
	invariant(typeof provider.delay === "number", `Provider delay should be set for ${provider.label}`);
	const vars = structuredClone(test.vars || {});
	const fileMetadata = collectFileMetadata(test.vars || vars);
	const conversationKey = `${provider.label || provider.id()}:${prompt.id}${test.metadata?.conversationId ? `:${test.metadata.conversationId}` : ""}`;
	const usesConversation = prompt.raw.includes("_conversation");
	if (!getEnvBool("PROMPTFOO_DISABLE_CONVERSATION_VAR") && !test.options?.disableConversationVar && usesConversation) vars._conversation = conversations?.[conversationKey] || [];
	Object.assign(vars, registers);
	const mergedPromptConfig = {
		...prompt.config ?? {},
		...test.options ?? {}
	};
	const setup = {
		provider: {
			id: provider.id(),
			label: provider.label,
			config: provider.config
		},
		prompt: {
			raw: "",
			label: promptLabel,
			config: mergedPromptConfig
		},
		vars
	};
	let latencyMs = 0;
	let traceContext = null;
	try {
		const renderedPrompt = await renderPrompt(prompt, vars, filters, provider, isRedteam ? [testSuite?.redteam?.injectVar ?? "prompt"] : void 0);
		let renderedJson = void 0;
		try {
			renderedJson = JSON.parse(renderedPrompt);
		} catch {}
		setup.prompt.raw = renderedPrompt;
		const startTime = Date.now();
		let response = {
			output: "",
			tokenUsage: createEmptyTokenUsage(),
			cost: 0,
			cached: false
		};
		if (test.providerOutput) response.output = test.providerOutput;
		else {
			const activeProvider = isApiProvider(test.provider) ? test.provider : provider;
			logger_default.debug(`Provider type: ${activeProvider.id()}`);
			traceContext = await generateTraceContextIfNeeded(test, evaluateOptions, testIdx, promptIdx, testSuite);
			const callApiContext = {
				vars,
				prompt: {
					...prompt,
					config: mergedPromptConfig
				},
				filters,
				originalProvider: provider,
				test,
				logger: logger_default,
				getCache,
				repeatIndex
			};
			if (repeatIndex > 0) callApiContext.bustCache = true;
			if (traceContext) {
				callApiContext.traceparent = traceContext.traceparent;
				callApiContext.evaluationId = traceContext.evaluationId;
				callApiContext.testCaseId = traceContext.testCaseId;
			}
			if (rateLimitRegistry) response = await rateLimitRegistry.execute(activeProvider, () => activeProvider.callApi(renderedPrompt, callApiContext, abortSignal ? { abortSignal } : void 0), createProviderRateLimitOptions());
			else response = await activeProvider.callApi(renderedPrompt, callApiContext, abortSignal ? { abortSignal } : void 0);
			if (response.metadata) {
				const sanitizedMetadata = safeJsonStringify(response.metadata);
				response.metadata = sanitizedMetadata ? JSON.parse(sanitizedMetadata) : {};
			}
			logger_default.debug(`Provider response properties: ${Object.keys(response).join(", ")}`);
			logger_default.debug(`Provider response cached property explicitly: ${response.cached}`);
		}
		latencyMs = Date.now() - startTime;
		let conversationLastInput = void 0;
		if (renderedJson && Array.isArray(renderedJson)) {
			const lastElt = renderedJson[renderedJson.length - 1];
			conversationLastInput = lastElt?.content || lastElt;
		}
		if (conversations) {
			conversations[conversationKey] = conversations[conversationKey] || [];
			conversations[conversationKey].push({
				prompt: renderedJson || renderedPrompt,
				input: conversationLastInput || renderedJson || renderedPrompt,
				output: response.output || "",
				metadata: response.metadata
			});
		}
		logger_default.debug("Evaluator response", { responsePreview: (safeJsonStringify(response) ?? "").slice(0, 100) });
		logger_default.debug(`Evaluator checking cached flag: response.cached = ${Boolean(response.cached)}, provider.delay = ${provider.delay}`);
		if (!response.cached && provider.delay > 0) {
			logger_default.debug(`Sleeping for ${provider.delay}ms`);
			await sleep(provider.delay);
		} else if (response.cached) logger_default.debug(`Skipping delay because response is cached`);
		const ret = {
			...setup,
			response,
			success: false,
			failureReason: ResultFailureReason.NONE,
			score: 0,
			namedScores: {},
			latencyMs: response.latencyMs ?? latencyMs,
			cost: response.cost,
			metadata: {
				...test.metadata,
				...response.metadata,
				[FILE_METADATA_KEY]: fileMetadata
			},
			promptIdx,
			testIdx,
			testCase: test,
			promptId: prompt.id || "",
			tokenUsage: createEmptyTokenUsage()
		};
		if (!ret.metadata?.sessionIds && !ret.metadata?.sessionId) {
			ret.metadata ??= {};
			ret.metadata.sessionId = getSessionId(response, { vars });
		}
		invariant(ret.tokenUsage, "This is always defined, just doing this to shut TS up");
		if (response.tokenUsage) {
			const providerId = provider.id();
			const trackingId = provider.constructor?.name ? `${providerId} (${provider.constructor.name})` : providerId;
			TokenUsageTracker.getInstance().trackUsage(trackingId, response.tokenUsage);
		}
		if (response.error) {
			ret.error = response.error;
			ret.failureReason = ResultFailureReason.ERROR;
			ret.success = false;
		} else if (response.output === null || response.output === void 0) if (isRedteam) ret.success = true;
		else {
			ret.success = false;
			ret.score = 0;
			ret.error = "No output";
		}
		else {
			let processedResponse = { ...response };
			if (provider.transform) processedResponse.output = await transform(provider.transform, processedResponse.output, {
				vars,
				prompt
			});
			const providerTransformedOutput = processedResponse.output;
			const testTransform = test.options?.transform || test.options?.postprocess;
			if (testTransform) processedResponse.output = await transform(testTransform, processedResponse.output, {
				vars,
				prompt,
				...response && response.metadata && { metadata: response.metadata }
			});
			invariant(processedResponse.output != null, "Response output should not be null");
			const blobbedResponse = await extractAndStoreBinaryData(processedResponse, {
				evalId,
				testIdx,
				promptIdx
			});
			if (blobbedResponse) processedResponse = blobbedResponse;
			let traceId;
			if (traceContext?.traceparent) {
				const parts = traceContext.traceparent.split("-");
				if (parts.length >= 3) traceId = parts[1];
			}
			const checkResult = await runAssertions({
				prompt: renderedPrompt,
				provider,
				providerResponse: {
					...processedResponse,
					providerTransformedOutput
				},
				test,
				latencyMs: response.latencyMs ?? latencyMs,
				assertScoringFunction: test.assertScoringFunction,
				traceId
			});
			if (!checkResult.pass) {
				ret.error = checkResult.reason;
				ret.failureReason = ResultFailureReason.ASSERT;
			}
			ret.success = checkResult.pass;
			ret.score = checkResult.score;
			ret.namedScores = checkResult.namedScores || {};
			if (!ret.tokenUsage.assertions) ret.tokenUsage.assertions = createEmptyAssertions();
			ret.tokenUsage.assertions.numRequests = (ret.tokenUsage.assertions.numRequests ?? 0) + 1;
			if (checkResult.tokensUsed) accumulateAssertionTokenUsage(ret.tokenUsage.assertions, checkResult.tokensUsed);
			ret.response = processedResponse;
			ret.gradingResult = checkResult;
		}
		if (response.tokenUsage) accumulateResponseTokenUsage(ret.tokenUsage, response);
		if (test.options?.storeOutputAs && ret.response?.output && registers) registers[test.options.storeOutputAs] = ret.response.output;
		return [ret];
	} catch (err) {
		const { errorWithStack, metadata, logContext } = buildProviderErrorContext({
			error: err,
			provider,
			test,
			promptIdx,
			testIdx
		});
		logger_default.error("Provider call failed during eval", logContext);
		return [{
			...setup,
			error: errorWithStack,
			success: false,
			failureReason: ResultFailureReason.ERROR,
			score: 0,
			namedScores: {},
			latencyMs,
			promptIdx,
			testIdx,
			testCase: test,
			promptId: prompt.id || "",
			metadata
		}];
	}
}
function buildProviderErrorContext({ error, provider, test, promptIdx, testIdx }) {
	const providerId = provider.id();
	const providerLabel = provider.label;
	const errorWithResponse = error;
	const status = errorWithResponse?.response?.status;
	const statusText = errorWithResponse?.response?.statusText;
	const responseData = errorWithResponse?.response?.data;
	const responseSnippet = (() => {
		if (responseData == null) return;
		const asString = typeof responseData === "string" ? responseData : safeJsonStringify(responseData);
		if (!asString) return;
		return asString.length > 500 ? `${asString.slice(0, 500)}...` : asString;
	})();
	const errorMessage = String(error);
	const stack = error?.stack;
	return {
		errorWithStack: stack ? stack.startsWith(errorMessage) ? stack : `${errorMessage}\n\n${stack}` : errorMessage,
		metadata: {
			...test.metadata || {},
			errorContext: {
				providerId,
				providerLabel,
				status,
				statusText,
				responseSnippet
			}
		},
		logContext: {
			providerId,
			providerLabel,
			status,
			statusText,
			responseSnippet,
			promptIdx,
			testIdx,
			pluginId: test.metadata?.pluginId,
			strategyId: test.metadata?.strategyId,
			error
		}
	};
}
/**
* Safely formats variables for display in progress bars and logs.
* Handles extremely large variables that could cause RangeError crashes.
*
* @param vars - Variables to format
* @param maxLength - Maximum length of the final formatted string
* @returns Formatted variables string or fallback message
*/
function formatVarsForDisplay(vars, maxLength) {
	if (!vars || Object.keys(vars).length === 0) return "";
	try {
		return Object.entries(vars).map(([key, value]) => {
			return `${key}=${String(value).slice(0, 100)}`;
		}).join(" ").replace(/\n/g, " ").slice(0, maxLength);
	} catch {
		return "[vars unavailable]";
	}
}
function generateVarCombinations(vars) {
	const keys = Object.keys(vars);
	const combinations = [{}];
	for (const key of keys) {
		let values = [];
		if (typeof vars[key] === "string" && vars[key].startsWith("file://")) {
			const filePath = vars[key].slice(7);
			const basePath = cliState_default.basePath || "";
			values = (globSync(filePath, {
				cwd: basePath || process.cwd(),
				windowsPathsNoEscape: true
			}) || []).map((path) => `file://${path}`);
			if (values.length === 0) throw new Error(`No files found for variable ${key} at path ${filePath} in directory ${basePath || process.cwd()}`);
		} else values = Array.isArray(vars[key]) ? vars[key] : [vars[key]];
		if (Array.isArray(vars[key]) && typeof vars[key][0] !== "string") values = [vars[key]];
		const newCombinations = [];
		for (const combination of combinations) for (const value of values) newCombinations.push({
			...combination,
			[key]: value
		});
		combinations.length = 0;
		combinations.push(...newCombinations);
	}
	return combinations;
}
var Evaluator = class {
	evalRecord;
	testSuite;
	options;
	stats;
	conversations;
	registers;
	fileWriters;
	rateLimitRegistry;
	constructor(testSuite, evalRecord, options) {
		this.testSuite = testSuite;
		this.evalRecord = evalRecord;
		this.options = options;
		this.stats = {
			successes: 0,
			failures: 0,
			errors: 0,
			tokenUsage: createEmptyTokenUsage()
		};
		this.conversations = {};
		this.registers = {};
		this.fileWriters = (Array.isArray(evalRecord.config.outputPath) ? evalRecord.config.outputPath.filter((p) => p.endsWith(".jsonl")) : evalRecord.config.outputPath?.endsWith(".jsonl") ? [evalRecord.config.outputPath] : []).map((p) => new JsonlFileWriter(p));
		this.rateLimitRegistry = createRateLimitRegistry({ maxConcurrency: options.maxConcurrency || DEFAULT_MAX_CONCURRENCY });
		this.rateLimitRegistry.on("ratelimit:hit", (data) => {
			logger_default.debug(`[Scheduler] Rate limit hit for ${data.rateLimitKey}`, {
				retryAfterMs: data.retryAfterMs,
				resetAt: data.resetAt,
				concurrencyChange: data.concurrencyChange
			});
		});
		this.rateLimitRegistry.on("ratelimit:learned", (data) => {
			logger_default.debug(`[Scheduler] Learned rate limits for ${data.rateLimitKey}`, {
				requestLimit: data.requestLimit,
				tokenLimit: data.tokenLimit
			});
		});
		this.rateLimitRegistry.on("concurrency:decreased", (data) => {
			logger_default.debug(`[Scheduler] Concurrency decreased for ${data.rateLimitKey}`, {
				previous: data.previous,
				current: data.current
			});
		});
		this.rateLimitRegistry.on("concurrency:increased", (data) => {
			logger_default.debug(`[Scheduler] Concurrency increased for ${data.rateLimitKey}`, {
				previous: data.previous,
				current: data.current
			});
		});
		redteamProviderManager.setRateLimitRegistry(this.rateLimitRegistry);
	}
	/**
	* Updates metrics and stats after a comparison assertion (select-best or max-score).
	*/
	updateComparisonStats(result, passed, reason, tokensUsed, wasSuccess, wasScore, metrics) {
		if (metrics) {
			metrics.assertPassCount += passed ? 1 : 0;
			metrics.assertFailCount += passed ? 0 : 1;
			if (tokensUsed) updateAssertionMetrics(metrics, tokensUsed);
			if (!passed && result.score !== wasScore) metrics.score += result.score - wasScore;
		}
		if (wasSuccess && !result.success) {
			result.failureReason = ResultFailureReason.ASSERT;
			result.error = reason;
			if (metrics) {
				metrics.testPassCount -= 1;
				metrics.testFailCount += 1;
			}
			this.stats.successes -= 1;
			this.stats.failures += 1;
		}
	}
	async _runEvaluation() {
		const { options } = this;
		let { testSuite } = this;
		const startTime = Date.now();
		const maxEvalTimeMs = options.maxEvalTimeMs ?? getMaxEvalTimeMs();
		let evalTimedOut = false;
		let globalTimeout;
		let globalAbortController;
		const processedIndices = /* @__PURE__ */ new Set();
		let ciProgressReporter = null;
		let progressBarManager = null;
		if (maxEvalTimeMs > 0) {
			globalAbortController = new AbortController();
			options.abortSignal = options.abortSignal ? AbortSignal.any([options.abortSignal, globalAbortController.signal]) : globalAbortController.signal;
			globalTimeout = setTimeout(() => {
				evalTimedOut = true;
				globalAbortController?.abort();
			}, maxEvalTimeMs);
		}
		const vars = /* @__PURE__ */ new Set();
		const checkAbort = () => {
			if (options.abortSignal?.aborted) throw new Error("Operation cancelled");
		};
		if (!options.silent) logger_default.info(`Starting evaluation ${this.evalRecord.id}`);
		checkAbort();
		const prompts = [];
		const assertionTypes = /* @__PURE__ */ new Set();
		const rowsWithSelectBestAssertion = /* @__PURE__ */ new Set();
		const rowsWithMaxScoreAssertion = /* @__PURE__ */ new Set();
		if (testSuite.extensions?.length) {
			if (!testSuite.defaultTest) testSuite.defaultTest = {};
			if (typeof testSuite.defaultTest !== "string" && !testSuite.defaultTest.assert) testSuite.defaultTest.assert = [];
		}
		testSuite = (await runExtensionHook(testSuite.extensions, "beforeAll", { suite: testSuite })).suite;
		if (options.generateSuggestions) {
			logger_default.info(`Generating prompt variations...`);
			const { prompts: newPrompts, error } = await generatePrompts(testSuite.prompts[0].raw, 1);
			if (error || !newPrompts) throw new Error(`Failed to generate prompts: ${error}`);
			logger_default.info(chalk.blue("Generated prompts:"));
			let numAdded = 0;
			for (const prompt of newPrompts) {
				logger_default.info("--------------------------------------------------------");
				logger_default.info(`${prompt}`);
				logger_default.info("--------------------------------------------------------");
				if (await promptYesNo("Do you want to test this prompt?", false)) {
					testSuite.prompts.push({
						raw: prompt,
						label: prompt
					});
					numAdded++;
				} else logger_default.info("Skipping this prompt.");
			}
			if (numAdded < 1) {
				logger_default.info(chalk.red("No prompts selected. Aborting."));
				process.exitCode = 1;
				return this.evalRecord;
			}
		}
		const existingPromptsMap = /* @__PURE__ */ new Map();
		if (cliState_default.resume && this.evalRecord.persisted && this.evalRecord.prompts.length > 0) {
			logger_default.debug("Resuming evaluation: preserving metrics from previous run");
			for (const existingPrompt of this.evalRecord.prompts) {
				const key = `${existingPrompt.provider}:${existingPrompt.id}`;
				existingPromptsMap.set(key, existingPrompt);
			}
		}
		for (const provider of testSuite.providers) for (const prompt of testSuite.prompts) {
			const providerKey = provider.label || provider.id();
			if (!isAllowedPrompt(prompt, testSuite.providerPromptMap?.[providerKey])) continue;
			const promptId = generateIdFromPrompt(prompt);
			const existingPromptKey = `${providerKey}:${promptId}`;
			const existingPrompt = existingPromptsMap.get(existingPromptKey);
			const completedPrompt = {
				...prompt,
				id: promptId,
				provider: providerKey,
				label: prompt.label,
				metrics: existingPrompt?.metrics || {
					score: 0,
					testPassCount: 0,
					testFailCount: 0,
					testErrorCount: 0,
					assertPassCount: 0,
					assertFailCount: 0,
					totalLatencyMs: 0,
					tokenUsage: createEmptyTokenUsage(),
					namedScores: {},
					namedScoresCount: {},
					cost: 0
				}
			};
			prompts.push(completedPrompt);
		}
		await this.evalRecord.addPrompts(prompts);
		let tests = testSuite.tests && testSuite.tests.length > 0 ? testSuite.tests : testSuite.scenarios ? [] : [{}];
		if (testSuite.scenarios && testSuite.scenarios.length > 0) {
			telemetry_default.record("feature_used", { feature: "scenarios" });
			let scenarioIndex = 0;
			for (const scenario of testSuite.scenarios) for (const data of scenario.config) {
				const scenarioTests = (scenario.tests || [{}]).map((test) => {
					const mergedMetadata = {
						...typeof testSuite.defaultTest === "object" ? testSuite.defaultTest?.metadata : {},
						...data.metadata,
						...test.metadata
					};
					if (!mergedMetadata.conversationId) mergedMetadata.conversationId = `__scenario_${scenarioIndex}__`;
					return {
						...typeof testSuite.defaultTest === "object" ? testSuite.defaultTest : {},
						...data,
						...test,
						vars: {
							...typeof testSuite.defaultTest === "object" ? testSuite.defaultTest?.vars : {},
							...data.vars,
							...test.vars
						},
						options: {
							...typeof testSuite.defaultTest === "object" ? testSuite.defaultTest?.options : {},
							...test.options
						},
						assert: [...data.assert || [], ...test.assert || []],
						metadata: mergedMetadata
					};
				});
				tests = tests.concat(scenarioTests);
				scenarioIndex++;
			}
		}
		maybeEmitAzureOpenAiWarning(testSuite, tests);
		const varNames = /* @__PURE__ */ new Set();
		const varsWithSpecialColsRemoved = [];
		const inputTransformDefault = typeof testSuite?.defaultTest === "object" ? testSuite?.defaultTest?.options?.transformVars : void 0;
		for (const testCase of tests) {
			testCase.vars = {
				...typeof testSuite.defaultTest === "object" ? testSuite.defaultTest?.vars : {},
				...testCase?.vars
			};
			if (testCase.vars) {
				const varWithSpecialColsRemoved = {};
				const inputTransform = testCase.options?.transformVars || inputTransformDefault;
				if (inputTransform) {
					const transformContext = {
						prompt: {},
						uuid: crypto.randomUUID()
					};
					const transformedVars = await transform(inputTransform, testCase.vars, transformContext, true, TransformInputType.VARS);
					invariant(typeof transformedVars === "object", "Transform function did not return a valid object");
					testCase.vars = {
						...testCase.vars,
						...transformedVars
					};
				}
				for (const varName of Object.keys(testCase.vars)) {
					varNames.add(varName);
					varWithSpecialColsRemoved[varName] = testCase.vars[varName];
				}
				varsWithSpecialColsRemoved.push(varWithSpecialColsRemoved);
			}
		}
		const runEvalOptions = [];
		let testIdx = 0;
		let concurrency = options.maxConcurrency || DEFAULT_MAX_CONCURRENCY;
		for (let index = 0; index < tests.length; index++) {
			const testCase = tests[index];
			invariant(typeof testSuite.defaultTest !== "object" || Array.isArray(testSuite.defaultTest?.assert || []), `defaultTest.assert is not an array in test case #${index + 1}`);
			invariant(Array.isArray(testCase.assert || []), `testCase.assert is not an array in test case #${index + 1}`);
			testCase.assert = [...typeof testSuite.defaultTest === "object" ? testSuite.defaultTest?.assert || [] : [], ...testCase.assert || []];
			testCase.threshold = testCase.threshold ?? (typeof testSuite.defaultTest === "object" ? testSuite.defaultTest?.threshold : void 0);
			testCase.options = {
				...typeof testSuite.defaultTest === "object" ? testSuite.defaultTest?.options : {},
				...testCase.options
			};
			testCase.metadata = {
				...typeof testSuite.defaultTest === "object" ? testSuite.defaultTest?.metadata : {},
				...testCase.metadata
			};
			testCase.prompts = testCase.prompts ?? (typeof testSuite.defaultTest === "object" ? testSuite.defaultTest?.prompts : void 0);
			if (!testCase.provider && typeof testSuite.defaultTest === "object" && testSuite.defaultTest?.provider) {
				const defaultProvider = testSuite.defaultTest.provider;
				if (isApiProvider(defaultProvider)) testCase.provider = defaultProvider;
				else if (typeof defaultProvider === "object" && defaultProvider.id) {
					const { loadApiProvider } = await import("./providers-DSS-5-GM.js");
					testCase.provider = await loadApiProvider(typeof defaultProvider.id === "function" ? defaultProvider.id() : defaultProvider.id, { options: defaultProvider });
				} else testCase.provider = defaultProvider;
			}
			testCase.assertScoringFunction = testCase.assertScoringFunction || (typeof testSuite.defaultTest === "object" ? testSuite.defaultTest?.assertScoringFunction : void 0);
			testCase.providers = testCase.providers ?? (typeof testSuite.defaultTest === "object" ? testSuite.defaultTest?.providers : void 0);
			if (typeof testCase.assertScoringFunction === "string") {
				const { filePath: resolvedPath, functionName } = parseFileUrl(testCase.assertScoringFunction);
				testCase.assertScoringFunction = await loadFunction({
					filePath: resolvedPath,
					functionName
				});
			}
			const prependToPrompt = testCase.options?.prefix || (typeof testSuite.defaultTest === "object" ? testSuite.defaultTest?.options?.prefix : "") || "";
			const appendToPrompt = testCase.options?.suffix || (typeof testSuite.defaultTest === "object" ? testSuite.defaultTest?.options?.suffix : "") || "";
			const varCombinations = getEnvBool("PROMPTFOO_DISABLE_VAR_EXPANSION") || testCase.options?.disableVarExpansion ? [testCase.vars] : generateVarCombinations(testCase.vars || {});
			const numRepeat = this.options.repeat || 1;
			for (let repeatIndex = 0; repeatIndex < numRepeat; repeatIndex++) for (const vars of varCombinations) {
				let promptIdx = 0;
				for (const provider of testSuite.providers) {
					if (!isProviderAllowed(provider, testCase.providers)) continue;
					for (const prompt of testSuite.prompts) {
						const providerKey = provider.label || provider.id();
						if (!isAllowedPrompt(prompt, testSuite.providerPromptMap?.[providerKey])) continue;
						if (!isAllowedPrompt(prompt, testCase.prompts)) continue;
						runEvalOptions.push({
							delay: options.delay || 0,
							provider,
							prompt: {
								...prompt,
								raw: prependToPrompt + prompt.raw + appendToPrompt
							},
							testSuite,
							test: (() => {
								const baseTest = {
									...testCase,
									vars,
									options: testCase.options
								};
								const tracingEnabled = getEnvBool("PROMPTFOO_TRACING_ENABLED", false) || testCase.metadata?.tracingEnabled === true || testSuite.tracing?.enabled === true;
								logger_default.debug(`[Evaluator] Tracing check: env=${getEnvBool("PROMPTFOO_TRACING_ENABLED", false)}, testCase.metadata?.tracingEnabled=${testCase.metadata?.tracingEnabled}, testSuite.tracing?.enabled=${testSuite.tracing?.enabled}, tracingEnabled=${tracingEnabled}`);
								if (tracingEnabled) return {
									...baseTest,
									metadata: {
										...testCase.metadata,
										tracingEnabled: true,
										evaluationId: this.evalRecord.id
									}
								};
								return baseTest;
							})(),
							nunjucksFilters: testSuite.nunjucksFilters,
							testIdx,
							promptIdx,
							repeatIndex,
							evaluateOptions: options,
							conversations: this.conversations,
							registers: this.registers,
							isRedteam: testSuite.redteam != null,
							concurrency,
							abortSignal: options.abortSignal,
							evalId: this.evalRecord.id,
							rateLimitRegistry: this.rateLimitRegistry
						});
						promptIdx++;
					}
				}
				testIdx++;
			}
		}
		for (const evalOption of runEvalOptions) {
			if (evalOption.test.assert?.some((a) => a.type === "select-best")) rowsWithSelectBestAssertion.add(evalOption.testIdx);
			if (evalOption.test.assert?.some((a) => a.type === "max-score")) rowsWithMaxScoreAssertion.add(evalOption.testIdx);
		}
		if (cliState_default.resume && this.evalRecord.persisted) try {
			const { default: EvalResult } = await import("./evalResult-BncZSWua.js");
			const completedPairs = await EvalResult.getCompletedIndexPairs(this.evalRecord.id, { excludeErrors: cliState_default.retryMode });
			const originalCount = runEvalOptions.length;
			for (let i = runEvalOptions.length - 1; i >= 0; i--) {
				const step = runEvalOptions[i];
				if (completedPairs.has(`${step.testIdx}:${step.promptIdx}`)) runEvalOptions.splice(i, 1);
			}
			const skipped = originalCount - runEvalOptions.length;
			if (skipped > 0) logger_default.info(`Resuming: skipping ${skipped} previously completed cases`);
		} catch (err) {
			logger_default.warn(`Resume: failed to load completed results. Running full evaluation. ${String(err)}`);
		}
		if (concurrency > 1) {
			const usesConversation = prompts.some((p) => p.raw.includes("_conversation"));
			const usesStoreOutputAs = tests.some((t) => t.options?.storeOutputAs);
			if (usesConversation) {
				logger_default.info(`Setting concurrency to 1 because the ${chalk.cyan("_conversation")} variable is used.`);
				concurrency = 1;
			} else if (usesStoreOutputAs) {
				logger_default.info(`Setting concurrency to 1 because storeOutputAs is used.`);
				concurrency = 1;
			}
		}
		let numComplete = 0;
		const processEvalStep = async (evalStep, index) => {
			if (typeof index !== "number") throw new Error("Expected index to be a number");
			evalStep.test = (await runExtensionHook(testSuite.extensions, "beforeEach", { test: evalStep.test })).test;
			const rows = await runEval(evalStep);
			for (const row of rows) {
				for (const varName of Object.keys(row.vars)) vars.add(varName);
				if (row.gradingResult?.tokensUsed && row.testCase?.assert) {
					for (const assertion of row.testCase.assert) if (MODEL_GRADED_ASSERTION_TYPES.has(assertion.type)) {
						const tokensUsed = row.gradingResult.tokensUsed;
						if (!this.stats.tokenUsage.assertions) this.stats.tokenUsage.assertions = createEmptyAssertions();
						accumulateAssertionTokenUsage(this.stats.tokenUsage.assertions, tokensUsed);
						break;
					}
				}
				if (row.success) this.stats.successes++;
				else if (row.failureReason === ResultFailureReason.ERROR) this.stats.errors++;
				else this.stats.failures++;
				if (row.tokenUsage) accumulateResponseTokenUsage(this.stats.tokenUsage, { tokenUsage: row.tokenUsage });
				if (evalStep.test.assert?.some((a) => a.type === "select-best")) rowsWithSelectBestAssertion.add(row.testIdx);
				if (evalStep.test.assert?.some((a) => a.type === "max-score")) rowsWithMaxScoreAssertion.add(row.testIdx);
				for (const assert of evalStep.test.assert || []) if (assert.type) assertionTypes.add(assert.type);
				numComplete++;
				try {
					await this.evalRecord.addResult(row);
				} catch (error) {
					const resultSummary = summarizeEvaluateResultForLogging(row);
					logger_default.error(`Error saving result: ${error} ${safeJsonStringify(resultSummary)}`);
				}
				for (const writer of this.fileWriters) await writer.write(row);
				const { promptIdx } = row;
				const metrics = prompts[promptIdx].metrics;
				invariant(metrics, "Expected prompt.metrics to be set");
				metrics.score += row.score;
				for (const [key, value] of Object.entries(row.namedScores)) {
					metrics.namedScores[key] = (metrics.namedScores[key] || 0) + value;
					const testVars = row.testCase?.vars || {};
					let contributingAssertions = 0;
					row.gradingResult?.componentResults?.forEach((result) => {
						if (renderMetricName(result.assertion?.metric, testVars) === key) contributingAssertions++;
					});
					metrics.namedScoresCount[key] = (metrics.namedScoresCount[key] || 0) + (contributingAssertions || 1);
				}
				if (testSuite.derivedMetrics) {
					const math = await import("mathjs");
					const promptEvalCount = metrics.testPassCount + metrics.testFailCount + metrics.testErrorCount + 1;
					if (Object.prototype.hasOwnProperty.call(metrics.namedScores, "__count")) logger_default.warn("Metric name '__count' is reserved for derived metrics and will be overridden.");
					const evalContext = {
						...metrics.namedScores,
						__count: promptEvalCount
					};
					for (const metric of testSuite.derivedMetrics) {
						if (metrics.namedScores[metric.name] === void 0) metrics.namedScores[metric.name] = 0;
						try {
							if (typeof metric.value === "function") metrics.namedScores[metric.name] = metric.value(evalContext, evalStep);
							else {
								const evaluatedValue = math.evaluate(metric.value, evalContext);
								metrics.namedScores[metric.name] = evaluatedValue;
							}
							evalContext[metric.name] = metrics.namedScores[metric.name];
						} catch (error) {
							logger_default.debug(`Could not evaluate derived metric '${metric.name}': ${error.message}`);
						}
					}
				}
				metrics.testPassCount += row.success ? 1 : 0;
				if (!row.success) if (row.failureReason === ResultFailureReason.ERROR) metrics.testErrorCount += 1;
				else metrics.testFailCount += 1;
				metrics.assertPassCount += row.gradingResult?.componentResults?.filter((r) => r.pass).length || 0;
				metrics.assertFailCount += row.gradingResult?.componentResults?.filter((r) => !r.pass).length || 0;
				metrics.totalLatencyMs += row.latencyMs || 0;
				accumulateResponseTokenUsage(metrics.tokenUsage, row.response);
				if (row.gradingResult?.tokensUsed) updateAssertionMetrics(metrics, row.gradingResult.tokensUsed);
				metrics.cost += row.cost || 0;
				await runExtensionHook(testSuite.extensions, "afterEach", {
					test: evalStep.test,
					result: row
				});
				if (options.progressCallback) options.progressCallback(numComplete, runEvalOptions.length, index, evalStep, metrics);
			}
		};
		const processEvalStepWithTimeout = async (evalStep, index) => {
			const timeoutMs = options.timeoutMs || getEvalTimeoutMs();
			if (timeoutMs <= 0) return processEvalStep(evalStep, index);
			const abortController = new AbortController();
			const combinedSignal = evalStep.abortSignal ? AbortSignal.any([evalStep.abortSignal, abortController.signal]) : abortController.signal;
			const evalStepWithSignal = {
				...evalStep,
				abortSignal: combinedSignal
			};
			let timeoutId;
			let didTimeout = false;
			try {
				return await Promise.race([processEvalStep(evalStepWithSignal, index), new Promise((_, reject) => {
					timeoutId = setTimeout(() => {
						didTimeout = true;
						abortController.abort();
						if (typeof evalStep.provider.cleanup === "function") try {
							evalStep.provider.cleanup();
						} catch (cleanupErr) {
							logger_default.warn(`Error during provider cleanup: ${cleanupErr}`);
						}
						reject(/* @__PURE__ */ new Error(`Evaluation timed out after ${timeoutMs}ms`));
					}, timeoutMs);
				})]);
			} catch (error) {
				if (!didTimeout) throw error;
				const sanitizedTestCase = { ...evalStep.test };
				delete sanitizedTestCase.provider;
				const timeoutResult = {
					provider: {
						id: evalStep.provider.id(),
						label: evalStep.provider.label,
						config: evalStep.provider.config
					},
					prompt: {
						raw: evalStep.prompt.raw,
						label: evalStep.prompt.label,
						config: evalStep.prompt.config
					},
					vars: evalStep.test.vars || {},
					error: `Evaluation timed out after ${timeoutMs}ms: ${String(error)}`,
					success: false,
					failureReason: ResultFailureReason.ERROR,
					score: 0,
					namedScores: {},
					latencyMs: timeoutMs,
					promptIdx: evalStep.promptIdx,
					testIdx: evalStep.testIdx,
					testCase: sanitizedTestCase,
					promptId: evalStep.prompt.id || ""
				};
				await this.evalRecord.addResult(timeoutResult);
				this.stats.errors++;
				const { metrics } = prompts[evalStep.promptIdx];
				if (metrics) {
					metrics.testErrorCount += 1;
					metrics.totalLatencyMs += timeoutMs;
				}
				numComplete++;
				if (options.progressCallback) options.progressCallback(numComplete, runEvalOptions.length, typeof index === "number" ? index : 0, evalStep, metrics || {
					score: 0,
					testPassCount: 0,
					testFailCount: 0,
					testErrorCount: 1,
					assertPassCount: 0,
					assertFailCount: 0,
					totalLatencyMs: timeoutMs,
					tokenUsage: {
						total: 0,
						prompt: 0,
						completion: 0,
						cached: 0,
						numRequests: 0
					},
					namedScores: {},
					namedScoresCount: {},
					cost: 0
				});
			} finally {
				if (timeoutId) clearTimeout(timeoutId);
			}
		};
		const originalProgressCallback = this.options.progressCallback;
		const isWebUI = Boolean(cliState_default.webUI);
		logger_default.debug(`Progress bar settings: showProgressBar=${this.options.showProgressBar}, isWebUI=${isWebUI}`);
		if (isCI() && !isWebUI) {
			ciProgressReporter = new CIProgressReporter(runEvalOptions.length);
			ciProgressReporter.start();
		} else if (this.options.showProgressBar && process.stdout.isTTY) progressBarManager = new ProgressBarManager(isWebUI);
		this.options.progressCallback = (completed, total, index, evalStep, metrics) => {
			if (originalProgressCallback) originalProgressCallback(completed, total, index, evalStep, metrics);
			if (isWebUI) {
				const provider = evalStep.provider.label || evalStep.provider.id();
				const vars = formatVarsForDisplay(evalStep.test.vars, 50);
				logger_default.info(`[${numComplete}/${total}] Running ${provider} with vars: ${vars}`);
			} else if (progressBarManager) {
				const phase = evalStep.test.options?.runSerially ? "serial" : "concurrent";
				progressBarManager.updateProgress(index, evalStep, phase, metrics);
			} else if (ciProgressReporter) ciProgressReporter.update(numComplete);
			else logger_default.debug(`Eval #${index + 1} complete (${numComplete} of ${runEvalOptions.length})`);
		};
		const serialRunEvalOptions = [];
		const concurrentRunEvalOptions = [];
		for (const evalOption of runEvalOptions) if (evalOption.test.options?.runSerially) serialRunEvalOptions.push(evalOption);
		else concurrentRunEvalOptions.push(evalOption);
		if (!this.options.silent) {
			if (serialRunEvalOptions.length > 0) logger_default.info(`Running ${serialRunEvalOptions.length} test cases serially...`);
			if (concurrentRunEvalOptions.length > 0) logger_default.info(`Running ${concurrentRunEvalOptions.length} test cases (up to ${concurrency} at a time)...`);
		}
		if (this.options.showProgressBar && progressBarManager) await progressBarManager.initialize(runEvalOptions, concurrency, 0);
		try {
			if (serialRunEvalOptions.length > 0) for (const evalStep of serialRunEvalOptions) {
				if (isWebUI) {
					const provider = evalStep.provider.label || evalStep.provider.id();
					const vars = formatVarsForDisplay(evalStep.test.vars || {}, 50);
					logger_default.info(`[${numComplete}/${runEvalOptions.length}] Running ${provider} with vars: ${vars}`);
				}
				const idx = runEvalOptions.indexOf(evalStep);
				await processEvalStepWithTimeout(evalStep, idx);
				processedIndices.add(idx);
			}
			await async.forEachOfLimit(concurrentRunEvalOptions, concurrency, async (evalStep) => {
				checkAbort();
				const idx = runEvalOptions.indexOf(evalStep);
				await processEvalStepWithTimeout(evalStep, idx);
				processedIndices.add(idx);
				await this.evalRecord.addPrompts(prompts);
			});
		} catch (err) {
			if (options.abortSignal?.aborted) if (evalTimedOut) logger_default.warn(`Evaluation stopped after reaching max duration (${maxEvalTimeMs}ms)`);
			else {
				logger_default.info("Evaluation interrupted, saving progress...");
				if (globalTimeout) clearTimeout(globalTimeout);
				if (progressBarManager) progressBarManager.stop();
				if (ciProgressReporter) ciProgressReporter.finish();
				this.evalRecord.setVars(Array.from(vars));
				await this.evalRecord.addPrompts(prompts);
				updateSignalFile(this.evalRecord.id);
				return this.evalRecord;
			}
			else {
				if (ciProgressReporter) ciProgressReporter.error(`Evaluation failed: ${String(err)}`);
				throw err;
			}
		}
		const compareRowsCount = rowsWithSelectBestAssertion.size + rowsWithMaxScoreAssertion.size;
		if (progressBarManager) {
			if (compareRowsCount > 0) progressBarManager.updateTotalCount(compareRowsCount);
		} else if (ciProgressReporter && compareRowsCount > 0) ciProgressReporter.updateTotalTests(runEvalOptions.length + compareRowsCount);
		let compareCount = 0;
		for (const testIdx of rowsWithSelectBestAssertion) {
			compareCount++;
			if (isWebUI) logger_default.info(`Running model-graded comparison ${compareCount} of ${compareRowsCount}...`);
			const resultsToCompare = this.evalRecord.persisted ? await this.evalRecord.fetchResultsByTestIdx(testIdx) : this.evalRecord.results.filter((r) => r.testIdx === testIdx);
			if (resultsToCompare.length === 0) {
				logger_default.warn(`Expected results to be found for test index ${testIdx}`);
				continue;
			}
			const compareAssertion = resultsToCompare[0].testCase.assert?.find((a) => a.type === "select-best");
			if (compareAssertion) {
				const outputs = resultsToCompare.map((r) => r.response?.output || "");
				const firstResult = resultsToCompare[0];
				const providerId = firstResult.provider.id;
				const originalProvider = this.testSuite.providers.find((p) => p.id() === providerId);
				const callApiContext = originalProvider ? {
					originalProvider,
					prompt: firstResult.prompt,
					vars: firstResult.testCase.vars || {}
				} : void 0;
				const gradingResults = await runCompareAssertion(resultsToCompare[0].testCase, compareAssertion, outputs, callApiContext);
				for (let index = 0; index < resultsToCompare.length; index++) {
					const result = resultsToCompare[index];
					const gradingResult = gradingResults[index];
					const wasSuccess = result.success;
					const wasScore = result.score;
					const metrics = prompts[result.promptIdx]?.metrics;
					if (result.gradingResult) {
						result.gradingResult.tokensUsed = result.gradingResult.tokensUsed || {
							total: 0,
							prompt: 0,
							completion: 0
						};
						if (gradingResult.tokensUsed) {
							if (!result.gradingResult.tokensUsed) result.gradingResult.tokensUsed = {
								total: 0,
								prompt: 0,
								completion: 0
							};
							updateAssertionMetrics({ tokenUsage: { assertions: result.gradingResult.tokensUsed } }, gradingResult.tokensUsed);
							if (gradingResult.tokensUsed && result.testCase?.assert) {
								for (const assertion of result.testCase.assert) if (MODEL_GRADED_ASSERTION_TYPES.has(assertion.type)) {
									updateAssertionMetrics({ tokenUsage: this.stats.tokenUsage }, gradingResult.tokensUsed);
									break;
								}
							}
						}
						result.success = result.gradingResult.pass = result.gradingResult.pass && gradingResult.pass;
						if (!gradingResult.pass) {
							result.gradingResult.reason = gradingResult.reason;
							result.score = result.gradingResult.score = gradingResult.score;
						}
						if (!result.gradingResult.componentResults) result.gradingResult.componentResults = [];
						result.gradingResult.componentResults.push(gradingResult);
					} else {
						const newPass = result.success && gradingResult.pass;
						result.gradingResult = {
							...gradingResult,
							pass: newPass
						};
						result.success = newPass;
						if (!gradingResult.pass) result.score = result.gradingResult.score = gradingResult.score;
					}
					this.updateComparisonStats(result, gradingResult.pass, gradingResult.reason || "", gradingResult.tokensUsed, wasSuccess, wasScore, metrics);
					if (this.evalRecord.persisted) await result.save();
				}
				if (progressBarManager) progressBarManager.updateComparisonProgress(resultsToCompare[0].prompt.raw);
				else if (ciProgressReporter) ciProgressReporter.update(runEvalOptions.length + compareCount);
				else if (!isWebUI) logger_default.debug(`Model-graded comparison #${compareCount} of ${compareRowsCount} complete`);
			}
		}
		const maxScoreRowsCount = rowsWithMaxScoreAssertion.size;
		if (maxScoreRowsCount > 0) {
			logger_default.info(`Processing ${maxScoreRowsCount} max-score assertions...`);
			for (const testIdx of rowsWithMaxScoreAssertion) {
				const resultsToCompare = this.evalRecord.persisted ? await this.evalRecord.fetchResultsByTestIdx(testIdx) : this.evalRecord.results.filter((r) => r.testIdx === testIdx);
				if (resultsToCompare.length === 0) {
					logger_default.warn(`Expected results to be found for test index ${testIdx}`);
					continue;
				}
				const maxScoreAssertion = resultsToCompare[0].testCase.assert?.find((a) => a.type === "max-score");
				if (maxScoreAssertion) {
					const maxScoreGradingResults = await selectMaxScore(resultsToCompare.map((r) => r.response?.output || ""), resultsToCompare, maxScoreAssertion);
					if (progressBarManager) progressBarManager.updateComparisonProgress(resultsToCompare[0].prompt.raw);
					else if (ciProgressReporter) ciProgressReporter.update(runEvalOptions.length + compareCount);
					else if (!isWebUI) logger_default.debug(`Max-score assertion for test #${testIdx} complete`);
					for (let index = 0; index < resultsToCompare.length; index++) {
						const result = resultsToCompare[index];
						const maxScoreGradingResult = {
							...maxScoreGradingResults[index],
							assertion: maxScoreAssertion
						};
						const existingComponentResults = result.gradingResult?.componentResults || [];
						const existingGradingResult = result.gradingResult;
						const wasSuccess = result.success;
						const wasScore = result.score;
						const metrics = prompts[result.promptIdx]?.metrics;
						const comparisonPassed = maxScoreGradingResult.pass;
						const previousPass = existingGradingResult?.pass ?? result.success;
						const nextPass = previousPass && comparisonPassed;
						const newScore = comparisonPassed ? existingGradingResult?.score ?? result.score : maxScoreGradingResult.score;
						result.gradingResult = {
							...existingGradingResult || {},
							pass: nextPass,
							score: newScore,
							reason: !comparisonPassed && previousPass ? maxScoreGradingResult.reason : existingGradingResult?.reason ?? "",
							componentResults: [...existingComponentResults, maxScoreGradingResult],
							namedScores: {
								...existingGradingResult?.namedScores || {},
								...maxScoreGradingResult.namedScores
							},
							tokensUsed: existingGradingResult?.tokensUsed || maxScoreGradingResult.tokensUsed,
							assertion: maxScoreAssertion
						};
						result.success = nextPass;
						if (!comparisonPassed) result.score = newScore;
						this.updateComparisonStats(result, comparisonPassed, maxScoreGradingResult.reason || "", maxScoreGradingResult.tokensUsed, wasSuccess, wasScore, metrics);
						if (this.evalRecord.persisted) await result.save();
					}
				}
			}
		}
		await this.evalRecord.addPrompts(prompts);
		try {
			if (progressBarManager) {
				progressBarManager.complete();
				progressBarManager.stop();
			} else if (ciProgressReporter) ciProgressReporter.finish();
		} catch (cleanupErr) {
			logger_default.warn(`Error during progress reporter cleanup: ${cleanupErr}`);
		}
		if (globalTimeout) clearTimeout(globalTimeout);
		if (evalTimedOut) {
			for (let i = 0; i < runEvalOptions.length; i++) if (!processedIndices.has(i)) {
				const evalStep = runEvalOptions[i];
				const timeoutResult = {
					provider: {
						id: evalStep.provider.id(),
						label: evalStep.provider.label,
						config: evalStep.provider.config
					},
					prompt: {
						raw: evalStep.prompt.raw,
						label: evalStep.prompt.label,
						config: evalStep.prompt.config
					},
					vars: evalStep.test.vars || {},
					error: `Evaluation exceeded max duration of ${maxEvalTimeMs}ms`,
					success: false,
					failureReason: ResultFailureReason.ERROR,
					score: 0,
					namedScores: {},
					latencyMs: Date.now() - startTime,
					promptIdx: evalStep.promptIdx,
					testIdx: evalStep.testIdx,
					testCase: evalStep.test,
					promptId: evalStep.prompt.id || ""
				};
				await this.evalRecord.addResult(timeoutResult);
				this.stats.errors++;
				const { metrics } = prompts[evalStep.promptIdx];
				if (metrics) {
					metrics.testErrorCount += 1;
					metrics.totalLatencyMs += timeoutResult.latencyMs;
				}
			}
		}
		this.evalRecord.setVars(Array.from(vars));
		if (testSuite.extensions?.length) {
			const resultsForExtension = (await this.evalRecord.getResults()).map((result) => "toEvaluateResult" in result ? result.toEvaluateResult() : result);
			await runExtensionHook(testSuite.extensions, "afterAll", {
				prompts: this.evalRecord.prompts,
				results: resultsForExtension,
				suite: testSuite,
				evalId: this.evalRecord.id,
				config: this.evalRecord.config
			});
		}
		const totalEvalTimeMs = Date.now() - startTime;
		this.evalRecord.setDurationMs(totalEvalTimeMs);
		const totalCost = prompts.reduce((acc, p) => acc + (p.metrics?.cost || 0), 0);
		const totalRequests = this.stats.tokenUsage.numRequests;
		const totalTokens = this.stats.tokenUsage.total;
		const cachedTokens = this.stats.tokenUsage.cached;
		const totalLatencyMs = this.evalRecord.results.reduce((sum, result) => sum + (result.latencyMs || 0), 0);
		const avgLatencyMs = this.evalRecord.results.length > 0 ? totalLatencyMs / this.evalRecord.results.length : 0;
		const usesConversationVar = prompts.some((p) => p.raw.includes("_conversation"));
		const usesTransforms = Boolean(tests.some((t) => t.options?.transform || t.options?.postprocess) || testSuite.providers.some((p) => Boolean(p.transform)));
		const usesScenarios = Boolean(testSuite.scenarios && testSuite.scenarios.length > 0);
		const usesExampleProvider = testSuite.providers.some((provider) => {
			const url = typeof provider.config?.url === "string" ? provider.config.url : "";
			const label = provider.label || "";
			return url.includes("promptfoo.app") || label.toLowerCase().includes("example");
		});
		const totalAssertions = prompts.reduce((acc, p) => acc + (p.metrics?.assertPassCount || 0) + (p.metrics?.assertFailCount || 0), 0);
		const passedAssertions = prompts.reduce((acc, p) => acc + (p.metrics?.assertPassCount || 0), 0);
		const modelGradedCount = Array.from(assertionTypes).filter((type) => MODEL_GRADED_ASSERTION_TYPES.has(type)).length;
		const providerPrefixes = Array.from(new Set(testSuite.providers.map((p) => {
			const idParts = p.id().split(":");
			return idParts.length > 1 ? idParts[0] : "unknown";
		})));
		const timeoutOccurred = evalTimedOut || this.evalRecord.results.some((r) => r.failureReason === ResultFailureReason.ERROR && r.error?.includes("timed out"));
		telemetry_default.record("eval_ran", {
			numPrompts: prompts.length,
			numTests: this.stats.successes + this.stats.failures + this.stats.errors,
			numRequests: this.stats.tokenUsage.numRequests || 0,
			numResults: this.evalRecord.results.length,
			numVars: varNames.size,
			numProviders: testSuite.providers.length,
			numRepeat: options.repeat || 1,
			providerPrefixes: providerPrefixes.sort(),
			assertionTypes: Array.from(assertionTypes).sort(),
			eventSource: options.eventSource || "default",
			ci: isCI(),
			hasAnyPass: this.stats.successes > 0,
			numPasses: this.stats.successes,
			numFails: this.stats.failures,
			numErrors: this.stats.errors,
			totalEvalTimeMs,
			avgLatencyMs: Math.round(avgLatencyMs),
			concurrencyUsed: concurrency,
			timeoutOccurred,
			totalTokens,
			promptTokens: this.stats.tokenUsage.prompt,
			completionTokens: this.stats.tokenUsage.completion,
			cachedTokens,
			totalCost,
			totalRequests,
			numAssertions: totalAssertions,
			passedAssertions,
			modelGradedAssertions: modelGradedCount,
			assertionPassRate: totalAssertions > 0 ? passedAssertions / totalAssertions : 0,
			usesConversationVar,
			usesTransforms,
			usesScenarios,
			usesExampleProvider,
			isPromptfooSampleTarget: testSuite.providers.some(isPromptfooSampleTarget),
			isRedteam: Boolean(options.isRedteam),
			hasOpenAiProviders: testSuite.providers.some((p) => isOpenAiProvider(p.id())),
			hasAnthropicProviders: testSuite.providers.some((p) => isAnthropicProvider(p.id())),
			hasGoogleProviders: testSuite.providers.some((p) => isGoogleProvider(p.id()))
		});
		if (this.evalRecord.persisted) await this.evalRecord.save();
		updateSignalFile(this.evalRecord.id);
		return this.evalRecord;
	}
	async evaluate() {
		await startOtlpReceiverIfNeeded(this.testSuite);
		const tracingEnabled = getEnvBool("PROMPTFOO_TRACING_ENABLED", false) || this.testSuite.tracing?.enabled === true || typeof this.testSuite.defaultTest === "object" && this.testSuite.defaultTest?.metadata?.tracingEnabled === true || this.testSuite.tests?.some((t) => t.metadata?.tracingEnabled === true);
		if (tracingEnabled) {
			logger_default.debug("[Evaluator] Initializing OTEL SDK for tracing");
			initializeOtel(getDefaultOtelConfig());
		}
		try {
			return await this._runEvaluation();
		} finally {
			if (tracingEnabled) {
				logger_default.debug("[Evaluator] Flushing OTEL spans...");
				await flushOtel();
				await shutdownOtel();
			}
			if (isOtlpReceiverStarted()) {
				logger_default.debug("[Evaluator] Waiting for span exports to complete...");
				await sleep(3e3);
			}
			await stopOtlpReceiverIfNeeded();
			await providerRegistry.shutdownAll();
			if (this.rateLimitRegistry) {
				const metrics = this.rateLimitRegistry.getMetrics();
				for (const [key, m] of Object.entries(metrics)) if (m.totalRequests > 0) logger_default.debug(`[Scheduler] Final metrics for ${key}`, {
					totalRequests: m.totalRequests,
					completedRequests: m.completedRequests,
					failedRequests: m.failedRequests,
					rateLimitHits: m.rateLimitHits,
					retriedRequests: m.retriedRequests,
					avgLatencyMs: Math.round(m.avgLatencyMs),
					p50LatencyMs: Math.round(m.p50LatencyMs),
					p99LatencyMs: Math.round(m.p99LatencyMs)
				});
			}
			this.rateLimitRegistry?.dispose();
			redteamProviderManager.setRateLimitRegistry(void 0);
			cliState_default.maxConcurrency = void 0;
		}
	}
};
function evaluate(testSuite, evalRecord, options) {
	return new Evaluator(testSuite, evalRecord, options).evaluate();
}

//#endregion
export { runEval as a, assertions_default as c, runAssertions as d, isAllowedPrompt as i, readAssertions as l, formatVarsForDisplay as n, generateIdFromPrompt as o, generateVarCombinations as r, doesPromptRefMatch as s, evaluate as t, renderMetricName as u };
//# sourceMappingURL=evaluator-CWgN_5z0.js.map